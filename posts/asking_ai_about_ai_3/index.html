<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>用AI学AI.3 [多模态] | 波波沙🏠</title>
<meta name=keywords content="AI"><meta name=description content="上一篇发布已过去一年半，这段时间生成式AI进化肉眼可见：

上下文长度从万级到百万级；
多模态模型（文本、图像、音频、视频）；
数学与编程能力大幅提升；
联网搜索与检索增强（RAG）；
深度思考和推理；
指令遵循能力大幅提升；
Agent与MCP；
除语言模型外，图像、视频、3D模型、动画等生成全面应用；
世界模型可生成数分钟的可交互视频；

国内大模型方面，24年底横空出世的DeepSeek V3/R1用极低的训练成本跻身最强模型之列，并通过开源在极短时间内集成进各类App。8月20日V3.1发布，更是拉开了国产GPU芯片替代Nvdia GPU训练的大幕。"><meta name=author content><link rel=canonical href=https://pps43.github.io/posts/asking_ai_about_ai_3/><link crossorigin=anonymous href=/assets/css/stylesheet.da461cdc6aa5b1045299cab0ebd07edbb2f1e481e0c7ae775d260fc5af887327.css integrity="sha256-2kYc3GqlsQRSmcqw69B+27Lx5IHgx653XSYPxa+Icyc=" rel="preload stylesheet" as=style><link rel=icon href=https://pps43.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pps43.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pps43.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pps43.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pps43.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pps43.github.io/posts/asking_ai_about_ai_3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css integrity="sha512-0aPQyyeZrWj9sCA46UlmWgKOP0mUipLQ6OZXu8l4IcAmD2u31EPEy9VcIMvl7SoAaKe8bLXZhYoMaE/in+gcgA==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://unpkg.com/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#aaaaaa"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(void 0,document.querySelectorAll(".language-mermaid"))}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-7GDH7EZ6GL"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7GDH7EZ6GL")}</script><meta property="og:url" content="https://pps43.github.io/posts/asking_ai_about_ai_3/"><meta property="og:site_name" content="波波沙🏠"><meta property="og:title" content="用AI学AI.3 [多模态]"><meta property="og:description" content="上一篇发布已过去一年半，这段时间生成式AI进化肉眼可见：
上下文长度从万级到百万级； 多模态模型（文本、图像、音频、视频）； 数学与编程能力大幅提升； 联网搜索与检索增强（RAG）； 深度思考和推理； 指令遵循能力大幅提升； Agent与MCP； 除语言模型外，图像、视频、3D模型、动画等生成全面应用； 世界模型可生成数分钟的可交互视频； 国内大模型方面，24年底横空出世的DeepSeek V3/R1用极低的训练成本跻身最强模型之列，并通过开源在极短时间内集成进各类App。8月20日V3.1发布，更是拉开了国产GPU芯片替代Nvdia GPU训练的大幕。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-30T00:00:00+00:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="用AI学AI.3 [多模态]"><meta name=twitter:description content="上一篇发布已过去一年半，这段时间生成式AI进化肉眼可见：

上下文长度从万级到百万级；
多模态模型（文本、图像、音频、视频）；
数学与编程能力大幅提升；
联网搜索与检索增强（RAG）；
深度思考和推理；
指令遵循能力大幅提升；
Agent与MCP；
除语言模型外，图像、视频、3D模型、动画等生成全面应用；
世界模型可生成数分钟的可交互视频；

国内大模型方面，24年底横空出世的DeepSeek V3/R1用极低的训练成本跻身最强模型之列，并通过开源在极短时间内集成进各类App。8月20日V3.1发布，更是拉开了国产GPU芯片替代Nvdia GPU训练的大幕。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pps43.github.io/posts/"},{"@type":"ListItem","position":2,"name":"用AI学AI.3 [多模态]","item":"https://pps43.github.io/posts/asking_ai_about_ai_3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"用AI学AI.3 [多模态]","name":"用AI学AI.3 [多模态]","description":"上一篇发布已过去一年半，这段时间生成式AI进化肉眼可见：\n上下文长度从万级到百万级； 多模态模型（文本、图像、音频、视频）； 数学与编程能力大幅提升； 联网搜索与检索增强（RAG）； 深度思考和推理； 指令遵循能力大幅提升； Agent与MCP； 除语言模型外，图像、视频、3D模型、动画等生成全面应用； 世界模型可生成数分钟的可交互视频； 国内大模型方面，24年底横空出世的DeepSeek V3/R1用极低的训练成本跻身最强模型之列，并通过开源在极短时间内集成进各类App。8月20日V3.1发布，更是拉开了国产GPU芯片替代Nvdia GPU训练的大幕。\n","keywords":["AI"],"articleBody":"上一篇发布已过去一年半，这段时间生成式AI进化肉眼可见：\n上下文长度从万级到百万级； 多模态模型（文本、图像、音频、视频）； 数学与编程能力大幅提升； 联网搜索与检索增强（RAG）； 深度思考和推理； 指令遵循能力大幅提升； Agent与MCP； 除语言模型外，图像、视频、3D模型、动画等生成全面应用； 世界模型可生成数分钟的可交互视频； 国内大模型方面，24年底横空出世的DeepSeek V3/R1用极低的训练成本跻身最强模型之列，并通过开源在极短时间内集成进各类App。8月20日V3.1发布，更是拉开了国产GPU芯片替代Nvdia GPU训练的大幕。\n本篇重点了解“多模态模型(Multimodal Large Models）”。它们不仅能理解语言文字，更能“看懂”图像、“听懂”声音，将人类丰富的感知世界以更全面的方式在数字世界中进行映射和创造。\n本篇由AI起稿，人工精修。\n系列文章\n用AI学AI.1 用AI学AI.2 用AI学AI.3 用AI学AI.4 多模态 多模态大模型（Multimodal LLM, MLLM）将非文本信号（图像、音频、视频、表格、图表等）与文本统一到同一“语言空间”中建模。核心做法通常是：\n使用专用编码器将各模态转成向量序列（视觉常用 ViT/CLIP，音频常用 Conformer）； ViT借鉴了处理文本的Transformer架构，将图像分割成一个个小“补丁”（Patches），类似于将一句话拆分成单词。然后，ViT分析这些补丁及其相互之间的空间关系，最终生成一个能够代表整张图像内容和布局的向量序列。\n通过对齐模块（线性投影、MLP、Q-Former/Adapter）把各种模态向量映射到“伪词元”； 实现融合的主要技术是跨模态注意力机制。想象你在阅读一篇图文并茂的文章。当读到“埃菲尔铁塔”这个词时，你的目光会自然地移动到旁边的铁塔图片上，将文字概念与视觉形象对应起来。跨模态注意力机制就扮演了你“目光”的角色。在这个机制中，一种模态的向量（比如文本中的“埃菲尔铁塔”）会扮演查询（Query）的角色，去“质询”另一种模态的向量（图片的所有补丁）。图片中与“埃菲尔铁塔”这个概念最相关的补丁向量会作为键（Key）和值（Value）被激活，并给予更高的“注意力权重”。通过这种方式，模型就能精确地将文本描述与图像的具体区域建立起像素级的关联。\n将这些“伪词元”与文本词元拼接，输入到解码式 LLM 统一进行自回归建模； 训练分两阶段：先做对齐预训练（图文对齐/描述/VQA），再做指令微调（SFT）及偏好对齐（RLHF/DPO），必要时加入工具调用与RAG。 最核心的架构差异在于，多模态模型在纯文本LLM的基础上，增加了前端的模态编码器和中端的模态融合模块。这些新增的组件是其能够感知和理解非文本世界的关键。\n由此带来的优势：\n看图读表更准确：截图、手写、图表都能“读懂”； 一条龙完成任务：看、想、做连成一气，少来回； 解释更清楚：能给出理由、圈出图中位置，过程更透明； 适应更快：见过类似场景就能举一反三； 集成更省心：一个接口支持多种输入，系统更简单、成本更低； 更会用工具：需要时会搜索资料、算表格、跑代码，结果更可靠。 市面上的多模态模型 GPT-4o 家族（端到端文本-语音-视觉） Google Gemini 1.5（长上下文、全模态） Claude 3.5 系列（具备 Vision 能力） LLaVA、BLIP-2、Kosmos-2（学术与开源代表） Qwen-VL / Qwen2-VL（阿里） DeepSeek-VL（深度求索） InternVL / InternLM-XComposer、MiniCPM-V（开源多模态） 下面以“LLaVA 风格”的典型架构为例，说明多模态 LLM 的拼接与训练路径。\nflowchart TD subgraph Perception Encoders I[Image] --\u003e|patches| ViT[Vision Encoder a.k.a. CLIP/ViT] A[Audio] --\u003e AudEnc[Audio Encoder] end ViT --\u003e VToks[Visual Tokens] AudEnc --\u003e AToks[Audio Tokens] subgraph Modality Alignment VToks --\u003e ProjV[Vision Projector] AToks --\u003e ProjA[Audio Projector] end ProjV --\u003e MTokV[\"Pseudo Tokens :vision\"] ProjA --\u003e MTokA[\"Pseudo Tokens :audio\"] T[Text Prompt] --\u003e Tok[Tokenizer] Tok --\u003e LLM[Decoder-only LLM] MTokV --\u003e LLM MTokA --\u003e LLM LLM --\u003e Out[Text / Tool Calls] subgraph Training direction LR Align[\"`Stage 1: Modality-Text Alignment Captioning/VQA/OCR`\"] SFT[\"`Stage 2: Instruction Tuning + Preference Optimization`\"] Align-.-SFT end Align -. guides .-\u003e LLM SFT -. guides .-\u003e LLM 要点：\n感知层各司其职，保持最优表征；对齐层把不同模态“翻译”为 LLM 可消费的词元； 统一的解码式 LLM 负责跨模态推理与响应生成； 通过 VQA/描述/OCR 等任务完成对齐，再用指令数据把能力打磨成“助手式”交互； 生产环境常结合检索（RAG）、工具（代码执行、搜索、表格计算）与多轮记忆。 小结 多模态让模型真正“看得见、听得到、读得懂、能动手”。在产品上，它把“感知—推理—行动”打穿；在工程上，它将多数据源统一到一个上下文接口。下一步的关键，将是更强的过程监督、更长上下文的稳定性，以及与现实世界工具和数据的更深融合。\n","wordCount":"1819","inLanguage":"en","datePublished":"2025-08-30T00:00:00Z","dateModified":"2025-08-30T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://pps43.github.io/posts/asking_ai_about_ai_3/"},"publisher":{"@type":"Organization","name":"波波沙🏠","logo":{"@type":"ImageObject","url":"https://pps43.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pps43.github.io/ accesskey=h title="波波沙🏠 (Alt + H)">波波沙🏠</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pps43.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://pps43.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pps43.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://book.enginew.cn/ title=BookOfGameDev><span>BookOfGameDev</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">用AI学AI.3 [多模态]</h1><div class=post-meta><span title='2025-08-30 00:00:00 +0000 UTC'>August 30, 2025</span>&nbsp;·&nbsp;4 min</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%a4%9a%e6%a8%a1%e6%80%81 aria-label=多模态>多模态</a></li><li><a href=#%e5%b8%82%e9%9d%a2%e4%b8%8a%e7%9a%84%e5%a4%9a%e6%a8%a1%e6%80%81%e6%a8%a1%e5%9e%8b aria-label=市面上的多模态模型>市面上的多模态模型</a></li><li><a href=#%e5%b0%8f%e7%bb%93 aria-label=小结>小结</a></li></ul></div></details></div><div class=post-content><p><a href=https://pps43.github.io/posts/asking_ai_about_ai_2/>上一篇</a>发布已过去一年半，这段时间生成式AI进化肉眼可见：</p><ul><li>上下文长度从万级到百万级；</li><li>多模态模型（文本、图像、音频、视频）；</li><li>数学与编程能力大幅提升；</li><li>联网搜索与检索增强（RAG）；</li><li>深度思考和推理；</li><li>指令遵循能力大幅提升；</li><li>Agent与MCP；</li><li>除语言模型外，图像、视频、3D模型、动画等生成全面应用；</li><li>世界模型可生成数分钟的可交互视频；</li></ul><p>国内大模型方面，24年底横空出世的DeepSeek V3/R1用极低的训练成本跻身最强模型之列，并通过开源在极短时间内集成进各类App。8月20日V3.1发布，更是拉开了国产GPU芯片替代Nvdia GPU训练的大幕。</p><p>本篇重点了解“多模态模型(Multimodal Large Models）”。它们不仅能理解语言文字，更能“看懂”图像、“听懂”声音，将人类丰富的感知世界以更全面的方式在数字世界中进行映射和创造。</p><p>本篇由AI起稿，人工精修。</p><p><img alt=the_prompt loading=lazy src=/asking_ai_about_ai_3/the_prompt.png></p><hr><p>系列文章</p><ol><li><a href=https://pps43.github.io/posts/asking_ai_about_ai/>用AI学AI.1</a></li><li><a href=https://pps43.github.io/posts/asking_ai_about_ai_2/>用AI学AI.2</a></li><li><a href=https://pps43.github.io/posts/asking_ai_about_ai_3/>用AI学AI.3</a></li><li><a href=https://pps43.github.io/posts/asking_ai_about_ai_4/>用AI学AI.4</a></li></ol><hr><h2 id=多模态>多模态<a hidden class=anchor aria-hidden=true href=#多模态>#</a></h2><p>多模态大模型（Multimodal LLM, MLLM）将非文本信号（图像、音频、视频、表格、图表等）与文本统一到同一“语言空间”中建模。核心做法通常是：</p><ul><li>使用<strong>专用编码器</strong>将各模态转成向量序列（视觉常用 ViT/CLIP，音频常用 Conformer）；<blockquote><p>ViT借鉴了处理文本的Transformer架构，将图像分割成一个个小“补丁”（Patches），类似于将一句话拆分成单词。然后，ViT分析这些补丁及其相互之间的空间关系，最终生成一个能够代表整张图像内容和布局的向量序列。</p></blockquote></li><li>通过<strong>对齐模块</strong>（线性投影、MLP、Q-Former/Adapter）把各种模态向量映射到“伪词元”；<blockquote><p>实现融合的主要技术是跨模态注意力机制。想象你在阅读一篇图文并茂的文章。当读到“埃菲尔铁塔”这个词时，你的目光会自然地移动到旁边的铁塔图片上，将文字概念与视觉形象对应起来。跨模态注意力机制就扮演了你“目光”的角色。在这个机制中，一种模态的向量（比如文本中的“埃菲尔铁塔”）会扮演查询（Query）的角色，去“质询”另一种模态的向量（图片的所有补丁）。图片中与“埃菲尔铁塔”这个概念最相关的补丁向量会作为键（Key）和值（Value）被激活，并给予更高的“注意力权重”。通过这种方式，模型就能精确地将文本描述与图像的具体区域建立起像素级的关联。</p></blockquote></li><li>将这些“伪词元”与文本词元拼接，输入到解码式 LLM 统一进行自回归建模；</li><li>训练分两阶段：先做对齐预训练（图文对齐/描述/VQA），再做指令微调（SFT）及偏好对齐（RLHF/DPO），必要时加入工具调用与RAG。</li></ul><p>最核心的架构差异在于，多模态模型在纯文本LLM的基础上，增加了前端的模态编码器和中端的模态融合模块。这些新增的组件是其能够感知和理解非文本世界的关键。</p><p>由此带来的优势：</p><ul><li>看图读表更准确：截图、手写、图表都能“读懂”；</li><li>一条龙完成任务：看、想、做连成一气，少来回；</li><li>解释更清楚：能给出理由、圈出图中位置，过程更透明；</li><li>适应更快：见过类似场景就能举一反三；</li><li>集成更省心：一个接口支持多种输入，系统更简单、成本更低；</li><li>更会用工具：需要时会搜索资料、算表格、跑代码，结果更可靠。</li></ul><h2 id=市面上的多模态模型>市面上的多模态模型<a hidden class=anchor aria-hidden=true href=#市面上的多模态模型>#</a></h2><ul><li>GPT-4o 家族（端到端文本-语音-视觉）</li><li>Google Gemini 1.5（长上下文、全模态）</li><li>Claude 3.5 系列（具备 Vision 能力）</li><li>LLaVA、BLIP-2、Kosmos-2（学术与开源代表）</li><li>Qwen-VL / Qwen2-VL（阿里）</li><li>DeepSeek-VL（深度求索）</li><li>InternVL / InternLM-XComposer、MiniCPM-V（开源多模态）</li></ul><p>下面以“LLaVA 风格”的典型架构为例，说明多模态 LLM 的拼接与训练路径。</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>flowchart TD

    subgraph Perception Encoders
        I[Image] --&gt;|patches| ViT[Vision Encoder a.k.a. CLIP/ViT]
        A[Audio] --&gt; AudEnc[Audio Encoder]
    end

    ViT --&gt; VToks[Visual Tokens]
    AudEnc --&gt; AToks[Audio Tokens]

    subgraph Modality Alignment
        VToks --&gt; ProjV[Vision Projector]
        AToks --&gt; ProjA[Audio Projector]
    end

    ProjV --&gt; MTokV[&#34;Pseudo Tokens :vision&#34;]
    ProjA --&gt; MTokA[&#34;Pseudo Tokens :audio&#34;]

    T[Text Prompt] --&gt; Tok[Tokenizer]
    Tok --&gt; LLM[Decoder-only LLM]
    MTokV --&gt; LLM
    MTokA --&gt; LLM

    LLM --&gt; Out[Text / Tool Calls]

    subgraph Training
        direction LR
        Align[&#34;`Stage 1: 
        Modality-Text Alignment
        Captioning/VQA/OCR`&#34;]

        SFT[&#34;`Stage 2: 
        Instruction Tuning
        + Preference Optimization`&#34;]

        Align-.-SFT
    end

    Align -. guides .-&gt; LLM
    SFT -. guides .-&gt; LLM
</code></pre><p>要点：</p><ul><li>感知层各司其职，保持最优表征；对齐层把不同模态“翻译”为 LLM 可消费的词元；</li><li>统一的解码式 LLM 负责跨模态推理与响应生成；</li><li>通过 VQA/描述/OCR 等任务完成对齐，再用指令数据把能力打磨成“助手式”交互；</li><li>生产环境常结合检索（RAG）、工具（代码执行、搜索、表格计算）与多轮记忆。</li></ul><h2 id=小结>小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h2><p>多模态让模型真正“看得见、听得到、读得懂、能动手”。在产品上，它把“感知—推理—行动”打穿；在工程上，它将多数据源统一到一个上下文接口。下一步的关键，将是更强的过程监督、更长上下文的稳定性，以及与现实世界工具和数据的更深融合。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://pps43.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://pps43.github.io/posts/asking_ai_about_ai_4/><span class=title>« Prev</span><br><span>用AI学AI.4 [注意力]</span>
</a><a class=next href=https://pps43.github.io/posts/using_quaternion_fromtorotation/><span class=title>Next »</span><br><span>实现Quaternion.FromToRotation的细节</span></a></nav></footer></article></main><footer class=footer><span>© 2016-2023 By 波波沙.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>