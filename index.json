[{"content":" PhysX Engine Series è¿™ä¸ªç³»åˆ—ä¸»è¦è®°å½•ä¸€äº›æœ€è¿‘åœ¨æ¸¸æˆä¸­ä½¿ç”¨Nvdia PhysX 3.4ç‰©ç†å¼•æ“çš„ä¸€äº›ç»éªŒã€‚ æœ¬æ–‡ä¸»è¦ä»‹ç»åœºæ™¯æŸ¥è¯¢çš„ä¸€äº›å†…éƒ¨æœºåˆ¶å’Œä½¿ç”¨æ–¹æ³•ã€‚\nPrerequisites å®˜æ–¹èµ„æ–™ï¼š\nPhysX 3.4 source code Download PhysXGuide.chm A Scene in PhysX engine is a container of objects in a hierachical manner.\n--- title: Scene Hierachy --- classDiagram direction LR class world class scene { Flags Gravity ... } class actor { ActorFlags Name GlobalPose ... } class shape { Flags GeometryType LocalPose ... } class geometry { halfExtents for box ... } class material { friction restitution damping } world \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;scene scene \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;actor actor \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;shape shape o--geometry shape o--material There are only position and rotation in GlobalPose and LocalPose, no \u0026ldquo;scale\u0026rdquo;. Scale only reflects on geometry\u0026rsquo;s actual size.\nPhysX Scene Query Three kinds of scene query:\nraycast sweep overlap In general, each SceneQuery traverses a culling structure containing the scene objects, performs a precise test using the GeometryQuery functions, and accumulates the results.\nYou can customize filtering logic via prefilter and postfilter.\nflowchart LR BP(Broad Phase) --\u0026gt; prefilter --\u0026gt; MP(Mid phase) --\u0026gt; NP(Narrow Phase) --\u0026gt; postfilter Broad phase traverses the global scene spatial partitioning structure to find the candidates for mid and narrow phases. midphase traverses the triangle mesh and heightfield internal culling structures, to find a smaller subset of the triangles in a mesh reported by the broad phase. Narrow phase performs exact intersection tests Pre-filtering happens before midphase and narrow phase and allows shapes to be efficiently discarded before the potentially expensive exact collision test. Post-filtering happens after the narrow phase test and can therefore use the results of the test (such as PxRaycastHit.position) to determine whether a hit should be discarded or not. More on traversal A scene uses two query structures, one for \u0026ldquo;static\u0026rdquo; objects (PxRigidStatic), one for \u0026ldquo;dynamic\u0026rdquo; objects (PxRigidBody). Each structure can use different culling algorithms, see PxPruningStructureType.\nPxPruningStructureType Explaination eNone Based on grid. Full rebuild when changed. eDYNAMIC_AABB_TREE Based on tree. Full rebuild when changed. Only choose this if all static actors in your scene are not modified after initialization. eSTATIC_AABB_TREE Based on grid and tree. Incremental rebuild when changed, unless by force. Choose this if frequently add/remove geometry, at the cost of higher memory More on prefilter and postfilter To make prefilter works, there are 3 steps.\nfirst attach data (PxFilterData) for on shape. It has four 32bit words to hold custom data, e.g., use word0 as layer of this shape. Here is an example:\nPxShape* shape = PxRigidActorExt::createExclusiveShape(*pxActor, PxBoxGeometry(extV), *pxMaterial); PxFilterData queryFilter; queryFilter.word0 = layer; shape-\u0026gt;setQueryFilterData(queryFilter); second define callback function for prefilter. See PxQueryFilterCallback. The logic is totally depend on yourself, just return PxQueryHitType to tell if this shape can pass. PxQueryHitType Explanation eNONE Shall not pass. eTOUCH Pass, but does not stop the raycast or sweep. eBLOCK Pass, but stop here. Here is an realworld example:\nclass PhysxQueryFilterCallback : public PxQueryFilterCallback { public: PhysxQueryFilterCallback(); PhysxQueryFilterCallback(bool isBlocking, bool includeTrigger); virtual PxQueryHitType::Enum preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags); virtual PxQueryHitType::Enum postFilter(const PxFilterData\u0026amp; filterData, const PxQueryHit\u0026amp; hit); private: PxQueryHitType::Enum m_HitType; bool m_IncludeTrigger; }; PxQueryHitType::Enum PhysxQueryFilterCallback::preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags) { bool isTrigger = shape-\u0026gt;getFlags() \u0026amp; physx::PxShapeFlag::eTRIGGER_SHAPE; if (isTrigger \u0026amp;\u0026amp; !m_IncludeTrigger) { return PxQueryHitType::eNONE; } PxFilterData shapefilterData = shape-\u0026gt;getQueryFilterData(); if (shapefilterData.word0 \u0026amp; filterData.word0 || shapefilterData.word1 \u0026amp; filterData.word1) { return m_HitType; } return PxQueryHitType::eNONE; } PxQueryHitType::Enum PhysxQueryFilterCallback::postFilter(const PxFilterData\u0026amp; filterData, const PxQueryHit\u0026amp; hit) { const PxLocationHit\u0026amp; impactHit = static_cast\u0026lt;const PxLocationHit\u0026amp;\u0026gt;(hit); if (impactHit.distance \u0026gt; 0.0f) return m_HitType; return PxQueryHitType::eNONE; } third step is to add PxQueryFilterData when query PxQueryFilterData has two fields:\nfield Explaination PxQueryFlags Supported flags are in PxQueryFlag::Enums, e.g. raise ePREFILTER means all shapes need to pass prefilter you defined. PxFilterData Has four 32bit words for you, e.g. use word0 as the \u0026ldquo;layermask\u0026rdquo; of the query. Here is an realworld example of raycast (return multiple objects):\nstruct MyRaycastHitResult { PhysXVec3 Position; PhysXVec3 Normal; float Distance; PhysXActor* Collider; }; typedef PhysXArray\u0026lt;MyRaycastHitResult\u0026gt; MyRaycastHitResults; bool MyRaycast(MyRaycastHitResults\u0026amp; hitResults, const PhysXVec3\u0026amp; startPos, const PhysXVec3\u0026amp; direction, float distance, unsigned int layerMask, bool includeTrigger) { const PxU32 bufferSize = 256; PxRaycastHit hitBuffer[bufferSize]; PxRaycastBuffer buf(hitBuffer, bufferSize); PxQueryFilterData filterData = PxQueryFilterData(); filterData.flags |= PxQueryFlag::ePREFILTER | PxQueryFlag::ePOSTFILTER; filterData.data.word0 = 0; filterData.data.word1 = layerMask; PhysxQueryFilterCallback filterCallback(false, includeTrigger); bool status = m_pxScene-\u0026gt;raycast(startPos, direction, distance, buf, PxHitFlag::eDEFAULT, filterData, \u0026amp;filterCallback, NULL); if (status) { hitResults.clear(); for (PxU32 i = 0; i \u0026lt; buf.nbTouches; i++) { PxRaycastHit touch = buf.touches[i]; MyRaycastHitResult hitResult; hitResult.Position = touch.position; hitResult.Normal = touch.normal; hitResult.Distance = touch.distance; hitResult.Collider = (PhysXActor*)touch.actor-\u0026gt;userData; hitResults.push_back(hitResult); } if (buf.hasBlock) { MyRaycastHitResult hitResult; hitResult.Position = buf.block.position; hitResult.Normal = buf.block.normal; hitResult.Distance = buf.block.distance; hitResult.Collider = (PhysXActor*)buf.block.actor-\u0026gt;userData; hitResults.push_back(hitResult); } } return status; } ","permalink":"https://pps43.github.io/posts/using_physx_scenequery/","summary":"PhysX Engine Series è¿™ä¸ªç³»åˆ—ä¸»è¦è®°å½•ä¸€äº›æœ€è¿‘åœ¨æ¸¸æˆä¸­ä½¿ç”¨Nvdia PhysX 3.4ç‰©ç†å¼•æ“çš„ä¸€äº›ç»éªŒã€‚ æœ¬æ–‡ä¸»è¦ä»‹ç»åœºæ™¯æŸ¥è¯¢çš„ä¸€äº›å†…éƒ¨æœºåˆ¶å’Œä½¿ç”¨æ–¹æ³•ã€‚ Prerequisites å®˜æ–¹èµ„æ–™ï¼š PhysX","title":"PhysX Engine Series: Scene Query"},{"content":"ä¸Šä¸€ç¯‡æˆ‘ä»¬èŠäº†AIæ¨¡å‹çš„åˆ†ç±»å’Œå¤§è‡´æ¶æ„ï¼Œç›®å‰æœ€ç«çš„ä¸¤ç±»æ¨¡å‹LLMå’ŒDiffusionéƒ½åŸºäºTransformeræ¶æ„è®­ç»ƒè€Œæˆï¼ŒChatGPTåˆ™æ˜¯LLMå®¶æ—ä¸­æœ€å‡ºè‰²çš„äº§å“ã€‚æœ¬æ–‡å°±æ¥èŠèŠæœ€è¿‘å‡ºç°çš„åŸºäºChatGPTçš„åˆ›æ–°æ¶æ„å’Œåº”ç”¨ã€‚ç”±äºChatGPTçš„è®­ç»ƒæ•°æ®æˆªæ­¢åˆ°2021å¹´9æœˆï¼Œä¸”ç¬”è€…å¹¶æ²¡æœ‰Plusä¼šå‘˜æ‰èƒ½ä½¿ç”¨çš„ChatGPT Pluginï¼Œæ‰€ä»¥è¿™æ¬¡è¯·åˆ°çš„æ˜¯Dr. Phindï¼ˆThe AI search engine for developersï¼‰ï¼Œå¤§å®¶æ¬¢è¿ã€‚\nç³»åˆ—æ–‡ç« \nè·ŸAIè€å¸ˆå­¦AIï¼ˆä¸€ï¼‰ è·ŸAIè€å¸ˆå­¦AIï¼ˆäºŒï¼‰ è¯¾å ‚é—®ç­” Auto-GPT Me:\nwhat is autogpt and how does it work\nDr. Phind:\nAuto-GPT is an experimental open-source project built on the latest ChatGPT model i.e GPT-4. It is a variant of ChatGPT developed by Significant Gravitas and it uses the ChatGPT API to work. Specifically, Auto-GPT utilizes the GPT-4 API, though it seems like it should work with the GPT-3.5 API as well. Auto-GPT is an AI that does the work for you by setting a goal and then generating and completing tasks. It does all the follow-up work for you, asking and answering its own prompts. It automates multi-step projects that would have required back-and-forth prompting if you worked directly with GPT-4. In other words, it \u0026ldquo;chains together LLM \u0026rsquo;thoughts\u0026rsquo;, to autonomously achieve whatever goal you set.\u0026rdquo;\nThere are new apps that have emerged to make Auto-GPT even easier to use, like AgentGPT and GodMode, which provide a simple interface where users can input what they want to accomplish directly on a browser page.\nMe:\nhow does AutoGPT work exactly? Find the techniques behind.\nDr. Phind:\nAuto-GPT is a Python application built on top of GPT-4 that allows the AI to function independently without requiring prompts from the user for each action.\nAuto-GPT breaks down the AIâ€™s actions into â€œthoughts,â€ â€œreasoning,â€ and â€œcriticism,â€ which enables users to understand precisely what the AI is doing and why itâ€™s doing it.\nAuto-GPT uses long and short-term memory management, which is similar to how humans learn from their mistakes. Auto-GPT has the ability to assess its work, improve upon past experiences, and leverage its history to generate more precise results. Auto-GPT\u0026rsquo;s integration with vector databases, a memory storage solution, allows it to retain context and make informed decisions.\nMe:\nlist some autonomous agent besides autogpt\nDr. Phind:\nhere are several other autonomous AI agents besides Auto-GPT, some of which are Camel, BabyAGI, and Westworld simulation.\nCamel is an autonomous AI agent that uses LangChain to store and retrieve information. It can learn and understand human language and is capable of making decisions based on the context. Camel can generate content for social media and can perform simple tasks like booking a restaurant reservation or scheduling a meeting (Reference).\nWestworld simulation is an autonomous AI agent that simulates the Westworld environment. It uses reinforcement learning to improve its decision-making ability and can learn from its mistakes. Westworld simulation can interact with other agents in the environment and can perform tasks like navigating through the environment, recognizing objects, and even interacting with human.\nâš¡Me: In fact it\u0026rsquo;s an interactive sandbox environment created by researchers from Stanford and Google, the paper is here (PDF) and this is the basic architecture. BabyAGI is another autonomous AI agent that uses GPT-4, Pinecone, LangChain, and Chrome to generate and execute tasks. It can make human-like cognitive decisions and can write and run codes to meet certain objectives. BabyAGI can retain long-term memory of tasks and events with the help of Pinecone and LangChain, making fetching information faster. Its decisiveness makes it an effective tool for cryptocurrency trading, robotics, and autonomous driving.\nâš¡Me: add info from Github: the script works by running an infinite loop that does the following steps: These autonomous AI agents have different capabilities and approaches to accomplishing tasks. Camel is useful for generating content for social media and performing simple tasks, Westworld simulation is useful for simulating environments and interacting with other agents and BabyAGI is useful for decision-making sectors like autonomous driving and robotics.\nBabyAGI Me:\nwhat is babyagi and compare with autogpt?\nDr. Phind:\nBabyAGI and Auto-GPT are autonomous AI agents designed to generate and perform tasks based on a given objective. The primary difference between the two is the way they approach the objective, the techniques they use, and their purpose.\nBabyAGI uses a combination of GPT-4, Pinecone, LangChain, and Chrome to create and execute tasks. It creates a task list based on the objective entered and executes them one by one. The results of one task determine the next one. It can retain a long-term memory of tasks and events with the help of Pinecone and LangChain, which makes fetching information faster. Since it uses a trial-and-error basis to decode the results of previous tasks, BabyAGI can make complex decisions without losing track of the predefined objective.\nAuto-GPT, on the other hand, generates and runs multiple tasks at once using GPT-4 and creates an artificial memory space using GPT-3.5 to store results from previous tasks. It has access to internet services like apps, websites, books, documents, and articles and can use them to complete tasks required to reach the objective. Auto-GPT is good at generating text-rich content and images. It can also provide human-like text responses, making it useful for generating content, summarizing texts, and translating text into more than a dozen languages\nVector Database Me:\nwhat is vector database and how does it integrate into above system?\nDr. Phind:\nVector database is a type of database that is designed to handle high-dimensional data such as embeddings, which are numerical representations of objects or concepts in a high-dimensional space. In the context of Auto-GPT, vector databases are used to store the embeddings of documents that are generated by OpenAI\u0026rsquo;s language model. The vector database is used to enable similarity search over these embeddings, which is used by Auto-GPT to find the most relevant documents to complete a task.\n(Me: Below is just one example to integrate pgvector to auto-gpt, there are other choice, e.g., weaviate)\nThe vector database is integrated into the Postgres database using pgvector, an open-source vector similarity search extension for Postgres. In more detail, open-source vector similarity search extension pgvector allows for storing and querying vector embeddings in a Postgres database. OpenAI\u0026rsquo;s text-embedding-ada-002 model outputs 1536 dimensions, so that is the size of the vector used in Auto-GPT. The pgvector extension introduces a new data type called vector, which is used to create a column named embedding with the vector data type in the Postgres database. The similarity search over these embeddings is performed using a function named match_documents, which takes a query embedding, a match threshold, and a match count as inputs and returns a table of document IDs, content, and similarity. The similarity is calculated using vector math operations like cosine distance. (Reference)\nMe:\nlist some popular vector database product or repo\n(Dr.Phind\u0026hellip;. Google and Google Bard comes to help):\nSure, here are some popular vector database products or repos:\nFAISS (Facebook AI Similarity Search) is a library for efficient similarity search of dense vectors. It is open source and available on GitHub. Milvus: Milvus is a vector database designed for high-performance similarity search and machine learning.Milvus 2.0 is a cloud-native vector database with storage and computation separated by design. All components in this refactored version of Milvus are stateless to enhance elasticity and flexibility. Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data. weaviate These are just a few of the many vector database products and repos that are available. The best product or repo for you will depend on your specific needs and requirements.\nLangChain Me:\nwhat is langchain and how does it work?\nDr. Phind:\nLangChain is a Python package that provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications [3]. It is designed to assist in four main areas, which are listed here in increasing order of complexity: prompts, long-term memory (LLMs), tools, chains, agents, and memory [2].\nLangChain is a framework for developing applications powered by language models. It is created to be data-aware and agentic, meaning that it connects a language model to other sources of data and allows a language model to interact with its environment.\n(\u0026hellip;) LangChain can be used in a variety of ways, including autonomous agents, personal assistants, question answering, chatbots, querying tabular data, code understanding, interacting with APIs, summarization, and evaluation.\nMe:\ndoes AutoGPT use Langchain?\nDr. Phind:\nThere is no mention of AutoGPT using LangChain in the provided context. While LangChain is a framework that makes it easier to build scalable AI/LLM apps and chatbots, there is no clear indication that AutoGPT specifically uses LangChain.\nè¯¾åç¬”è®° Autonomous agents (e.g., AutoGPT, BabyAGI) can auto-create tasks towards the GOAL you give them, and auto-find the way to complete task based on GPT models and other data sources (websites, your documents, etc), and even auto-improve themselves from previous mistakes. The basic structure is somehow similar to GOAP in video game\u0026rsquo;s AI design. They can achieve these because they can have long-term and short-term memory powered by Vector Database. ( memory size is much more than single GPT4\u0026rsquo;s context which is 32K tokens, ~24K words.) Vector Database is a specialized db to handle vectors (read, write, calculate similarities, etc). they are useful for text because text can be transformed into numberic vectors by embedding technique. Similarity between embeddings (vectors) reflect relevance between text. Hence above agents can retrieve relevant data from large datasource and sending to GPT as context. Frameworks like LangChain makes it easier to implement autonomous agents because it provides a standard interface to handle different tools and data sources. æ‹“å±•é˜…è¯»\nTutorial: ChatGPT Over Your Data (using LangChain) Create your own chat bot with GPT-Index \u0026amp; LangChain ","permalink":"https://pps43.github.io/posts/asking_ai_about_ai_2/","summary":"ä¸Šä¸€ç¯‡æˆ‘ä»¬èŠäº†AIæ¨¡å‹çš„åˆ†ç±»å’Œå¤§è‡´æ¶æ„ï¼Œç›®å‰æœ€ç«çš„ä¸¤ç±»æ¨¡å‹LLMå’ŒDiffusionéƒ½åŸºäºTransformeræ¶æ„è®­ç»ƒè€Œæˆï¼ŒChatGPT","title":"è·ŸAIè€å¸ˆå­¦AIï¼ˆäºŒï¼‰"},{"content":"æŒ‰ç…§ç›®å‰AIåœ¨å­¦æœ¯ç•Œã€å·¥ä¸šç•Œæ¨é™ˆå‡ºæ–°çš„é€Ÿåº¦ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦ä»¬çº·çº·è¡¨ç¤ºï¼šçœŸçš„å­¦ä¸åŠ¨å•¦ï¼ä¸è¦æ…Œï¼ŒAIè€å¸ˆæ¥äº†~ åœ¨æœ¬ç³»åˆ—ä¸­ï¼Œæˆ‘ä¸ä¼šæå‰å­¦ä¹ é¢„å¤‡çŸ¥è¯†ï¼Œä»¥é—¨å¤–æ±‰çš„è§†è§’å»è¿½é—®AIå¹¶äº†è§£ä¸€äº›åŸºæœ¬æ¦‚å¿µã€‚æ—¢å¯ä»¥é”»ç‚¼æé—®æŠ€å·§ï¼Œè¿˜å¯ä»¥åœ¨å¬å®Œé«˜å±‹å»ºç“´çš„è®²è§£åæœ‰é’ˆå¯¹æ€§çš„å»å­¦ä¹ æ„Ÿå…´è¶£çš„éƒ¨åˆ†ï¼Œæ•ˆç‡å€å¢ï¼\nç³»åˆ—æ–‡ç« \nè·ŸAIè€å¸ˆå­¦AIï¼ˆä¸€ï¼‰ è·ŸAIè€å¸ˆå­¦AIï¼ˆäºŒï¼‰ è¯¾å ‚é—®ç­” Generative Models, Transformers, Large Language Models (LLM) ? Me:\nexplain generative models and list some types of it. And explain the relationship and differences with LLM.\nChatGPT:\nGenerative models are a type of machine learning model that learns to generate new data that is similar to the training data it was trained on. Unlike discriminative models, which learn to classify input data into specific categories, generative models focus on learning the underlying patterns and structure of the input data, allowing them to generate new data that is similar to the input data.\nThere are several types of generative models, including:\nVariational Autoencoders (VAEs) Generative Adversarial Networks (GANs) Autoregressive models, such as PixelCNN and PixelRNN Boltzmann Machines Restricted Boltzmann Machines (RBMs) Deep Belief Networks (DBNs) Language Models (LMs) are a type of generative model that specifically focuses on generating natural language. Like other generative models, LMs are trained on a large corpus of text data and learn to generate new text that is similar to the input data. However, LMs differ from other types of generative models in that they specifically focus on generating text and incorporate language-specific features, such as grammar, syntax, and vocabulary.\nOne key difference between LMs and other types of generative models is that LMs are typically based on probabilistic models, such as n-gram models or neural network models, whereas other types of generative models, such as GANs and VAEs, are typically based on deep learning architectures. Additionally, LMs may incorporate language-specific constraints, such as word order and grammar, that are not present in other types of generative models.\nMe:\nDo you mean generative models are unsupervised learning, while discriminative models are supervised learning?\nChatGPT:\nGenerative models can be trained using both supervised and unsupervised learning approaches, depending on the specific task and the type of data being used. In unsupervised learning, the model is trained on a dataset without explicit labels, and the goal is to learn the underlying structure and patterns in the data. In supervised learning, the model is trained using labeled data, and the goal is to learn to predict the label of new, unseen data.\nHowever, it is true that discriminative models are typically used for supervised learning tasks, such as classification, regression, and sequence labeling. These models learn to map input data to specific output labels or categories. In contrast, generative models are more commonly used for unsupervised learning tasks, such as density estimation, data synthesis, and anomaly detection. These models learn to generate new data that is similar to the training data, without necessarily mapping input data to specific output labels or categories.\nMe:\nGot it. Now explain what transformers is about in deep learning, and the relationship between generative models and the transformers.\nChatGPT:\nTransformers are a type of deep learning model that has become increasingly popular in natural language processing (NLP) tasks, such as language modeling, machine translation, and sentiment analysis. The Transformer architecture was introduced in a 2017 paper titled \u0026ldquo;Attention Is All You Need\u0026rdquo; by Vaswani et al., and it has since become the foundation for many state-of-the-art NLP models.\nThe key innovation of the Transformer architecture is the use of self-attention mechanisms to replace traditional recurrent or convolutional neural networks. Self-attention allows the model to weigh the importance of different parts of the input sequence when generating each output token. This enables the model to capture long-range dependencies and context more effectively, resulting in better performance on many NLP tasks.\nGenerative models, such as language models, can be built using the Transformer architecture. In fact, many of the most successful language models, such as OpenAI\u0026rsquo;s GPT series and Google\u0026rsquo;s BERT, are based on the Transformer architecture. These models are trained on large amounts of text data using unsupervised learning techniques, such as autoregressive language modeling or masked language modeling. Once trained, the models can be used to generate new text that is similar to the training data, or to perform downstream NLP tasks, such as sentiment analysis or machine translation.\nIn summary, the Transformer architecture is a powerful tool for building deep learning models for NLP tasks, including generative models such as language models. By leveraging self-attention mechanisms, Transformers are able to capture long-range dependencies and context more effectively, resulting in better performance on many NLP tasks.\nModels, Architectures? Me:\nDo you mean one model can be implemented in different architectures, and transformers are just one of the architectures to build models? If this is true, list some other architectures popular.\nChatGPT:\nYes, that\u0026rsquo;s correct. A single machine learning or deep learning model can be implemented using different architectures, depending on the specific task and the type of data being used. The choice of architecture can have a significant impact on the performance and efficiency of the model.\nIn addition to the Transformer architecture, there are several other popular deep learning architectures for building models in various domains. Here are a few examples:\nConvolutional Neural Networks (CNNs): These are commonly used in computer vision tasks, such as image classification, object detection, and segmentation. Recurrent Neural Networks (RNNs): These are often used for sequence modeling tasks, such as natural language processing, speech recognition, and time-series prediction. Long Short-Term Memory Networks (LSTMs): These are a type of RNN that are designed to handle long-term dependencies in sequences. Autoencoders: These are used for unsupervised learning tasks, such as feature extraction, data compression, and anomaly detection. Generative Adversarial Networks (GANs): These are used for generative modeling tasks, such as image synthesis, data augmentation, and video prediction. Variational Autoencoders (VAEs): These are also used for generative modeling tasks, such as image synthesis and data generation, but they incorporate probabilistic models and encode latent variables. These are just a few examples of the many deep learning architectures that are available for building models. The choice of architecture will depend on the specific task and the characteristics of the data being used.\nDiffusion, Stable Diffusion, latent space? Me:\nExplain stable diffusion to me. Is it a model, architecture, or others?\nChatGPT:\nStable Diffusion is a probabilistic model for generative modeling and data synthesis, and it is related to the concept of Generative Diffusion Models (GDMs).\nAt a high level, the Stable Diffusion model uses an iterative diffusion process to model the distribution of the data. The diffusion process involves iteratively adding noise to the input data and then progressively filtering out the noise using a series of conditional distributions. This process results in a sequence of probability distributions, each of which approximates the true data distribution more closely than the previous one.\nUnlike traditional diffusion models, which can suffer from instability due to the accumulation of noise over time, the Stable Diffusion model incorporates a stability regularization term to prevent this from happening. This regularization term encourages the diffusion process to converge to a stable distribution, resulting in more accurate and reliable generative models.\nThe Stable Diffusion model is closely related to Generative Diffusion Models (GDMs), which also use a diffusion process to model the data distribution. However, GDMs suffer from instability due to the accumulation of noise over time, and they require careful tuning of the learning rate to avoid this. The Stable Diffusion model addresses this issue by incorporating a stability regularization term, which improves the stability and reliability of the model.\nIn summary, Stable Diffusion is a probabilistic model for generative modeling and data synthesis that uses an iterative diffusion process and a stability regularization term to improve the stability and reliability of the model. It is closely related to Generative Diffusion Models (GDMs) but incorporates additional features to address the instability issues that can arise in traditional diffusion models.\nMe:\nDo you know stable diffusion 2? It\u0026rsquo;s said to be a latent diffusion model, what does it mean?\nChatGPT:\nYes, Stable Diffusion 2 is a follow-up work to the original Stable Diffusion model and is a latent diffusion model.\nA latent diffusion model is a type of generative model that learns a low-dimensional representation (or latent space) of the data that captures the underlying structure of the data. In other words, instead of modeling the data directly, the model learns to generate the data by sampling from a distribution in the latent space and then mapping the samples back to the data space.\nStable Diffusion 2 extends the original Stable Diffusion model by introducing a latent diffusion process. The model learns a diffusion process in the latent space, which allows it to generate data that has a more complex and structured distribution than the original model. The latent diffusion process involves iteratively adding noise to the latent space and then filtering out the noise using a series of conditional distributions. This process results in a sequence of probability distributions in the latent space, each of which approximates the true data distribution more closely than the previous one.\nOnce the model has learned the diffusion process in the latent space, it can generate new data by sampling from the final distribution in the latent space and then mapping the samples back to the data space. By using a latent diffusion process, Stable Diffusion 2 is able to generate data that has a more complex and structured distribution than the original model, making it suitable for more challenging generative modeling tasks.\nIn summary, Stable Diffusion 2 is a latent diffusion model that learns a diffusion process in the latent space to generate complex and structured data distributions. It is a more advanced version of the original Stable Diffusion model and is suitable for more challenging generative modeling tasks.\nMe:\nexplain latent space like I\u0026rsquo;m 5\nChatGPT: ğŸ‘\nImagine you have a big box full of toys. Each toy has different colors, shapes, and sizes. Now, imagine you have a secret map that tells you how to mix and match the toys to create new toys that no one has ever seen before. This secret map is like a code, and it\u0026rsquo;s hidden inside the box.\nThe box of toys is like the data, and the secret map is like the latent space. The latent space is a hidden space that tells you how to mix and match the different parts of the data to create something new. It\u0026rsquo;s like a secret code that only the computer knows.\nWhen you train a model to learn the latent space, it\u0026rsquo;s like teaching the computer to find the secret map inside the box of toys. Once the computer knows the secret map, it can create new toys that no one has ever seen before by mixing and matching the parts of the toys in new and interesting ways.\nSummarize the talk Me:\nSummarize our conversation above to give a thorough and concise relationship among all the models, architectures, and other important concepts we talked about.\nChatGPT:\nIn summary, we discussed several important concepts in deep learning, including generative models, discriminative models, language models, and architectures such as Transformers and Generative Adversarial Networks (GANs).\nGenerative models are used for unsupervised learning and aim to model the data distribution to generate new data. Discriminative models, on the other hand, are used for supervised learning and aim to learn the decision boundary between different classes.\nLanguage models are a type of generative model that learns to predict the probability of a sequence of words, given some context. Transformers are a popular architecture used for language modeling, which use a self-attention mechanism to capture long-range dependencies in the input.\nGenerative Adversarial Networks (GANs) are a type of generative model that uses two neural networks, a generator and a discriminator, to generate new data that is similar to the training data. The generator tries to create new data that can fool the discriminator, while the discriminator tries to distinguish between the generated data and the real data.\nFinally, we discussed Stable Diffusion and Stable Diffusion 2, which are probabilistic generative models that use a diffusion process to model the data distribution. Stable Diffusion 2 is a latent diffusion model that learns a diffusion process in the latent space to generate complex and structured data distributions. The latent space is a hidden space that tells the model how to mix and match the different parts of the data to create something new.\nè¯¾åç¬”è®° We train a model by choosing an architecture. Some models are named after the architecture they use, e.g., GAN, VAE. We fine-tune existing model as a foundation to generate a new model, with less training effort. LLM and Diffusion are both generative, probabilistic models. ChatGPT is a product powered by GPT model (belongs to LLM family). Stable Diffusion is a Diffusion model, and its sevond version learns pattern in a hidden box called latent space. Above models all use Transformers as the architecture. Transformers are a type of attention-based neural network. mindmap root((Deep Learning)) models discriminative model K nearest Logistic regression SVM Random Forest ... generative model deterministic CNN ... probabilistic VAE GAN LLM Diffusion architectures CNN LSTM GAN VAE ... Transformer BERT GPT ... ","permalink":"https://pps43.github.io/posts/asking_ai_about_ai/","summary":"æŒ‰ç…§ç›®å‰AIåœ¨å­¦æœ¯ç•Œã€å·¥ä¸šç•Œæ¨é™ˆå‡ºæ–°çš„é€Ÿåº¦ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦ä»¬çº·çº·è¡¨ç¤ºï¼šçœŸçš„å­¦ä¸åŠ¨å•¦ï¼ä¸è¦æ…Œï¼ŒAIè€å¸ˆæ¥äº†~ åœ¨æœ¬ç³»åˆ—ä¸­ï¼Œæˆ‘ä¸ä¼šæå‰å­¦ä¹ é¢„å¤‡çŸ¥è¯†ï¼Œ","title":"è·ŸAIè€å¸ˆå­¦AIï¼ˆä¸€ï¼‰"},{"content":" \u0026ldquo;There are decades where nothing happens; and there are weeks where decades happen.\u0026rdquo; â€• Vladimir Lenin\nåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåœ¨è¿™ä¸ªæœˆå¸¦ç»™äººä»¬çš„æ„Ÿå—ï¼Œç”¨åˆ—å®çš„è¿™å¥è¯æ¦‚æ‹¬å†è´´åˆ‡ä¸è¿‡äº†ã€‚ä½œä¸ºæ™®é€šäººï¼Œå»æ‹¥æŠ±è¿™äº›AIå·¥å…·ï¼Œå°±åƒä¼šä½¿ç”¨æ™ºèƒ½æ‰‹æœºå’Œæœç´¢å¼•æ“ï¼›å¯¹äººç±»æ¥è¯´ï¼Œå°±åƒå­¦ä¼šç”¨ç”µï¼Œå­¦ä¼šç”¨ç«ã€‚\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39; } }%% timeline title Era of AI comes in 2023 2-7: Microsoft New Bing 3-12: Open AI ChatGPT 90% cheaper 3-15 : Open AI GPT-4 3-16 : Microsoft Copilot : Midjourney V5 : Google PaLM API 3-21 : Adobe FireFly : Nvdia GTC 3-22 : Github Copilot X : Google Bard 3-24 : Open AI ChatGPT Plugins æ›´æ–°ï¼š4æœˆä»¥æ¥ï¼ŒAIåº”ç”¨çš„æ–°æ¦‚å¿µã€æ–°æ¶æ„ã€æ–°äº§å“å¦‚å¯’æ­¦çºªå¤§çˆ†ç‚¸ä¸€èˆ¬æ¶Œç°ï¼ˆAutoGPT é¦–å½“å…¶å†²ï¼‰ï¼ŒéäººåŠ›æ‰€èƒ½ç©·ä¸¾ã€‚ è¿™ä¸ªç½‘ç«™æ”¶å½•äº†å¤§é‡AIå·¥å…·ï¼Œæœ¬æ–‡ä¹Ÿä¼šæŒç»­æ›´æ–°ç¬”è€…å¸¸ç”¨ã€è§‰å¾—å¥½ç”¨çš„å·¥å…·ã€‚\nFor General Purpose Open AI\u0026rsquo;s ChatGPT, and Plugins. GPT3.5 is free to use. Microsoft\u0026rsquo;s NewBing. It\u0026rsquo;s said to be powered by GPT4 (internal version). Google\u0026rsquo;s Bard. After trying many LLM, ChatGPT is still the best one to be professional and smart. But I still prefer asking different models to get different points of view. Some common tips when asking:\nRole play. act as .... Here is a collection of role-related prompts. Give template input-output. Tell chatgpt to anwser step by step. For Doc Edge + NewBing. Explain any webpage (including PDF) side by side. ChatDoc/ChatPDF, upload PDF and analyze. â³Microsoft\u0026rsquo;s Copilot. For Software Development phind, the AI search engine for developers. Cursor editor, or vscode plugin CodeCursor, read/write current document/code, FREE to try. Old Github Copilot (based on OpenAI\u0026rsquo;s CODEX), costs $10/mo after 60d trial. â³Github Copilot X â³Copilot for Docs, used to learn a SDK/framework/API, can based on private content. The gist to generate code is, to describe a single-responsibility function to let AI generate, rather than a function with long description of chained operations.\nFor 3D/2D Art Stable-Diffusion (SD) web-ui, totally free and opensource, run model locally on PC.\nDownload/Share models on civitai/Hugging face Use ControlNet (Github )to add more controll on specific SD model. Use LoRA (Low-rank adaption) to train faster with less memory. Use Text Inversion to train with amazingly small output. Use [DreamBooth] to train if you need to be really expressive. Midjourney, famous for its artistic style, 25 times FREE try.\nAdobe\u0026rsquo;s Firefly\nOpen AI\u0026rsquo;s DALL-E-2, generates image with natural language and long prompts, but limited-access and less control.\nBing\u0026rsquo;s Image Creator, generate image with natural language, and free to try.\nFor Music Mubert Soundraw.io Want more power? If you want to:\ntrain your own AI based on these models know the strength and weakness of current AI models know why \u0026amp; how Generative AI works, mathematically Here are my personal ideas:\nFor text, play with LLaMA/llama.cpp, or its fined tuned version Alpaca/Alpaca-LoRA. For image, play with Stable-Diffusion and its plugins. They can run on PC/Mac. Weakness of current LLM models: math; chain of decision. But they are improving. \u0026ldquo;Dive into Deep Learning\u0026rdquo; by ææ²ã€‚ä¸­æ–‡ç‰ˆã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ Hardware considerations Training on cloud is cheaper and least effort to start. (Google\u0026rsquo;s Colab is even FREE) Training on local hardware, if use multiple GPUs (with NVLink), traffic bandwidth between GPUs is the botthleneck. (DGX A100 specs: 8xA100 GPUs, total 640GB VRAM, 600GB/s GPU-to-GPU bandwidth.) ","permalink":"https://pps43.github.io/posts/ai_toolkit_for_dev/","summary":"\u0026ldquo;There are decades where nothing happens; and there are weeks where decades happen.\u0026rdquo; â€• Vladimir Lenin åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåœ¨è¿™ä¸ªæœˆå¸¦ç»™äººä»¬çš„æ„Ÿå—ï¼Œç”¨åˆ—å®çš„è¿™å¥è¯æ¦‚æ‹¬å†è´´åˆ‡ä¸è¿‡äº†ã€‚ä½œä¸ºæ™®é€šäººï¼Œå»æ‹¥æŠ±è¿™äº›AIå·¥å…·ï¼Œ","title":"Generative AI Toolkit (5.12)"},{"content":"å¸‚é¢ä¸Šä»¥DevOpsä¸ºä¸»é¢˜çš„ä¹¦ï¼Œä»¥åŠå’Œé¡¹ç›®æ”»å…³çš„å½±è§†ä½œå“æœ‰ä¸å°‘äº†ï¼Œä½†è¿™æœ¬å°è¯´è¯»èµ·æ¥ä¾ç„¶ç´§å¼ åˆºæ¿€ã€‚é™¤äº†æ­ç¤ºç®¡ç†ç°ä»£ITç»„ç»‡ä¸ç®¡ç†ä¼ ç»Ÿå·¥å‚çš„å…±é€šä¹‹å¤„ï¼Œä¹¦é‡Œæ›´å¼ºè°ƒäº†ä»¥ä¸€ç§å…¨å±€çš„è§†è§’æ¥é‡æ–°çœ‹å¾…è‡ªå·±çš„å·¥ä½œç¯å¢ƒï¼Œå’Œè‡ªå·±åœ¨å·¥ä½œä¸­æ‰®æ¼”çš„è§’è‰²ï¼Œå¹¶ä¸”ï¼Œå…¶åŒ…å«çš„é—®é¢˜åˆ†ææ–¹æ³•ã€å®è·µæ–¹æ³•ä¹Ÿæˆ–å¤šæˆ–å°‘å¯ç”¨äºä¸ªäººæ•ˆèƒ½çš„ä¼˜åŒ–ã€‚\nä¸‹é¢æ˜¯ä¸€äº›ä¹¦æ‘˜å’Œç®€è¯„ã€‚\né¢å¯¹å‹åŠ›å’Œç°å® æ¯”å°”ï¼Œæˆ‘çŸ¥é“ä½ æ²¡æœ‰ç”³è¯·è¿™ä¸ªèŒä½ï¼Œä½†å…¬å¸å·²å‘½æ‚¬ä¸€çº¿ï¼Œæˆ‘éœ€è¦ä½ æ¥å¸®åŠ©æˆ‘æ‹¯æ•‘è¿™å®¶ä¼Ÿå¤§çš„å…¬å¸ã€‚æˆ‘èƒ½æŒ‡æœ›ä½ å—ï¼Ÿè¿˜æ²¡æ¥å¾—åŠå†æ¬¡ç¤¼è²Œè°¢ç»ï¼Œæˆ‘çªç„¶å¬åˆ°è‡ªå·±è¯´â€œå¯ä»¥ï¼Œä½ å¯ä»¥æŒ‡æœ›æˆ‘ã€‚â€æˆ‘æ…Œäº†ï¼Œå¼ºè¿«è‡ªå·±ä½å˜´ï¼Œä»¥å…åšå‡ºæ›´å¤šæ„šè ¢çš„æ‰¿è¯ºã€‚â€¦â€¦â€œæˆ‘ä¼šå°½åŠ›çš„ï¼Œè¿˜æœ‰ï¼Œèƒ½ä¸èƒ½è¯·ä½ è‡³å°‘è§£é‡Šä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆåœ¨è¿™ä¸ªä½å­ä¸Šçš„äººéƒ½å¹²ä¸é•¿ï¼Ÿä½ æœ€å¸Œæœ›æˆ‘åšä»€ä¹ˆï¼Œæœ€ä¸å¸Œæœ›æˆ‘åšä»€ä¹ˆï¼Ÿâ€\nä¸´å±å—å‘½æ—¶ï¼Œä¸€å®šè¦å¼„æ¸…æ¥šé¢†å¯¼çš„æ„å›¾å’Œå‰ä»»å¤±è´¥çš„åŸå› ã€‚\næˆ‘è¿˜æ¥ä¸åŠå›åº”ï¼Œèæ‹‰å°±å¤§å£°è¯´ï¼šè¿™æ°æ°è¡¨æ˜ï¼Œæ¯”å°”åŠå…¶å›¢é˜Ÿç¼ºä¹å¯¹äºç´§è¿«æ€§çš„å¿…è¦è®¤çŸ¥ã€‚è¿½æ±‚å®Œç¾æ˜¯æˆäº‹çš„å¤§æ•Œï¼Œæ¯”å°”ï¼Œæˆ‘ä»¬å¯æ²¡æœ‰é—²å·¥å¤«ä¸ºäº†è¿åˆä½ çš„é»„é‡‘æ ‡å‡†è€Œç²¾é›•ç»†ç¢â€¦â€¦éš¾é“è¿™ä¸æ˜¯æœ‰ç‚¹å¤ªè½»ç‡ã€å¤ªä¸å…¬å¹³äº†å—ï¼Ÿä½†èæ‹‰ä¸å±‘ä¸€é¡¾çš„è¯´ï¼šæˆ‘ç›¸ä¿¡å†³å®šå·²ç»åšå‡ºäº†ã€‚â€¦â€¦æ¥ä¸‹æ¥çš„9å¤©é‡Œï¼Œæˆ‘ä»¬æ‰€æœ‰äººéƒ½è¦ç†¬å¤œåŠ ç­äº†ã€‚è¿™ç§å…¨å‘˜å‡ºåŠ¨çš„å·¥ä½œçŠ¶æ€æ˜¯ITäººç”Ÿæ´»çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯æƒ³åˆ°æˆ‘ä»¬åˆå¾—å› ä¸ºå…¶ä»–äººç–äºè®¡åˆ’ è€Œä¸å¾—ä¸å¥‹åŠ›æ‹¼æï¼Œæˆ‘è¿˜æ˜¯æœ‰äº›æ¼ç«ã€‚\nåœ¨å¸‚åœºè´Ÿè´£äººèæ‹‰æ‹¿å‡ºä¸€åˆ‡ä¸ºäº†é¡¹ç›®è¿™ä¸ªå°šæ–¹å®å‰‘æ—¶ï¼Œç”±äºæ²¡æœ‰å®é™…æ•°æ®æ”¯æ’‘ï¼Œåªå› ä¸ºé“ç†è€Œä¸å»åšæ˜¯ä¸å¤Ÿçš„ï¼Œæ‰€ä»¥åªèƒ½å…ˆå¿ä¸‹æ¥ã€‚åå€’æ˜¯ä¸é¡¾å®é™…æƒ…å†µå¼ºè¡Œæ¨è¿›é¡¹ç›®å‘å¸ƒçš„èæ‹‰ï¼Œå¾—åˆ°äº†CEOå²è’‚å¤«çš„æ”¯æŒã€‚å› ä¸ºCEOæ›´å…³å¿ƒè‚¡ä»·ã€ä»¥åŠè‘£äº‹ä¼šå¯¹ä»–çš„çœ‹æ³•ï¼\næˆ‘è®¾æ³•å¤è¿°é‚£äº›ç¼œå¯†ç†æ€§ã€æ¡ç†åˆ†æ˜çš„è®ºç‚¹ï¼Œä»–ä»¬æ˜¯æˆ‘èŠ±äº†æ•´ä¸ªå‘¨æœ«çš„äº‹ä»¶æ’ç»ƒçš„â€¦â€¦æˆ‘ä¸€è¾¹è¯´ä¸€è¾¹ä¸æ–­è§‚å¯Ÿå²è’‚å¤«ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œä»–ä¸€ç›´é¢æ— è¡¨æƒ…ã€‚â€¦â€¦å²è’‚å¤«æ„¤æ€’çš„å›ç­”ï¼šä»€ä¹ˆä¼˜å…ˆçº§é«˜ä¸é«˜çš„ç‹—å±é—®é¢˜ï¼Ÿâ€¦â€¦æˆ‘åœ¨å¿ƒé‡Œé»˜æ•°ä¸‰ä¸‹æ‰å¼€å£ï¼šå½“ç„¶ï¼Œæˆ‘è¡¨è¾¾çš„ä¸å¤Ÿæ¸…æ¥šï¼Œâ€¦â€¦æˆ‘ä»¬çš„åŸºç¡€æ¶æ„å¤ªè¿‡è„†å¼±â€¦â€¦å¥½å§ï¼Œæˆ‘ä»¬ä¼šå°½æœ€å¤§åŠªåŠ›ï¼Œä½†æˆ‘è¦éƒ‘é‡å£°æ˜ï¼Œæˆ‘ä»¬çš„äººæ‰‹ä¸¥é‡ä¸è¶³ï¼Œæ— æ³•é«˜è´¨é‡å®Œæˆå…¶ä¸­ä»»ä½•ä¸€é¡¹å·¥ä½œï¼Œæ›´åˆ«è¯´å…¨éƒ¨äº†ã€‚â€¦â€¦æ¯”å°”ï¼Œå‡¤å‡°é¡¹ç›®å·²ç»è¶…æ”¯1000ä¸‡ç¾å…ƒï¼Œæˆ‘ä»¬å¿…é¡»é©¬ä¸Šå¾—åˆ°æ­£å‘ç°é‡‘æµã€‚å¢åŠ ä»»ä½•é¢„ç®—éƒ½æ˜¯ä¸å¯èƒ½çš„ï¼Œå¦‚æœä¼šæœ‰ä»€ä¹ˆè°ƒæ•´ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜å¾—åœ¨ä½ çš„éƒ¨é—¨å‡æ‰å‡ ä¸ªäººã€‚â€¦â€¦ç¦»å¼€çš„æ—¶å€™ï¼Œæˆ‘æŠŠèŠ±è´¹æ•´ä¸ªå‘¨æœ«æ—¶é—´å‡†å¤‡çš„æ¼”ç¤ºç¨¿æ‰”è¿›äº†åƒåœ¾æ¡¶ã€‚\nCEOæœ‰ä»–çš„è®¡ç®—æ–¹å¼å’Œå±æœºè§†è§’ï¼Œæ‰€ä»¥ä¸»è§’ä¸Šæ¥æ‘†äº‹å®è¯´é“ç†ä¸ä¼šå¾—åˆ°æ”¯æŒã€‚\nå…¬å¸é«˜ç®¡å¼ºè¿«æˆ‘çš„å·¥ç¨‹å¸ˆä¼˜å…ˆæ‰§è¡Œä»–ä»¬çš„å‘½ä»¤ï¼Œè¿™å®Œå…¨æ˜¯èƒ¡é—¹ã€‚æˆ‘ï¼ˆå¯¹å¸ƒä¼¦ç‰¹ï¼‰è¡¥å……é“ï¼šå¦‚æœæœ‰äººä¸ºäº†å‡¤å‡°é¡¹ç›®ä¹‹å¤–çš„äº‹å’Œä½ è”ç³»ï¼Œå°±æŠŠä»–ä»¬æ¨ç»™éŸ¦æ–¯ï¼ˆç›´æ¥ä¸Šçº§ï¼‰ï¼Œè®©ä»–å»å¯¹ä»˜é‚£å¸®ç¬¨è›‹ã€‚æ— è®ºå¦‚ä½•éƒ½è¦æƒ³åŠæ³•æ”¹æ­£å¤§å®¶ç›´æ¥æ¥æ‰¾ä½ å¸®å¿™çš„åä¹ æƒ¯ã€‚æˆ‘å…è®¸ä½ æŠŠç”µè¯è®¾æˆé™éŸ³ï¼ŒæŠŠçŠ¶æ€è®¾ç½®æˆä½ æ²¡ç©ºã€‚éšä¾¿æ€ä¹ˆæ ·éƒ½è¡Œã€‚\nå¸ƒä¼¦ç‰¹æ˜¯ä¸€åå¯¹ç³»ç»Ÿäº†è§£æœ€å¤šçš„æ˜æ˜Ÿå‘˜å·¥ï¼Œæ‰€æœ‰é‡è¦çš„ç³»ç»Ÿå˜æ›´éƒ½éœ€è¦ä»–å¸®å¿™ï¼Œä½†ä»–å°±åƒä¸€ä¸ªé»‘ç›’ï¼Œæ˜¯ç³»ç»Ÿçš„çº¦æŸç‚¹ã€‚é¦–è¦ä»»åŠ¡æ˜¯å‘ç°å¹¶ä¿æŠ¤ç³»ç»Ÿçš„çº¦æŸç‚¹ã€‚\næˆ‘æƒŠè®¶çš„ç›®çªå£å‘†ï¼šå²è’‚å¤«ï¼Œæƒ…å†µå¾—æœ‰å¤šç³Ÿç³•ä½ æ‰è‚¯æ¨è¿Ÿè¿™æ¬¡å‘å¸ƒï¼Ÿæˆ‘å‘Šè¯‰ä½ è¿™æ¬¡æ˜¯è¿è¡Œå°†æ˜¯éå¸¸é²è½çš„å†’é™©ï¼â€¦â€¦è™½ç„¶å¾ˆä¸æƒ…æ„¿ï¼Œä½†æˆ‘è¿˜æ˜¯è§‰å¾—è‡ªå·±æ¬ å…¬å¸æœ€åä¸€æï¼Œå»é˜»æ­¢è¿™æ¬¡ç–¯ç‹‚çš„è¡ŒåŠ¨ã€‚â€¦â€¦ï¼ˆå¼ºè¡Œå‘å¸ƒåï¼‰â€¦â€¦å²è’‚å¤«åœä¸‹è„šæ­¥ï¼Œç”¨æ‰‹æŒ‡ç€æˆ‘çš„å‰é¢è¯´ï¼šæˆ‘å¯¹è´£ä»»çš„ç†è§£ï¼Œæ¯”ä½ ä¸€è¾ˆå­å­¦çš„è¿˜è¦å¤šï¼æˆ‘å—å¤Ÿäº†ä½ æ•´å¤©åš·åš·ç€å¤©è¦å¡Œäº†ï¼Œæ—¶å€™åœ¨é«˜é«˜å…´å…´åœ°è¯´â€˜æˆ‘æ—©å°±å‘Šè¯‰è¿‡ä½ äº†â€™ã€‚ä½ å¾—å¸¦ç€å®é™…çš„è§£å†³æ–¹æ¡ˆæ¥æ‰¾æˆ‘ã€‚â€¦â€¦æˆ‘éœ€è¦ä¸šåŠ¡éƒ¨é—¨å‘Šè¯‰æˆ‘ï¼Œä»–ä»¬ä¸å†å—éƒ¨é—¨ITéƒ¨é—¨çš„é’³åˆ¶ã€‚æˆ‘æ‹…ä»»CEOä¾èµ–ï¼Œä¸€ç›´éƒ½å¬åˆ°è¿™æ ·çš„æŠ•è¯‰ï¼ŒITæ‹–ç´¯äº†æ¯ä¸€é¡¹é‡è¦ä¸¾æªã€‚\nåŸæ¥æ˜¯è¿™ç§å‘é…µå¾ˆä¹…çš„æƒ…ç»ªå¸¦æ¥äº†åè§ï¼Œå¯¼è‡´å²è’‚å¤«è®¨åŒæ¯”å°”çš„ç¨³æ‰ç¨³æ‰“ï¼Œåçˆ±èæ‹‰çš„å†’è¿›ã€‚\nï¼ˆå²è’‚å¤«è®²è¿°è‡ªå·±çš„ä¸ªäººç»å†å’Œå¼±ç‚¹ï¼Œå¹¶é¼“åŠ±ä¼šä¸Šçš„æ¯ä¸€ä¸ªäººéƒ½è¿™ä¹ˆåšï¼‰å±•ç°è‡ªå·±è„†å¼±çš„ä¸€é¢æœ‰åŠ©äºå»ºç«‹èµ·ä¿¡ä»»çš„åŸºç¡€ã€‚\nçº¦ç¿°æ°”æ€¥è´¥åçš„è¯´ï¼šä½ ä»¥ä¸ºä½ æ˜¯è°ï¼Ÿæˆ‘åœ¨åŠªåŠ›ä¿æŒè¿™å®¶å…¬å¸çš„å®‰å…¨ï¼Œè®©é‚£äº›å®¡è®¡å¸ˆç¦»å¾—è¿œè¿œçš„ï¼æˆ‘â€¦â€¦å¾—äº†å§ï¼Œåªä¼šå¸®å€’å¿™çš„CISOå…ˆç”Ÿã€‚åŸƒç‘å…‹æ‰“æ–­ä»–è¯´ï¼Œæ­£å¦‚ä½ åˆšæ‰çœ‹åˆ°çš„ï¼Œä¸ç”¨ä½ å‡ºæ‰‹ï¼Œè¿™å®¶å…¬å¸å°±èƒ½è®©å®¡è®¡å¸ˆç¦»å¾—è¿œè¿œçš„ã€‚ä½ å°±åƒä¸ªç®¡é“å·¥ï¼Œä¸çŸ¥é“è‡ªå·±åœ¨ä¸ºä¸€æ¶é£æœºæœåŠ¡ï¼Œæ›´åˆ«è¯´äº†è§£é£è¡Œè·¯çº¿ï¼Œæˆ–è€…èˆªç©ºå…¬å¸çš„è¥ä¸šçŠ¶å†µäº†ã€‚â€¦â€¦ä½ çœŸçš„ä¸æ˜ç™½ï¼Œæ˜¯ä¸æ˜¯ï¼Ÿæ— æé™é›¶éƒ¨ä»¶å…¬å¸æœ€å¤§çš„é£é™©æ˜¯åœä¸šç ´äº§ï¼Œè€Œä½ ä¼¼ä¹ä¸€å¿ƒæƒ³ç”¨ä½ é‚£äº›ä¸å‘¨å…¨çš„è€ƒè™‘å’Œæ— å…³ç´§è¦çš„æŠ€æœ¯ç»†èŠ‚ï¼Œè®©ä»–åŠ é€Ÿå€’é—­ã€‚æ€ªä¸å¾—ä½ ä¼šè¢«è¾¹ç¼˜åŒ–ï¼â€¦â€¦\nçº¦ç¿°èº«ä¸Šæœ‰å‡ ä¸ªé—®é¢˜ï¼š\næ²¡æœ‰ç†è§£æ€»ç›®æ ‡ã€‚æ•°æ®å®‰å…¨æ˜¯é‡è¦ï¼Œä½†å…¬å¸å’Œä¸šåŠ¡éƒ½è¦é»„äº†ï¼Œä¿è¯æ•°æ®å®‰å…¨åªæ˜¯ç©ºè°ˆè€Œå·²ã€‚ æ²¡æœ‰ç†è§£ä¸šåŠ¡å…¨æµç¨‹ã€‚äº‹åä»–æ‰çŸ¥é“å³ä½¿å‰åºçš„ITç³»ç»Ÿä¸­æœ‰å„ç§å®‰å…¨éšæ‚£ï¼Œä½†åç»­æ¯ä¸€ç¬”äº¤æ˜“éƒ½æœ‰äººå·¥å®¡æ ¸ï¼Œæ‰€ä»¥å®¡è®¡å¸ˆå¹¶ä¸ä¼šæ¥æ‰¾éº»çƒ¦ã€‚ æ²¡æœ‰çœŸæ­£ç†è§£è‡ªå·±çš„èŒè´£å’Œèƒ½åŠ›å¦‚ä½•åŠ©åŠ›äºæ€»ä½“ç›®æ ‡ã€‚æ‰€ä»¥ä»–å€¡å¯¼çš„æ•°æ®å®‰å…¨åœ¨å…¶ä»–äººçœ‹æ¥åªæœ‰å¦¨ç¢ä½œç”¨ã€‚ä»–åœ¨è¯¥é˜¶æ®µçœŸæ­£çš„ä½œç”¨åœ¨äºç”¨ç»éªŒå’Œèƒ½åŠ›å‡å°‘å…¶ä»–éƒ¨é—¨åœ¨å®‰å…¨ä¸Šçš„æ— æ•ˆåŠªåŠ›ã€‚ å…‹é‡Œæ–¯é¦–å…ˆå›ç­”ï¼šæˆ‘ä¹‹å‰å°±è¯´è¿‡ï¼Œå°±è¿æ¬¡è¦çš„æ¼æ´ä¿®å¤éƒ½é—®é¢˜é‡é‡ï¼Œæˆ‘ä»¬ä¸èƒ½æ‰¿å—æ¯æœˆä¸€æ¬¡çš„å‘å¸ƒâ€¦â€¦å²è’‚å¤«å›ç­”ï¼šæˆè´¥å°±çœ‹è¿™ä¸ªå­£åº¦ã€‚æˆ‘ä»¬å‘ä¸–äººè®¸è¯ºè¿‡ï¼Œä¼šåœ¨ä¸Šä¸ªæœˆæŠŠå‡¤å‡°å¼„å‡ºæ¥ã€‚â€¦â€¦æˆ‘ä»¬æ²¡æœ‰æ—¶é—´äº†ã€‚æˆ‘å¯¹å…‹é‡Œæ–¯è¯´ï¼Œå¦‚æœä½ è¯´å‡¤å‡°å›¢é˜Ÿåº”è¯¥æ”¾ç¼“é€Ÿåº¦ï¼Œæˆ‘ä¸ä¼šæœ‰å¼‚è®®ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä»ç„¶è¦æƒ³åŠæ³•æ»¡è¶³å²è’‚å¤«çš„è¦æ±‚ï¼Œå¦‚æœæˆ‘ä»¬ä¸èƒ½å†å‡¤å‡°çš„æ¡†æ¶å†…åšæˆè¿™ä»¶äº‹ï¼Œä¹Ÿè®¸å¯ä»¥åœ¨å‡¤å‡°ä¹‹å¤–åšåˆ°ã€‚æˆ‘æè®®å¯ä»¥ä»å‡¤å‡°ä¸»å›¢é˜Ÿåˆ†å‡ºä¸€å°é˜Ÿäººï¼Œç»„å»ºç‰¹åˆ«è¡ŒåŠ¨é˜Ÿã€‚\nè€é¡¹ç›®è€æµç¨‹ç§¯é‡éš¾è¿”ï¼Œè¿™é‡Œæä¾›äº†ä¸€ç§å‡å°‘æ£è‚˜çš„æ€è·¯ã€‚åæ–‡èƒ½çœ‹åˆ°ï¼Œç‰¹åˆ«è¡ŒåŠ¨é˜Ÿå¥æ•ˆäº†ï¼Œè€Œä¸”å…¶ä¼˜ç§€çš„å·¥ä½œæµè¿˜è¢«åŸå‡¤å‡°å›¢é˜Ÿé€æ­¥å€Ÿé‰´ï¼Œå½¢æˆåå“ºã€‚\nå»ºç«‹è®¤çŸ¥ åŠæˆå“æ˜¯ä¸ªéšå½¢æ€æ‰‹ã€‚\nå› æ­¤ï¼Œç®¡ç†ä»»ä½•ä¸€å®¶å·¥å‚æœ€å…³é”®çš„æœºåˆ¶ä¹‹ä¸€ï¼Œå°±æ˜¯å·¥ä½œä»»åŠ¡å’ŒåŸææ–™çš„å‘å¸ƒã€‚æ²¡æœ‰è¿™ä¸ªæœºåˆ¶å°±æ— æ³•æ§åˆ¶åŠæˆå“ã€‚ä¸ºäº†åœæ­¢åŠæˆå“åœ¨å·¥ä½œæµä¸­å †ç§¯ï¼Œå³ä½¿å…¶ä»–äººé—²å¾—æ— èŠï¼Œä¹Ÿä¸åº”è¯¥åœ¨ç“¶é¢ˆå·²ç»è¾¾åˆ°é¥±å’Œçš„æƒ…å†µä¸‹ï¼Œç»§ç»­åˆ¶é€ åŠæˆå“ã€‚\nåœ¨ç“¶é¢ˆä¹‹å¤–çš„ä»»ä½•åœ°æ–¹åšå‡ºçš„æ”¹è¿›éƒ½æ˜¯å‡è±¡ã€‚\nä½œä¸ºITè¿ç»´éƒ¨çš„å‰¯æ€»è£ï¼Œä½ çš„å·¥ä½œæ—¶ç¡®ä¿å½¢æˆä¸€æ¡è¿…é€Ÿã€å¯é¢„æµ‹ã€æŒç»­ä¸æ–­çš„è®¡åˆ’å†…å·¥ä½œæµï¼Œä»è€Œå‘ä¸šåŠ¡éƒ¨é—¨äº¤ä»˜å·¥ä½œä»·å€¼ï¼ŒåŒæ—¶å°½å¯èƒ½é™ä½è®¡åˆ’å¤–å·¥ä½œçš„å½±å“å’Œç ´åã€‚æˆ‘ä¸ç®¡æ¯ä¸ªäººè§‰å¾—è‡ªå·±çš„é¡¹ç›®æœ‰å¤šé‡è¦ï¼Œæˆ‘ä»¬è¦çŸ¥é“çš„æ˜¯ï¼Œè¯¥é¡¹ç›®èƒ½å¦æé«˜æˆ‘ä»¬åœ¨çº¦æŸç‚¹ä¸Šçš„å·¥ä½œèƒ½åŠ›ã€‚\nå®é™…ä¸Šæœ‰å››ç§ç±»å‹çš„å·¥ä½œï¼šæ¥è‡ªä¸šåŠ¡æ–¹çš„ã€è‡ªèº«åŸºç¡€æ¶æ„çš„ã€æ“ä½œå˜æ›´ã€è®¡åˆ’å¤–å·¥ä½œã€‚\nè®¡åˆ’å¤–å·¥ä½œæ˜¯æ¢å¤æ€§å·¥ä½œï¼ˆæ¯”å¦‚å±å±±ä»£ç å‡ºBUGåçš„æ•‘ç«ï¼‰ï¼Œå ç”¨ä½ çš„æ—¶é—´å¹¶é˜»ç¢å®Œæˆå‰ä¸‰ç§æ­£å‘äº§å‡ºçš„å·¥ä½œã€‚å¦‚æœä¸åŠ æ§åˆ¶ï¼ŒæŠ€æœ¯å€ºåŠ¡å°†å¯¼è‡´å…¬å¸é‡Œèƒ½å¤Ÿå”¯ä¸€å®Œæˆçš„å·¥ä½œå°±æ˜¯è®¡åˆ’å¤–å·¥ä½œã€‚\næµç¨‹æ˜¯ç”¨æ¥ä¿æŠ¤äººçš„ã€‚\næ ¹æ®éŸ¦æ–¯è®²çš„æ•…äº‹ï¼Œæˆ‘ä»¬ç”šè‡³éƒ½ä¸è¯¥è®©å¸ƒä¼¦ç‰¹ç¢°åˆ°é”®ç›˜ï¼Œä»–å¯ä»¥å‘Šè¯‰å¤§å®¶åº”è¯¥è¾“å…¥ä»€ä¹ˆï¼Œä½†åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œéƒ½ä¸å‡†ä»–åšå“ªäº›æˆ‘ä»¬æ— æ³•åœ¨æ—¶å€™è®°å½•çš„äº‹â€¦â€¦æ¯è§£å†³ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„çŸ¥è¯†åº“é‡Œå°±ä¼šå¤šä¸€ç¯‡å…³äºå¦‚ä½•è§£å†³æŸä¸ªç–‘éš¾æ‚ç—‡çš„æ–‡ç« ï¼Œè€Œä¸”èƒ½å¤Ÿå®æ–½ä¿®å¤çš„äººä¼šè¶Šæ¥è¶Šå¤šã€‚\nè¿™é‡Œæåˆ°çš„è§£å†³ç–‘éš¾æ‚ç—‡çš„æ–‡ç« ï¼Œç±»ä¼¼æˆ‘åœ¨å¾®è½¯å·¥ä½œæ—¶ç»„å†…ç‰¹åˆ«é‡è§†çš„postmortemï¼Œä¹Ÿç±»ä¼¼è¾¾åˆ©æ¬§åœ¨æ¡¥æ°´ç‰¹åˆ«æ¨è¡Œçš„é”™è¯¯æ—¥å¿—ã€‚ç›®çš„æ˜¯åŒæ ·çš„é”™è¯¯ç»ä¸çŠ¯ä¸¤æ¬¡ï¼Œè€Œä¸”ä¸‹æ¬¡å…¶ä»–äººä¹Ÿå¯ä»¥å¤„ç†ã€‚\nä¸€æ—¦é€šæ¸…æ¥šæœ€å¸¸å‡ºç°çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Œå°±éœ€è¦å»ºç«‹èµ·å·¥ä½œä¸­å¿ƒå’Œå·¥ä½œè·¯å¾„ã€‚\nä½ å¿…é¡»è·³å‡ºåŸæ¥çš„ä¸“æœ‰é¢†åŸŸï¼Œæ‰èƒ½å¼„æ¸…æ¥šæ•´ä½“çš„æˆåŠŸéœ€è¦ä½ çš„å“ªäº›å·¥ä½œæ¥è¾¾æˆã€‚\nITæ˜¯ä¸€ç§æŠ€èƒ½ï¼Œå°±åƒèƒ½è¯»ä¼šç®—ä¸€æ ·ã€‚ç†è§£æŠ€æœ¯èƒ½å¤Ÿåšä»€ä¹ˆï¼Œä¸èƒ½åšä»€ä¹ˆï¼Œå·²ç»æˆä¸ºä¸€å®¶å…¬å¸æ¯ä¸ªéƒ¨é—¨å¿…é¡»å…·å¤‡çš„æ ¸å¿ƒç«äº‰åŠ›ä¹‹ä¸€ã€‚\nåˆ›å»ºä¸€ä¸ªè®©äººæ„Ÿè§‰æ— èƒ½ä¸ºåŠ›çš„ç³»ç»Ÿï¼Œä½¿æˆ‘ä»¬èƒ½å¯¹äººç±»åŒèƒåšçš„æœ€å…·ç ´åæ€§çš„ä¸€ä»¶äº‹â€”â€”æˆ‘ä»¬å‰¥å¤ºäº†ä»–äººæ§åˆ¶è‡ªå·±æˆæœçš„èƒ½åŠ›ï¼Œç”šè‡³è¥é€ äº†ä¸€ç§æ–‡åŒ–ï¼Œè®©äººä»¬å› ä¸ºå®³æ€•é­å—æƒ©ç½šã€å¤±è´¥è€Œä¸æ•¢åšæ­£ç¡®çš„äº‹ã€‚è¿™åˆ¶é€ äº†â€œä¹ å¾—æ€§æ— åŠ©â€çš„ç¯å¢ƒï¼Œäººä»¬å˜å¾—ä¸æ„¿æˆ–æ— æ³•é‡‡å–è¡ŒåŠ¨æ¥é¿å…æœªæ¥é‡åˆ°åŒæ ·çš„é—®é¢˜ã€‚\nä¸‰æ­¥å»ºç«‹é«˜æ•ˆçš„å·¥ä½œæµ ç›®æ ‡ï¼šæµé‡æœ€å¤§åŒ–ï¼›å¯çµæ´»åº”å¯¹è°ƒæ•´ã€‚\nç¬¬ä¸€å·¥ä½œæ³• æ˜ç¡®ä»·å€¼é“¾ã€‚ä»æœ€ç»ˆçš„ç»©æ•ˆæŒ‡æ ‡åæ¨ï¼Œå…¶å®ç°ä¾èµ–ä»€ä¹ˆï¼Œä»¥åŠä¸æ»¡è¶³æ—¶æœ‰ä»€ä¹ˆé£é™©ã€‚ æœ€å¤§ä¼˜åŒ–æ­£å‘å·¥ä½œæµã€‚ è®©ç­‰å¾…æ—¶é—´å¯è§†åŒ–ã€‚åœ¨å·¥å‚çš„æµæ°´çº¿ä¸Šåœ¨å“ªé‡Œå‡ºç°æ‹¥å µæˆ–ç©ºè½¬æ˜¾è€Œæ˜“è§ï¼Œä½†ITæˆ–æ—¥å¸¸ç”Ÿæ´»ä¸­å¾ˆå®¹æ˜“å¿½ç•¥ï¼ˆåˆå«æš—æ—¶é—´ï¼‰ã€‚Kanbanæ˜¯ä¸€ç§æ¯”è¾ƒå¥½çš„æ–¹å¼ï¼Œæ³¨æ„åªæœ‰åœ¨ç”Ÿäº§ç¯å¢ƒé‡ŒæˆåŠŸè¿è¡Œèµ·æ¥ã€æˆ–å¼€å§‹äº§ç”Ÿä»·å€¼çš„æ—¶å€™ï¼Œæ‰èƒ½ç®—â€œå®Œæˆâ€ã€‚ å‡å°‘ä¸­æ–­ã€‚ç”Ÿäº§ä¸­æ–­åœ¨åˆ¶é€ ä¸šé‡Œæ˜¾çœ¼ä¸”ä»£ä»·æé«˜ï¼Œæ‰€æœ‰çš„åŠæˆå“éƒ½å°†æŠ¥åºŸã€‚ä½†æŠ€æœ¯å·¥ä½œè€…å¾ˆå®¹æ˜“è¢«æ‰“æ–­ï¼Œå› ä¸ºåæœä¸å¯è§ã€‚é€šè¿‡ä¸¥æ ¼é™åˆ¶å¤šä»»åŠ¡çš„æ•°é‡ã€æ–°ä»»åŠ¡çš„æ’å…¥å‡å°‘æ‰“æ–­çš„æ¬¡æ•°ã€‚ æœç»ç¼ºé™·å‘ä¸‹æ¸¸ä¼ é€’ã€‚å‘ç°éš¾é¢˜ï¼Œç¾¤ç­–ç¾¤åŠ›ï¼Œå»ºç«‹æœºåˆ¶è€Œä¸æ˜¯ä»¥åå†è¯´ã€‚ å‡å°äº¤ä»˜çš„å†…å®¹å¤§å°å’Œç­‰å¾…é—´éš”ã€‚ æŒç»­è¯†åˆ«ã€æ”¹å–„çº¦æŸç‚¹ã€‚çº¦æŸç‚¹å¯ä»¥æ˜¯ä¸€ä¸ªäººï¼Œæˆ–è€…å¤§å®¶éƒ½å€šèµ–çš„åŒä¸€ç§èµ„æºã€å‰ç½®æ­¥éª¤ã€‚ä»¥DevOpsä¸ºä¾‹ï¼Œé€šå¸¸è¦ä¾æ¬¡ä¼˜åŒ–ä»¥ä¸‹çº¦æŸç‚¹ï¼š1ã€ç¯å¢ƒæ­å»ºï¼›2ã€ä»£ç éƒ¨ç½²ï¼›3ã€å‡†å¤‡å’Œæ‰§è¡Œæµ‹è¯•ï¼›4ã€æ¶æ„è€¦åˆã€‚ æ¶ˆé™¤æµªè´¹ã€‚ä¾‹å¦‚ï¼š1ã€åŠæˆå“ï¼ˆæ–‡æ¡£ã€å˜æ›´å•ç­‰ï¼‰ï¼›2ã€å¤šä½™å·¥åºï¼ˆå¯¹åç»­æµç¨‹æ— ä»·å€¼ï¼‰ï¼›3ã€ä»»åŠ¡åˆ‡æ¢ï¼›4ã€èµ„æºç«äº‰å’Œç­‰å¾…ï¼›5ã€éæ ‡å‡†æˆ–æ‰‹åŠ¨æ“ä½œã€‚ ç¬¬äºŒå·¥ä½œæ³• å¿«é€Ÿã€æŒç»­çš„è·å¾—åé¦ˆã€‚å¦åˆ™æ²¡äººæ•¢åœ¨ä¸€ä¸ªå¤æ‚ç³»ç»Ÿä¸­æ”¾å¿ƒçš„å·¥ä½œï¼ˆä¸ç”¨æ‹…å¿ƒè‡ªå·±çš„æŸä¸€ä¸ªæ“ä½œåœ¨å°†æ¥æåç³»ç»Ÿä¸­å¦ä¸€ä¸ªæœ¬ä¸ç›¸å…³çš„åŠŸèƒ½ï¼‰ã€‚ ä»ä»¥å¾®è½¯ä¸ºä¾‹ï¼ŒSubstrateä»“åº“é‡Œæœ‰å¤šä¸ªM365ç›¸å…³çš„äº§å“ï¼Œçº¦5000+å·¥ç¨‹å¸ˆç»´æŠ¤ç€ä¸Šç™¾GBçš„æºç æ–‡ä»¶ï¼Œå³ä½¿ç©·å°½ä¸€ç”Ÿä¹Ÿæ²¡æœ‰äººèƒ½å……åˆ†äº†è§£è¿™æ ·ä¸€ä¸ªå·²ç»å­˜æ´»å‡ åå¹´çš„åºç„¶å·¨ç‰©ã€‚ç„¶è€Œè¿™æ ·ä¸€ä¸ªç³»ç»Ÿçš„è¿­ä»£é€Ÿåº¦å´éå¸¸æƒŠäººï¼Œä¿éšœè¿™ä¸ªå·¥ä½œæµçš„å…¶ä¸­ä¸€ä¸ªæ–¹å¼æ˜¯ï¼š\næ¯ä¸€æ¬¡ä»£ç æäº¤éƒ½ä¼šè§¦å‘å¤šç§è‡ªåŠ¨åŒ–æµ‹è¯•å’Œæ¨¡æ‹Ÿéƒ¨ç½²ï¼Œåœ¨å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿå†…å‘Šè¯‰ä½ æ˜¯å¦å¯ä»¥å°†ä»£ç åˆå…¥ä¸»å¹²ã€‚ åŒæ—¶ä¼šè‡ªåŠ¨@æ‰€æœ‰ç›¸å…³æ¨¡å—çš„è´Ÿè´£äººåœ¨DevOpsç³»ç»Ÿä¸­è¿›è¡ŒReviewï¼Œç›´åˆ°æ‰€æœ‰åˆ©ç›Šç›¸å…³è€…åŒæ„åˆå…¥ã€‚ ç¬¬ä¸‰å·¥ä½œæ³• å»ºç«‹å…·æœ‰åˆ›æ„å’Œé«˜å¯ä¿¡åº¦çš„æ–‡åŒ–ã€‚å®ƒå¼ºè°ƒæ¯ä¸ªäººéƒ½æ˜¯æŒç»­å­¦ä¹ è€…ï¼Œå¿…é¡»å‹‡äºæ‰¿æ‹…é£é™©ï¼Œé€šè¿‡ç§‘å­¦çš„æ–¹å¼æ”¹è¿›ã€‚å½¼æ­¤åˆ†äº«ç»éªŒï¼Œç»éªŒç§¯ç´¯æ²‰æ·€ã€‚\nå°†æ—¥å¸¸å·¥ä½œçš„æ”¹è¿›åˆ¶åº¦åŒ– å°†å±€éƒ¨çš„ä¼˜ç§€ç»éªŒå…¨å±€åŒ– é¢†å¯¼å±‚ä¸ºå›¢é˜Ÿåˆ›é€ å­¦ä¹ æ¡ä»¶ï¼Œé¢†å¯¼åŠ›å¹¶ä¸ä½“ç°åœ¨åšå‡ºçš„æ‰€æœ‰å†³å®šéƒ½æ˜¯å¯¹çš„ã€‚ ","permalink":"https://pps43.github.io/posts/notes_on_phoenix_project/","summary":"å¸‚é¢ä¸Šä»¥DevOpsä¸ºä¸»é¢˜çš„ä¹¦ï¼Œä»¥åŠå’Œé¡¹ç›®æ”»å…³çš„å½±è§†ä½œå“æœ‰ä¸å°‘äº†ï¼Œä½†è¿™æœ¬å°è¯´è¯»èµ·æ¥ä¾ç„¶ç´§å¼ åˆºæ¿€ã€‚é™¤äº†æ­ç¤ºç®¡ç†ç°ä»£ITç»„ç»‡ä¸ç®¡ç†ä¼ ç»Ÿå·¥å‚çš„å…±é€š","title":"å‡¤å‡°é¡¹ç›®"},{"content":" è¿‘æ—¥è¯»å®Œã€Šè´«ç©·çš„è´¨æ„ŸÂ·ç‹æ¢†çš„è‹±å›½è§‚å¯Ÿã€‹ï¼ˆè±†ç“£è¯»ä¹¦ï¼ŒGoodreadsï¼‰ä¸€ä¹¦åï¼Œå°†åªè¨€ç‰‡è¯­ç»„åˆèµ·æ¥ä½œä¸€ç‚¹è®°å½•ã€‚æ­¤ä¹¦è¯´äº†å¾ˆå¤šè‹±å›½æ™®é€šäººçš„äº‹æƒ…ã€ç¤¾ä¼šä¼ ç»Ÿï¼Œä½†ç›¸ä¿¡æˆ‘ä»¬è¯»èµ·æ¥åº”è¯¥ä¸ä¼šå¤ªé™Œç”Ÿã€‚é€è¿‡è¯¸å¤šç”Ÿæ´»ä¸­çš„è§‚å¯Ÿï¼Œè‡ªç„¶å¼•ç”³å‡ºå…»è€ã€åŒ»ç–—ã€ç¤¾ä¼šåˆ¶åº¦ã€é€†å…¨çƒåŒ–ç­‰è¯é¢˜ã€‚\nåªè¨€ç‰‡è¯­ è‹±å›½æ”¿åºœæ›¾æ¨å‡ºå¤§é‡å»‰ç§Ÿæˆ¿ï¼Œè€æ—§äº†ä¾¿æ‹¿æ¥å–é’±ã€‚ç”±äºæ˜¯å¸‚é¢ä»·æ ¼çš„ä¸€åŠï¼Œéå¸¸å—ç‚’æˆ¿å®¢çš„æ¬¢è¿ã€‚æœ‰é’±äººä¹°æ¥ç»§ç»­å‡ºç§Ÿï¼Œç§Ÿé‡‘æ¯”åŸæ¥è¿˜è¦é«˜ã€‚ä½†ç”±äºæ”¿åºœä¸å†è´Ÿè´£ä¿®ç¼®ï¼Œè¿™äº›æˆ¿å­é€æ¸å˜å¾—åˆè´µåˆè„ã€‚\næ–¯è’‚èŠ¬Â·é˜¿å§†æ–¯ç‰¹æœ—æŒ‡å‡ºï¼šç©·äººä¸æ˜¯è´«å›°çš„ç½ªé­ç¥¸é¦–ï¼Œè€Œæ˜¯ä»–çš„äº§ç‰©ã€‚è´«å›°ä¹Ÿä¸å¯èƒ½é€šè¿‡è‡ªå¾‹è‡ªæ„ˆï¼Œç´§ç¼©åªä¼šå¯¼å‘æ›´æè‡´çš„è´«ç©·ã€‚\n19ä¸–çºªçš„æ”¹é©å®¶ä¸ºåº•å±‚äººæ°‘æŒ‡å‡ºçš„é€šå¾€ä¸­äº§ä¹‹è·¯æ˜¯äº”æ¡ç®´è¨€ï¼šå‹¤åŠ³ï¼ŒåšæŒï¼ŒèŠ‚ä¿­ï¼Œå®¡æ…ï¼Œè‡ªåŠ›æ›´ç”Ÿã€‚ä½†ç¤¾ä¼šç°å®æ˜¯ï¼Œæƒè´µä»¬æŒæ¡ç€è¯è¯­æƒï¼Œä¸€æ—¦ä½ é™·å…¥è´«ç©·ï¼Œæ­¤å‰çš„è¾›å‹¤ä»˜å‡ºå…¨ä¸ç®—æ•°ï¼Œç¤¾ä¼šå¯¹ä½ çš„å›°é¡¿åªæœ‰ä¸€æ¡è§£é‡Šï¼šæ‡’æƒ°ã€‚\nç§äººå…»è€é™¢çš„æ¯”ä¾‹æ€¥å‰§ä¸Šå‡ï¼Œå¾ˆå¤šæŠ•æœºå•†å¤§ä¸¾æŠ•èµ„å…»è€é™¢ï¼Œç»è¥ä¸å–„åˆ™æ‰”ç»™æ”¿åºœé€ƒä¹‹å¤­å¤­ã€‚æ”¶è´¹998è‹±é•‘ä¸€å‘¨ã€‚æ²¡é’±äº†æ€ä¹ˆåŠï¼Ÿå¯ä»¥å–æˆ¿æˆ–è€…å…ˆä½ï¼Œæ­»åæˆ¿å­è¢«æ”¶èµ°ã€‚ä½†ä¸€èˆ¬æˆ¿äº§çš„ä»·æ ¼åªå¤Ÿä½å››å¹´ã€‚å·¥èµ„ä½é›‡å‘˜å°‘ï¼Œè™å¾…äº‹ä»¶å› æ­¤æä¸ºæ™®éã€‚å‡ ä¹æ¯æ‰€å¤§å­¦éƒ½åœ¨æƒ³æ–¹è®¾æ³•å–æ¶ˆå…¨èŒåˆçº¦ï¼Œå› ä¸ºè¿™æ ·å°±èƒ½çœæ‰ä¸å°‘èŒä¸šå…»è€é‡‘äº†ã€‚\nè„±æ¬§æ˜¯è¿™ä¹ˆå¤šå¹´æ¥ä»–ä»¬ï¼ˆå†œæ°‘ï¼Œæµæ°´çº¿å·¥äººç­‰ç­‰ï¼‰æ‰€è·å¾—å”¯ä¸€ä¸€æ¬¡å‘å£°ï¼Œæ˜¯ä»–ä»¬æƒ³è±¡ä¸­å¯¹æŠ—å¤–æ¥ç§»æ°‘ã€ä»¥åŠå›½é™…å„æ–­èµ„æœ¬ä¸»ä¹‰å¸¦æ¥çš„å›°è‹¦ç”Ÿæ´»çŠ¶æ€çš„ä¸€ç§æ–¹å¼ã€‚\nKeep calm and carry on. è¿™å¥è¯è‡ªç»´å¤šåˆ©äºšæ—¶ä»£å°±è¢«è‹±å›½ç¤¾ä¼šè§†ä¸ºè¡Œä¸ºå…¸èŒƒã€‚ææ…Œæ˜¯å¯è€»çš„ã€‚æ­£å¦‚è´«ç©·æ˜¯å¯è€»çš„ã€‚ç–«æƒ…åï¼Œä¸€éƒ¨åˆ†äººè¿‡ä¸Šäº†åœ¨å®¶åŠå…¬çš„ç”Ÿæ´»ï¼Œå¦ä¸€éƒ¨åˆ†äººåˆ™æˆä¸ºâ€œå‰çº¿å·¥äººâ€ï¼ˆçœ‹æŠ¤ï¼Œé¤é¥®å·¥ä½œç­‰ï¼‰ã€‚è‡ªå°åŸåï¼Œäººä»¬å¯¹å‰çº¿å·¥äººçš„èµç¾å¼€å§‹å±‚å‡ºä¸ç©·ï¼Œä½†è¿™æ”¹å˜ä¸äº†ä»–ä»¬çš„é«˜æ­»äº¡ç‡ã€‚\n1863å¹´ï¼Œå¤å…¸è‡ªç”±ä¸»ä¹‰è€…å¼•é¢†äº†åºŸå¥´è¿åŠ¨å¹¶è·å¾—å·¨å¤§æˆåŠŸã€‚å…¨çƒåŒ–åï¼Œæ–°è‡ªç”±ä¸»ä¹‰è€…ï¼ˆå¤§èµ„æœ¬å®¶ï¼‰å´ç”¨è‡ªç”±å¸‚åœºé‡æ–°åŒ…è£…äº†è“„å¥´ä¸»ä¹‰ï¼Œä¸”ä¸æ¯«ä¸è§‰æœ‰ä»»ä½•é“å¾·è´Ÿæ‹…ã€‚åŠè¯¡çš„æ˜¯ï¼Œè®¸å¤šè¢«æŸå®³çš„äººä¹Ÿä¿æŒç€åŒæ ·çš„é“å¾·ä¼˜è¶Šæ„Ÿï¼šè‡ªç”±å¸‚åœºæ˜¯å®Œç¾çš„ï¼Œç©·äººæ˜¯ä¸å¹¸çš„ï¼Œå› ä¸ºä»–ä»¬å¤©ç”Ÿæ™ºå•†ä½ä¸‹ã€‚å¯Œäººæ˜¯ä¼˜èƒœåŠ£æ±°çš„äº§ç‰©ï¼Œé€šè¿‡åˆ›é€ è´¢å¯Œç¼´çº³ç¨é‡‘ï¼Œè¿åŒç©·äººä¸€èµ·å—ç›Šã€‚\nç›¸å…³é˜…è¯» Why Poverty? (PBS documentary films) Poor Economics (ä¸­è¯‘åã€Šè´«ç©·çš„æœ¬è´¨ã€‹)ï¼Œè±†ç“£è¯»ä¹¦, Goodreads ","permalink":"https://pps43.github.io/posts/notes_on_texture_of_hard_times/","summary":"è¿‘æ—¥è¯»å®Œã€Šè´«ç©·çš„è´¨æ„ŸÂ·ç‹æ¢†çš„è‹±å›½è§‚å¯Ÿã€‹ï¼ˆè±†ç“£è¯»ä¹¦ï¼ŒGoodreadsï¼‰ä¸€ä¹¦åï¼Œå°†åªè¨€ç‰‡è¯­ç»„åˆèµ·æ¥ä½œä¸€ç‚¹è®°å½•ã€‚æ­¤ä¹¦è¯´äº†å¾ˆå¤šè‹±å›½æ™®é€šäººçš„äº‹æƒ…ã€ç¤¾","title":"è´«ç©·çš„è´¨æ„Ÿï¼ˆç‹æ¢†ï¼‰"},{"content":" æœ¬æ–‡æ€»ç»“è‡ªRay Dalio: How the Economic Machine Worksï¼ŒåŸè§†é¢‘åœ°å€\næ ¸å¿ƒæ¦‚å¿µç®€è¿° ä»€ä¹ˆæ˜¯äº¤æ˜“ ä¹°æ–¹ä½¿ç”¨è´§å¸ã€ä¿¡ç”¨ï¼ˆä¹Ÿå«ä¿¡è´·ï¼‰å‘å–æ–¹äº¤æ¢å•†å“ã€æœåŠ¡ã€é‡‘èèµ„äº§ã€‚ äº¤æ˜“é¦–å…ˆæ˜¯ç”±äººçš„å¤©æ€§é©±åŠ¨çš„ã€‚ ä¹°æ–¹çš„æ”¯å‡ºæ€»é¢ï¼ˆè€Œä¸æ˜¯å…¶ä»–ï¼‰æ˜¯ç»æµçš„é©±åŠ¨åŠ›ï¼Œæ‰€æœ‰çš„ç»æµå‘¨æœŸå’ŒåŠ¨åŠ›éƒ½æ˜¯äº¤æ˜“é€ æˆçš„ã€‚ äº¤æ˜“çš„ä¸»ä½“é€šå¸¸æœ‰ï¼šä¸ªäººã€ä¼ä¸šã€é“¶è¡Œã€æ”¿åºœã€‚ æ”¿åºœå¦‚ä½•å‚ä¸äº¤æ˜“ æ”¿åºœå…¶å®åŒæ—¶æ˜¯æœ€å¤§çš„ä¹°æ–¹å’Œå–æ–¹ã€‚ æ”¿åºœä¸»è¦ç”±ä¸¤éƒ¨åˆ†ï¼šä¸­å¤®æ”¿åºœï¼Œè´Ÿè´£æ”¶ç¨ï¼›ä¸­å¤®é“¶è¡Œï¼Œè´Ÿè´£èŠ±é’±ã€‚ ä¸­å¤®é“¶è¡Œé€šè¿‡åˆ©ç‡å’Œå°é’±æ§åˆ¶ç»æµä¸­çš„è´§å¸å’Œä¿¡ç”¨ï¼ˆä¿¡è´·æ•°é‡ï¼‰ã€‚ ä»€ä¹ˆæ˜¯ä¿¡è´· ä¿¡è´·ä¹Ÿæ˜¯ä¸€ç§äº¤æ˜“ã€‚æ¯”å¦‚ä½ æ‰¾é“¶è¡Œå€Ÿé’±ï¼Œä½ æ˜¯å€Ÿæ¬¾äººï¼Œé“¶è¡Œæ˜¯è´·æ¬¾äººã€‚ä½ æ˜¯äº¤æ˜“çš„å–æ–¹ï¼Œé“¶è¡Œæ˜¯ä¹°æ–¹ã€‚ è¿™ç§äº¤æ˜“çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºï¼šäº¤æ˜“æ˜¯å»¶è¿Ÿå®Œæˆçš„ï¼Œå³å€Ÿæ¬¾äººä¿è¯å°†æ¥å¿è¿˜è´·æ¬¾å’Œåˆ©æ¯ï¼Œç›´åˆ°é‚£ä¸ªæ—¶å€™ï¼Œäº¤æ˜“æ‰ç®—å®Œæˆã€‚ äº¤æ˜“æœªå®Œæˆå‰çš„çŠ¶æ€ï¼Œå³å€ºåŠ¡ã€‚å€ºåŠ¡æ˜¯è´·æ¬¾äººçš„èµ„äº§ï¼Œå€Ÿæ¬¾äººçš„è´Ÿå€ºã€‚ åªæœ‰äº¤æ˜“å®Œæˆæ—¶ï¼Œè¿™äº›èµ„äº§å’Œè´Ÿå€ºæ‰ä¼šæ¶ˆå¤±ã€‚ ä»€ä¹ˆæ˜¯ä»·æ ¼ $$ ä»·æ ¼ = \\frac{ä¹°æ–¹æ”¯å‡ºæ€»é¢}{äº§é”€æ€»é‡} =\\frac{(è´§å¸ + ä¿¡è´·)} {äº§é”€æ€»é‡} $$\nä»€ä¹ˆæ˜¯å¸‚åœº ä¸€ç§å¸‚åœºï¼ŒåŒ…å«åŒä¸€ç§å•†å“çš„æ‰€æœ‰ä¹°æ–¹å’Œæ‰€æœ‰å–æ–¹ã€‚æœ‰åƒåƒä¸‡ä¸‡ç§å•†å“æˆ–æœåŠ¡å¯¹åº”åƒåƒä¸‡ä¸‡ç§å¸‚åœºã€‚\nä»€ä¹ˆæ˜¯ç»æµ å…¨éƒ¨å¸‚åœºçš„å…¨éƒ¨äº¤æ˜“ç»Ÿç§°ä¸ºç»æµã€‚\nç»æµå½¢åŠ¿å˜åŒ–çš„ä¸‰æ¡è„‰ç»œ è¿™ä¸‰ä¸ªç›¸å¯¹ç‹¬ç«‹çš„å› ç´ å åŠ èµ·æ¥ï¼Œæ„æˆç»æµå½¢åŠ¿çš„å˜åŒ–ï¼š\nç”Ÿäº§ç‡çš„æé«˜ï¼ˆè¿‘ä¼¼çº¿æ€§ï¼‰ å€ºåŠ¡å°å‘¨æœŸï¼ˆ5~8å¹´ï¼‰ å€ºåŠ¡å¤§å‘¨æœŸï¼ˆ75~100å¹´ï¼‰ æ·±å…¥ç»æµå‘¨æœŸ ç»æµä¸ºä»€ä¹ˆä¾èµ–ä¿¡è´· ç®€è€Œè¨€ä¹‹ï¼Œå› ä¸ºå€Ÿæ¬¾äººé€šè¿‡ä¿¡è´·å¯ä»¥å¢åŠ è‡ªå·±çš„æ”¯å‡ºï¼Œè€Œæ”¯å‡ºæ˜¯ç»æµçš„é©±åŠ¨åŠ›ã€‚åœ¨æ²¡æœ‰ä¿¡è´·çš„ç¤¾ä¼šä¸­ï¼Œå¢åŠ æ”¯å‡ºçš„å”¯ä¸€æ–¹å¼æ˜¯å¢åŠ ç”Ÿäº§ã€‚ç”±äºä¿¡è´·å¯ä»¥å‡­ç©ºäº§ç”Ÿï¼Œæ”¯å‡ºå¯ä»¥è¶…è¶Šå½“å‰ç”Ÿäº§ï¼Œæ‰€ä»¥èƒ½è®©ç»æµæ³¨å…¥æ›´å¤šæ´»åŠ›ã€‚\næ›´å…·ä½“çš„è¯´ï¼Œäº¤æ˜“åŒæ–¹ä¸­ï¼ŒAçš„æ”¯å‡ºæ˜¯Bçš„æ”¶å…¥ï¼ŒAæ”¯å‡ºå¢åŠ ç­‰äºBæ”¶å…¥å¢åŠ ï¼ŒBå¿è¿˜èƒ½åŠ›æå‡ï¼Œä¿¡è´·èƒ½åŠ›ä¹Ÿä¼šæå‡ï¼ŒBçš„æ”¯å‡ºå¯ä»¥è¿›ä¸€æ­¥å¢åŠ ï¼Œåœ¨å¦ä¸€åœºäº¤æ˜“ä¸­ï¼ŒBçš„æ”¯å‡ºå¢åŠ ç­‰äºCçš„æ”¶å…¥å¢åŠ â€¦â€¦äº§ç”Ÿå¾ªç¯ã€‚ä¸è¿‡è¦æ³¨æ„ï¼Œå€Ÿé’±å®è´¨æ˜¯å‘æœªæ¥æŸä¸€æ—¶åˆ»çš„è‡ªå·±å€Ÿé’±ï¼Œå³æœªæ¥é‚£ä¸ªæ—¶åˆ»è‡ªå·±çš„æ”¶å…¥å‡æ‰æ¶ˆè´¹å¿…é¡»è¶³å¤Ÿè¿˜é’±æ‰èƒ½ä¿è¯äº¤æ˜“æœ€ç»ˆçš„æœ‰æ•ˆæ€§ã€‚\nç»æµä¸Šä¸‹èµ·ä¼ä¸å–å†³äºäººä»¬å¤šä¹ˆå–„äºåˆ›æ–°æˆ–å‹¤å¥‹å·¥ä½œï¼Œè€Œçœ‹ä¿¡è´·çš„æ€»é‡ã€‚è€Œäººçš„å¤©æ€§å’Œå€Ÿè´·æœºåˆ¶å†³å®šäº†ä¿¡è´·æ€»é‡ä¸€å®šæ˜¯æ³¢åŠ¨çš„ã€‚\nçŸ­æœŸä¿¡è´·å‘¨æœŸä¸ºä»€ä¹ˆå‡ºç° è€ƒè™‘å•ä¸€å•†å“çš„æƒ…å†µã€‚å¦‚æœæ”¯å‡ºï¼ˆæ¶ˆè´¹ï¼‰æˆ–è€…æ”¶å…¥ï¼ˆæ¶ˆè´¹æ˜¯å¦ä¸€ä¸ªäººçš„æ”¶å…¥ï¼‰çš„å¢é•¿é€Ÿåº¦è¶…è¿‡æ‰€å‡ºå”®å•†å“çš„ç”Ÿäº§é€Ÿåº¦ï¼Œå•†å“ä»·æ ¼å°±ä¼šä¸Šæ¶¨ã€‚å¤®è¡Œä¸ºäº†é˜²æ­¢é€šè´§è†¨èƒ€ï¼Œä¼šæé«˜åˆ©ç‡ã€‚åˆ©ç‡ç›´æ¥å½±å“äººä»¬è´·æ¬¾å’Œå€Ÿæ¬¾æ„æ„¿â€”â€”è¿™ä¹Ÿæ˜¯é™åˆ¶æ”¯å‡ºçš„å”¯ä¸€å› ç´ ã€‚è¿˜è´·å¢åŠ é€ æˆå¯æ”¯é…çš„æ”¯å‡ºå‡å°‘ï¼Œå•†å“ä»·æ ¼ä¸‹è·Œã€‚\nå½“ä¿¡è´·æ˜“äºè·å¾—åˆ™ç»æµæ‰©å¼ ï¼Œä¿¡è´·ä¸æ˜“è·å¾—åˆ™ç»æµæ”¶ç¼©ï¼Œè¿™ä¸ªå‘¨æœŸç”±å¤®è¡Œä¸»å¯¼ï¼Œä¸€èˆ¬5~8å¹´ã€‚\né•¿æœŸä¿¡è´·å‘¨æœŸä¸ºä»€ä¹ˆå‡ºç° ç»å†ä¸€ä¸ªçŸ­æœŸå‘¨æœŸåï¼Œç»æµå¢é•¿å’Œå€ºåŠ¡è§„æ¨¡éƒ½ä¼šè¶…è¿‡å‰ä¸€ä¸ªå‘¨æœŸã€‚å€ºåŠ¡è§„æ¨¡çš„æ‰©å¤§å…¶å®æ­£æ˜¯äººçš„å¤©æ€§ï¼šæ›´å–œæ¬¢æ¶ˆè´¹è€Œä¸æ˜¯è¿˜é’±ã€‚å› æ­¤ç»å†è‹¥å¹²çŸ­å‘¨æœŸåï¼Œå€ºåŠ¡å¢åŠ çš„é€Ÿåº¦å…¶å®ä¼šè¶…è¿‡æ”¶å…¥ï¼ˆè™½ç„¶æ”¶å…¥ä¹Ÿåœ¨å¢åŠ ï¼‰ã€‚åˆ°é‚£æ—¶ï¼Œç¤¾ä¼šæ€»å€ºåŠ¡ä¼šå˜å¾—è¿‡é‡è€Œå¯¼è‡´æ”¯å‡ºã€æ”¶å…¥ã€ä¿¡ç”¨çš„å…¨é¢ä¸‹é™ï¼Œè¿™ä¸ªè¿‡ç¨‹å«å»æ æ†åŒ–ã€‚\nç¤¾ä¼šæ€»å€ºåŠ¡ï¼šæ”¿åºœå€ºåŠ¡ã€å±…æ°‘å€ºåŠ¡ã€éé‡‘èä¼ä¸šå€ºåŠ¡ã€‚ç¤¾ä¼šæ€»å€ºåŠ¡è¿‡é‡ä¸€èˆ¬æ˜¯æŒ‡è¶…è¿‡GDPçš„å¥½å‡ å€ã€‚\nå»æ æ†åŒ–è¿‡ç¨‹å¼€å§‹åï¼Œä¼šå‡ºç°èµ„äº§ä»·æ ¼ä¸‹è·Œã€ä¼ä¸šç ´äº§ã€ä¿¡è´·æ¶ˆå¤±ã€æ— æ³•æ­£å¸¸è¿˜è´·ï¼Œåªèƒ½å˜å–èµ„äº§ï¼Œè¿›ä¸€æ­¥é€ æˆèµ„äº§ä»·æ ¼ä¸‹è·Œï¼Œå½¢æˆæ¶æ€§å¾ªç¯ï¼Œä¸”æ— æ³•é€šè¿‡é™ä½åˆ©ç‡æ‘†è„±ï¼ˆé™åˆ°0ä»¥åå°±åˆ°åº•äº†ï¼‰ã€‚ä¾‹å¦‚æ¬§ç¾1929å¹´ã€2008å¹´ï¼Œæ—¥æœ¬1989å¹´ã€‚\nflowchart LR èµ„äº§ä»·æ ¼ä¸‹è·Œ --\u0026gt; ä¼ä¸šæˆ–ä¸ªäººç ´äº§ --\u0026gt; ä¿¡è´·æ¶ˆå¤± --\u0026gt; å˜å–èµ„äº§ -.-\u0026gt; èµ„äº§ä»·æ ¼ä¸‹è·Œ æ ¹æ®å…·ä½“æƒ…å†µçš„ä¸åŒï¼Œè¯¥å‘¨æœŸå¯èƒ½æŒç»­75~100å¹´ã€‚\næœ‰å“ªäº›æ–¹å¼è§£å†³å€ºåŠ¡è¿‡é‡é—®é¢˜ è¡¡é‡å€ºåŠ¡è´Ÿæ‹…æ˜¯å¦è¿‡é‡ï¼š\n$$ å€ºåŠ¡è´Ÿæ‹… = \\frac{å€ºåŠ¡}{æ”¶å…¥} $$\nè§£å†³å€ºåŠ¡è¿‡é‡çš„å››ç§ä¸»è¦æ‰‹æ®µï¼š\næ‰€æœ‰äº¤æ˜“ä¸»ä½“å‰Šå‡æ”¯å‡ºï¼›\nå› ä¸ºAçš„æ”¯å‡ºæ˜¯Bçš„æ”¶å…¥ï¼ŒBæ”¶å…¥ä¹Ÿä¸‹é™äº†ï¼Œé‚£ä¹ˆBçš„å€ºåŠ¡è´Ÿæ‹…æ›´é‡ï¼Œæ‰€æœ‰äººæ›´ç—›è‹¦ã€‚ä¼šæ¼”å˜æˆé€šè´§ç´§ç¼©ã€‚ ä¾‹å¦‚ä¼ä¸šä¸ºäº†å‰Šå‡æˆæœ¬é€ æˆå¤§é‡äººå£å¤±ä¸šã€‚ é€šè¿‡åˆ¤å®šè¿çº¦å’Œé‡ç»„å‡å°‘å€ºåŠ¡ï¼›\nå¤§é‡å€Ÿæ¬¾äººæ— æ³•å¿è¿˜å³å€ºåŠ¡è¿çº¦åï¼Œè´·æ¬¾äººä¼šå‘é“¶è¡Œç´¢è¦å­˜æ¬¾ï¼Œé€ æˆæŒ¤å…‘ï¼Œé“¶è¡Œè‡ªèº«ä¹Ÿä¼šè¿çº¦ã€‚è¿™å°±å«ç»æµè§æ¡ï¼Œå³æ‰€æœ‰äººéƒ½æ„Ÿè§‰è‡ªå·±çš„èµ„äº§è’¸å‘äº†ã€‚ æ¯”å€ºåŠ¡è¿çº¦ç¨å¥½ä¸€ç‚¹çš„æ–¹å¼æ˜¯å€ºåŠ¡é‡ç»„ï¼Œå³è¿˜æ¬¾å‡å°‘æˆ–å»¶æœŸæˆ–åˆ©ç‡é™ä½ã€‚ ä¸è®ºå“ªç§æ–¹å¼ï¼Œç”±äºAçš„å€ºåŠ¡æ˜¯Bçš„èµ„äº§ï¼Œå€ºåŠ¡æ¶ˆå¤±æˆ–å‡å°‘ï¼Œæ„å‘³ç€Bçš„èµ„äº§å‡å°‘ã€‚ä¾æ—§ä¼šæ¼”å˜æˆé€šè´§ç´§ç¼©ã€‚ æ”¿åºœè´¢å¯Œå†åˆ†é…ï¼›\nåœ¨å»æ æ†åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œä¼´éšç€å¤±ä¸šäººå£ä¸Šå‡ï¼Œå› æ­¤æ”¿åºœæ”¯å‡ºå˜å¤šç”šè‡³è´¢æ”¿èµ¤å­—ï¼Œå¿…é¡»æ”¶æ›´å¤šçš„ç¨æˆ–å‘è¡Œå€ºåˆ¸ã€‚ å‘è°èèµ„æˆ–å¾ç¨å‘¢ï¼Ÿé€šå¸¸æ˜¯å¯Œäººã€‚ä½†å®¹æ˜“å¼•å‘ç¤¾ä¼šåŠ¨è¡ã€‚å¦‚æœæ˜¯å›½ä¸å›½ä¹‹é—´ï¼Œä¼šå¼•å‘æˆ˜äº‰ã€‚ å¤®è¡Œå¢å‘è´§å¸ï¼›\nå‰æ–‡æåˆ°å¤®è¡Œçš„ä¸¤ä¸ªæ‰‹æ®µï¼šåˆ©ç‡å’Œå°é’ã€‚åˆ©ç‡é™ä½åˆ°0ä¹‹åï¼Œåªèƒ½å°é’ã€‚ å°å‡ºçš„é’ç¥¨åªèƒ½ç”¨æ¥ä¹°å…¥é‡‘èèµ„äº§å’Œæ”¿åºœå€ºåˆ¸ã€‚å‰è€…å¯ä»¥ä¿ä½é‡‘èèµ„äº§ï¼ˆä¾‹å¦‚è‚¡å¸‚ï¼‰çš„ä»·æ ¼ï¼Œå› æ­¤æé«˜äº†æŒæœ‰äººçš„ä¿¡ç”¨ï¼ˆä½†ä»…ä»…å¯¹æŒæœ‰é‡‘èèµ„äº§çš„äººæœ‰ç”¨ï¼‰ï¼Œåè€…å…¶å®æ˜¯æŠŠå°å‡ºçš„é’±å€Ÿç»™æ”¿åºœï¼Œç”±æ”¿åºœé€šè¿‡æ•‘æµé‡‘å’Œåˆºæ¿€è®¡åˆ’å¢åŠ äººä»¬æ”¶å…¥ï¼Œå®è´¨ä¸Šæ˜¯å°†ç»æµç³»ç»Ÿåˆ†æ•£çš„æ€»å€ºåŠ¡ï¼Œé›†ä¸­åˆ°æ”¿åºœçš„å¤´ä¸Šã€‚ å¢å‘çš„è´§å¸åˆšå¥½æŠµæ¶ˆè´·æ¬¾çš„é™å¹…ï¼Œå°±ä¸ä¼šå¼•å‘é€šè´§è†¨èƒ€ï¼ˆå‚è€ƒå‰æ–‡è¯´è¿‡çš„ä»·æ ¼å…¬å¼ï¼‰ã€‚æ¸©å’Œçš„é€šè´§è†¨èƒ€æœ¬èº«å¯¹ç»æµå¹¶ä¸æœ‰å®³ã€‚ä½†å¢å‘å¤ªçŒ›ä¼šå¯¼è‡´æ¶æ€§é€šè´§è†¨èƒ€ï¼Œä¾‹å¦‚1920å¹´çš„å¾·å›½ã€‚ ç»“åˆå…·ä½“æƒ…åŠ¿ï¼Œé€šè¿‡å·§å¦™åœ°ç»„åˆä½¿ç”¨ä¸Šè¿°å››ç§æ‰‹æ®µï¼Œæ‰å¯ä»¥åšåˆ°å’Œè°çš„å»æ æ†ã€‚ä¹Ÿä¸è¦å¤ªæ‚²è§‚ï¼Œå› ä¸ºå’Œè°çš„å»æ æ†åŒ–è¿‡ç¨‹æ¯”ä¿¡ç”¨æ‰©å¼ æ—¶æœŸæ›´å¥½ï¼Œå› ä¸ºæ­¤æ—¶ç»æµæ˜¯åœ¨è‡ªç„¶ç¼“æ…¢å¢é•¿çš„ï¼Œå¤¯å®æ ¹åŸºã€‚\nåæ€ ä¸Šè¿°æ¨¡å‹å¤§è‡´å¯ä»¥åˆ¤æ–­å½“å‰æˆ‘ä»¬å¤„äºå“ªä¸ªé˜¶æ®µï¼Œé¢„æµ‹æœªæ¥å¤§è¶‹åŠ¿å¦‚ä½•ã€‚ ä¸Šè¿°æ¨¡å‹è§£é‡Šäº†ç»æµå‘¨æœŸçš„å¿…ç„¶æ€§ï¼šç°ä»£ç¤¾ä¼šçš„ç»æµå‘å±•ä¾èµ–ä¿¡è´·ï¼Œä¿¡è´·ä¾èµ–æœºåˆ¶å’Œäººæ€§ï¼Œæœºåˆ¶æœ‰ä¸€å®šçš„è°ƒæ•´ç©ºé—´ï¼ˆä½†äº¤æ˜“å»¶è¿Ÿå®Œæˆå’Œè¦è¿æœ¬å¸¦åˆ©æ˜¯åˆšæ€§çš„ï¼‰ã€äººæ€§åˆ™ä¸å¯é¿å…çš„åŒ…å«è´ªå©ªã€ææƒ§ã€å–œæ¬¢æ¶ˆè´¹è€Œä¸æ˜¯è¿˜é’±ã€ä»ä¼—â€¦â€¦ ä¸Šè¿°æ¨¡å‹å¯¹äºä¸ªäººçš„å¯å‘æ„ä¹‰ï¼š ä¸è¦è®©å€ºåŠ¡çš„å¢é•¿é€Ÿåº¦è¶…è¿‡æ”¶å…¥ï¼Œå› ä¸ºæœ€ç»ˆä¼šè®©å€ºåŠ¡æŠŠä½ å‹å®ï¼› ä¸è¦è®©æ”¶å…¥çš„å¢é•¿é€Ÿåº¦è¶…è¿‡ç”Ÿäº§ç‡ï¼Œå› ä¸ºæœ€ç»ˆè¿™ä¼šè®©ä½ å¤±å»ç«äº‰åŠ›ï¼› å°½ä¸€åˆ‡åŠªåŠ›æé«˜ç”Ÿäº§ç‡ï¼Œå› ä¸ºå®ƒåœ¨é•¿æœŸèµ·æœ€å…³é”®çš„ä½œç”¨ã€‚ ","permalink":"https://pps43.github.io/posts/how_the_economic_machine_works/","summary":"æœ¬æ–‡æ€»ç»“è‡ªRay Dalio: How the Economic Machine Worksï¼ŒåŸè§†é¢‘åœ°å€ æ ¸å¿ƒæ¦‚å¿µç®€è¿° ä»€ä¹ˆæ˜¯äº¤æ˜“ ä¹°æ–¹ä½¿ç”¨è´§å¸ã€ä¿¡ç”¨ï¼ˆä¹Ÿå«ä¿¡è´·ï¼‰å‘å–æ–¹äº¤æ¢å•†å“ã€æœåŠ¡ã€é‡‘èèµ„äº§ã€‚ äº¤æ˜“é¦–å…ˆ","title":"ç»æµæœºå™¨å¦‚ä½•è¿è¡Œï¼ˆè¾¾åˆ©æ¬§ï¼‰"}]