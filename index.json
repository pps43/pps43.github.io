[{"content":" 本篇着重于收集编程和软件工程中的洞察和智慧，大部分来自业内顶尖的前辈，例如Joel Spolsky, John Carmack等等，也有个人的思考。当我这些年经历过不同行业、国内外不同公司、不同技术栈后，在真正认同这些观点对于产能带来多大提升的同时，也发现它们是多么容易被忽略。总结在这里，常看常新。\nAbout Code I It’s harder to read code than to write it.\n\u0026ndash;Joel, \u0026ldquo;Things You Should Never Do, Part I\u0026rdquo;\n在你抱怨项目代码是一座屎山时，很多时候是因为阅读代码的固有难度带来的负面体验。固有难度是指，你不了解突然插入的这段代码的意图（多数是为了修某个bug），也很难从全局对一长串代码加以认识。不少人因此而畏惧代码重用而复制一部分来形成“自己的特供版本”，殊不知这才是形成真正屎山的源头。\n也因此，为了减少屎山的产生，我们要不遗余力的从各个层面降低阅读代码的门槛。\n留下注释说明新增这段代码的意图，从你开始！ 使用更好的代码浏览工具。这一点Rider的实时搜索要好于VS。SciTools出品的Understand在某些时候也可以尝试用于理解宏观结构。2023年有了GPT，结合AI来理解代码更是值得尝试的方法。后面还会提到，看代码的同时结合调试更能事半功倍。 优化编程语言，比如引入更简洁且代码表达力强的语法，即使只是语法糖！在某些情况下可以考虑创造DSL（微信支付团队通过实践证实了这种做法是高效且可靠的）。 适当重构业务逻辑。通过多人多版本的迭代，一段代码的表达力可能变得支离破碎。即使只是通过重命名让变量和函数表意更清晰，也是有意义的。你很可能会因此而发现不合预期但尚未发现的bug。庞大如微软的Exchange的一些底层服务，重构和翻新也很积极——这些需要其他基建配合，中小公司需要评估性价比。 II Use as many tools as you can find to understand the code. You should do experiments on the system, by adding logs, using asserts as dynamic comments, and using debugger as your companion (even if writing new code).\n\u0026ndash;Carmack, interview with Lex Fridman.\nGo through code as it\u0026rsquo;s running. You can see the entry point, the entire memory, call stack/threads\u0026hellip;. the architecture, compiled template code ( C++ template is hard!)\u0026hellip; In a word, Never read like a book.\n\u0026ndash;Cherno, in “BEST WAY to read and understand code”.\n如何阅读代码？你所能做的最差的方法就是像读一本书那样去阅读。人生苦短。八仙过海，各显神通。总之就是不羞于同时以各种方式来更好的了解代码。有一点是共同的：一边调试，一边阅读！\n其他常见的方法列举如下，面对不同类型的项目和不同的目的加以斟选：\n尝试修改源码后跑UT 尝试实现源码的功能，然后对比着看别人的思路 查资料，找关键类或接口 看提交记录，了解一段功能的演变 看设计文档（如有） III It\u0026rsquo;s so important to quickly understand everything going on here (in code base). Thus C has advantage. If you know C you can jump in and not have to learn what paradigms they are using because the simplicity of c. But lisp, you have to restructure your mind before you dare to touch some code\u0026hellip; Almost every case I\u0026rsquo;ve seem when people mixed languages on their project, that\u0026rsquo;s an mistake. I would stay just in one language so every body can work on it.\n\u0026ndash;Carmack, interview with Lex Fridman.\n语言特性的简洁、框架的简洁会带来一个优势：任何人都可以jump in去了解并解决问题。人的脑容量是有限的，解决问题前需要做的准备工作、或者说了解的上下文越少越好。与此同理的一条是：项目采用单一语言开发的优势可能比想象中还要大。\nIV I’d much rather have a coding convention where wrong code, at least, looked wrong. You instantly gain the incremental benefit that every time a programmer’s eyes pass over a line of code, that particular bug is checked for and prevented.\n\u0026ndash;Joel, \u0026ldquo;Make wrong code look wrong\u0026rdquo;\n我想从三个层面聊聊看法。\n编程规范 变量命名 语言设计 一个好的代码风格规范会让错误的代码看起来就很可疑，而不需要任何的认知负担。一个反例是：\nchar *src, dest; 上面这行c代码中，dest的类型其实是char，但你甚至无法区分这是有意为之还是一个错误。另一个反例是；\nif (ok) doSomething(); doMorething(); 上面这段c代码中，doMorething()永远会被执行。它可能是后来添加的代码，但你同样无法区分这是有意为之还是一个错误。你得深入业务逻辑才能理顺。工程师的时间就这样被浪费了。 在微软工作时，有成白上千条实践检验后的代码规范被写入编译器的规则中（如果是C#，可以轻易的通过RoslynCompiler做到），来保证上述有害的代码不会进入仓库，它甚至不能通过你本地编译。\n另外大部分程序员（并不像有意冒犯）对变量或函数命名不得要领，造成理解困难，甚至掩藏了代码中的错误。他们有的误以为字符少认知成本就低，偏爱单字母或缩略语，问其原因大都是linux吹；有的走向另一个极端，即使是局部变量也要把它的来龙去脉反应在名称中，因此语句写的密密麻麻，问其原因是逐字逐句读起来连贯。我认为，起名应当将其语义表达出来的同时力求简洁，如果是函数内的局部变量，那么使用缩略语也可以，只用在定义出加以说明。另外充分利用变量的类型、变量定义的位置、变量注释都是有效的方法增强表达力。\n上升到语言层面。前面也提到，好的语法糖在让表达变得简洁的同时，还能降低认知难度，让代码更不容易产生笔误，这就是语言设计者大大的功劳。另外在某些生产系统中，限制过于灵活的语法造成的理解负担，在原语言基础上封装一层DSL也是有效的方法。\nBeyond Code I My biggest point was everything we are doing really should flow from user value (by metrics + vision). Don\u0026rsquo;t just be pround of some architecture or specific tech or specific code. It\u0026rsquo;s a fun puzzle game but that really should not be a major motivator for you.\n\u0026ndash;Carmack, interview with Lex Fridman.\n以游戏开发者为例说明。在研究技术的同时，不要忘记产品价值才是目的。不要止步于对某一项技术过于崇拜或炫技，那只是一些智力游戏。这话从卡神的口中说出来，相信没有任何一个资深游戏从业者有底气否认了。\n","permalink":"https://pps43.github.io/posts/thinking_of_software_engineering/","summary":"本篇着重于收集编程和软件工程中的洞察和智慧，大部分来自业内顶尖的前辈，例如Joel Spolsky, John Carmack等等，也有个人的思考。当我这些年经历过不","title":"Wisdom of Software Engineering"},{"content":" 之前曾提到过，这本写于1937年的书曾经入选日本教材，并影响了宫崎骏一生的轨迹。他曾写文解释这本书如何触发他思考当时的社会以及未来的世界：（这本书）传达出一个讯息，那就是无论处在多么艰困的时代或是残酷的时代，都要活的像一个人。我读后深感这虽是一本写给少年的读物，读起来很轻快，却非常深刻隽永，令人动容。合上书页，我仿佛也看到那远方的天空、无尽的人们。\n“要成为能帮助世界进步的人（Make the world a better place）”——我再次坚定了内心的信念。\n相信这本书会一直流传下去。豆瓣读书, GoodReads\n舅舅的笔记片段1 所有小孩子的想法都不像地动说，而像天动说。请你观察小孩的知识，他们一切都以自己为中心。电车轨道在我们家左边，邮筒在我们家右边，菜贩在弯进转角的地方。静子家在我们家对面，阿三家在我们家隔壁。小孩子的想法就像这样，一切都以自己家为中心。人是其他人的过程也一样，例如那是我爸爸的同事、这是我妈妈的亲戚……长大之后，想法或多或少的会慢慢接近地动说……人习惯以自己为中心思考、判断事物；即使长大了，这种特性同样根深柢固。等你长大了就会明白，在广大的世界上，能完全改掉以自己为中心思考的习惯的人，其实非常稀少。尤其碰到事关利害得失的时候，要跳脱自己的立场作出正确的判断，实在很难。面对利害得失如果还能抱持像哥白尼那样的想法，可说是非常了不起。……如果人一直坚信地球是宇宙的中心，恐怕就无法了解宇宙真正的样貌，同样地，总是以自我为中心判断事物，也会让人不了解周遭事物的真相。这种人一定看不清重大的真理。\n……对这些事情有了不起的想法，才能成为了不起的人。话虽如此，没有人能简单地用一句“这个社会是这么回事，人活在社会上有这样的意义”向你说明。好吧，就算有人能说明，关于这种事情，光听别人说明，觉得自己懂了，也无法立刻吸收。……不管别人怎么说明，只有喝过的人才知道水真正的味道。……光有眼睛和耳朵还不够，必须张开懂得欣赏的心灵之眼和心灵之耳才行。而且，要张开心灵之眼和心灵之耳，必须实际接触卓越的作品，深刻地感动过才行。想了解人活在世界上有什么意义，更需要靠你实实在在地活着，在人生中扎扎实实地体会，如果自己不曾亲自体会，再怎么响叮当的大人物也教不来。……不是别人，就是你自己。你自己活着，凭着自己感受到的一切，才能了解那些伟人话语真正的意涵。学习人生绝对不能像学习数学、科学一样，光靠读书是无法明白的。所以，最重要的是，从自己真正的感受、真切的感动出发，思考其中的涵义。当你心有所感、有什么想法从内心深处涌现时，千万不能有一丝敷衍。一定要想清楚，自己在什么情况下、对什么事情、有什么感觉。这样一来，你就会慢慢明白，自己在某个时间、某个地方受到感动，这种无法重复的唯一经验，其实具有不限于那个时刻的意义。这就是真正属于你的思想。\n我和你母亲都打从心底希望你能成为了不起的人，而不只是会念书、有礼貌、在老师和朋友眼中没有缺点的中学生。……重要的不是世俗的眼光或其他，而是你自己必须从灵魂深处了解人到底是什么地方了不起。……你必须珍惜自己的感受和深深感动的事。不要忘记这些事，而且要认真思考这些事的意义。\n只要浏览一下你生活中所用的必需品，就可以发现，所有事物的背后都有数不清的人在努力工作着。然而，他们全都是你不曾见过的人。你说，你觉得怪怪的。 世界这么大，我们当然不可能认识所有人。不过，你吃的、穿的、住的——为了制作你需要的一切而辛苦工作的人，以及靠那些东西活着的你，如果真的是毫无瓜葛的陌生人，那么的确如你所说，实在很奇怪。……可是，小哥白尼，不必我说你也知道，人必须活得像人。人活在不像人与人之间的关系当中，实在可惜。即使面对“毫无瓜葛的陌生人”，也应该建立像人的关系。——我这么说，当然不是要你现在立刻采取什么行动。我只是希望等你渐渐长大，能开始认真注意这样的事。这是人类进步到现在的阶段还未解决的问题之一。……人与人发自善意为彼此付出而感到喜悦，是最美好的事。而且，这就是真正的人与人之间的关系。小哥白尼，你应该也同意吧。\n小哥白尼，即使是有自尊心的人，一旦日子过得穷苦，还是难免有点自卑，这是人之常情。所以我们要时时告诫自己，千万不要让那些人更觉得自卑。人最厌恶的莫过于自尊心受伤。生活穷困的人容易尝到这种滋味，所以千万不要因一时大意而伤害了他们容易受伤的自尊心。……照道理说来，就算贫穷，也不必感到自卑。不须我多说你也知道，人真正的价值……不过，除非你也和贫穷的人处于同样的境遇，尝遍贫穷的艰辛，还能保有自信心，顶天立地，否则你就没有资格伤害贫穷的人。\n没错。这的确是错的。既然是人，就该让所有人都过得像人。世界应该是这样的。只要有诚恳善良的心，一定会这么想。不过到目前为止，令人遗憾的是，这个世界还不是这样。虽然人类整体进步了，却还没有达到这样的境界。这都是必须留待以后解决的问题。……即使同样从小学毕业，并非所有人都像你们一样有机会读中学。即使就读同一所中学，如果生长在浦川那样的家庭，也得减少读书时间，帮家里工作。至于你，现在没有任何事妨碍你读书。只要你愿意学习，就能自由学到人类历经几万年努力累积的智慧。既然如此——不，接下来我不须多说，你应该也明白。像你这么幸运的人应该做什么，应该抱持什么念头过日子。即使我不说，你应该也明白。\n在穷苦的环境长大、只上过小学、毕业后就靠劳力赚钱维生的人当中，有很多人长大之后，知识还不如你多。几何、代数、物理这些科目到了中学之后才教，他们往往连非常简单的知识都不知道。对于事物的偏好，也多半不入流。光从这一点看来，你难免会觉得自己比那些人更高一等。不过，换个角度看，他们才是扛着这个世界的人。他们远远胜过你，是很了不起的人。——你想想看，世人生活所需的东西都是人类劳动的产物。即使是做学问、艺术这些高尚的工作所需的东西，同样都是他们付出血汗制作的。没有他们辛苦的劳动，就没有文明，世界也不会进步。 话说回来，你自己呢？你自己创造了什么？\n为生产而付出的劳动让人活得像人。我说的不只是食物、衣服这些东西。即使在学问的世界、艺术的世界，创造的人也远比接受的人更重要。……所以，今后你千万别忘了生产者和消费者的区隔。从这个角度看，你一定会发现，神气巴拉地坐在汽车里、住豪宅的人，反而有很多是没有价值的人。你也一定会发现，被世人瞧不起的人当中，有很多都值得我们尊敬。\n水谷姐姐胜子对话片段 “当然是拿破仑赢了。经过两天激烈的战斗，拿破仑终于击溃敌军。不过，胜负不是重点。”\n“打仗怎么能输。”\n“你还不懂重点。——不论输或臝，英雄就是英雄。不，即使输了却令人感到伟大的人才是真英雄。速夫，亏你是个男孩子，怎么不懂这个道理。”\n胜子假装难过地皱着眉，歪了歪头。她的西瓜皮发型又乱了，头发垂到脸上。然后，胜子好像在思考什么严肃的事，两手插在裤子口袋，在四个男孩面前安静地来回踱步。浦川和北见已经完全被鎮住，目瞪口呆地看着胜子。小哥白尼和水谷互看了一眼。\n“姊姊觉得自己就是拿破仑。”水谷小声地说。小哥白尼瞪大了眼。\n“既然是战争，当然没有人想输。”胜子边走边说。\n“而且每个人都知道生命可贵，也都不想受伤。虽然我还没亲眼见过战争，不过，实际上了战场，应该会觉得非常可怕。任何人第一次上战场，一定会吓得浑身发抖。可是——人心里若有英雄精神在燃烧，就能忘记战场的可怕。内心浮现的勇气能让人超越一切痛苦，甚至不惜牺牲宝贵的生命。我认为这是最了不起的事，因为人能超越人的境界——”\n“嗯——”北见深感佩服。\n“而且，不只是不惜牺牲宝贵的生命。如果只是不怕死，就连那些不顾后果、不要命的暴徒也做得到。逞强、像疯子似的、拿生命开玩笑的人根本没什么了不起。那些人顶多只是和野狗一样。可是，如果一个人不逞强、不发狂、还愿意牺牲生命——我觉得这真的很了不起。\n人类的赞歌是勇气的赞歌。——《 JOJO的奇妙冒险 》\n“如果黑川真的要动手打北见，我们可以告诉他，要打就连我们一起打。如果北见什么坏事都没做也得挨打，那我们就一起挨打。这样一来，黑川应该不会真的动手。”没有人说话。\n“如果黑川还是要动手呢？”胜子问道。\n“那我们就——我们就和北见一起挨打。没办法，只能这样。”\n“浦川，了不起！”胜子从椅子上跳了下来。\n“对，这个方法最好。我们一起保护北见，如果真的保护不了，那也没办法，只好大家和他一起挨打。这就是英雄精神。到时候我也会加入你们的行列。我也会叫爸爸去学校，帮我们谈判。如果爸爸不去，就叫妈妈去，妈妈不去，我自己去。去找校长谈，把那个柔道社的家伙逐出校门。——北见，振作起来。速夫，你也要振作。”\n舅舅的笔记片段2 （拿破仑）他不露半点疲态，时时精神奕奕，不论面临多么困难的局面，都展现不屈的斗志和王者应有的自信优越。——看到一个人能这么强悍、这么活力充沛，任何人都会忍不住惊叹。就连歌德这种热爱人道和平、为人类进步带来希望的大文豪，一谈到拿破仑，也掩不住内心的赞叹，佩服拿破仑源源不断的活动力和天才般的决策力。……所以你们崇拜拿破仑也是理所当然。可是——可是，小哥白尼，你们千万不能忘记，我们赞叹拿破仑的一生是为了他过人的活动力。我们真的忍不住惊叹，原来人可以发挥这么强的实行力。不，不只是惊叹，甚至感到人的坚毅不拔。正因为如此，我们阅读拿破仑传记时才会感到热血沸腾。……可是，在我们甘拜下风的同时，还得大胆质疑：究竟他们靠着非凡的能力成就了什么、他们成就的非凡的事又造福了谁？毕竟，靠着非凡的能力也可能成就非凡的坏事……\n号称英雄、伟人的人物当中，只有对人类进步有贡献的人才真正值得尊敬。在他们非凡的事业当中，只有顺应人类进步所做的事，才真正有价值。\n小哥白尼，你猜发生了什么事。几万名英国人不约而同地脱帽，安静地站着，对他深深致敬。拿破仑打了败仗，在欧洲无处可以容身，被长年以来的宿敌逮住，带回英国，可是他没有一丝沮丧的模样。即使身陷囹圄，依然不失王者风范，毫不畏惧地接受自己的命运。他的气魄打动了数万人的心，让大家肃然起敬。这是多么强大的人格啊。\n——等你长大之后慢慢就会了解，世上有许多和善之人虽有好心肠，却因为性格懦弱而无法发挥善心。有很多人并不是坏人，却因为过于懦弱，反而为自己和他人带来不幸。和人类的进步脱钩的英雄精神固然空虚，缺乏英雄气魄的善良往往也一样空虚。\n只有人才能够感受到的痛苦，是什么样的痛苦？……原本抱着一丝希望却被粉碎，会让我们内心受伤，留着眼睛看不到的血。如果我们的心没有得到体贴的爱，不久之后就会饥渴难耐。……（最痛苦的）是当我们自觉反了无法弥补的过错的时候。\n……不过，小哥白尼，像个男子汉一样承认自己的过错并因此感到痛苦，这是天地之间唯有人才能做得到的事。……我们都该从痛苦中汲取新的自信……“错误相对于真理的关系，正如睡眠之于清醒。我曾见过人从错误中醒来，就像苏醒之后再次朝着真理前进”歌德如是说。\n小哥白尼的片段 （挖水仙）我懂了！即使埋在那么深的地方，它也忍不住要长大。……（听完舅舅讲述第一尊佛像的故事）从希腊到东方世界的最东边，这么遥远的距离——两年前的时光之河——诞生又死去的几十亿人——透过许多民族才得以诞生的诸多美丽文化！这是多么壮阔广大的景观。小哥白尼觉得胸口情绪满溢，感受到莫名的震撼。\n舅舅，我重复看了您的笔记本多次，有些内容对我来说还太难，但我没有跳过任何部分，全部看过。……我真的开始觉得自己必须成为一个好人。就像舅舅所说，我是消费专家，没有生产任何东西。我和浦川不同，即使现在想生产什么，也办不到。但是我能成为一个好人。只要自己成为好人，就能为世界创造一个好人；这我也做得到。我相信，只要打定主意成为一个好人，就有能力为世界创造更多。……我认为将来世界上所有人都应该成为彼此的朋友。人类从以前到现在不断进步，今后一定会进步到那样的世界。而我想成为能帮助世界进步的人。\n","permalink":"https://pps43.github.io/posts/notes_on_how_you_live_your_life/","summary":"之前曾提到过，这本写于1937年的书曾经入选日本教材，并影响了宫崎骏一生的轨迹。他曾写文解释这本书如何触发他思考当时的社会以及未来的世界：（","title":"你想活出怎样的人生（吉野源三郎）"},{"content":" 本文主要介绍PhysX角色控制器相关的内部机制和使用方法。\nPhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code, Doc）的一些经验，有不少对官方资料的补充。\nWarm-up Character Controller (a.k.a CCT) is a special physical object handling player movement. In PhysX, CCT is not a Rigidbody, which means it does not integrate seamlessly in collision system. However, there is a kinematic actor underlying in CCT, and you can attach custom data via PxController::getActor()-\u0026gt;userData.\nGenerally, CCT can be kinematic or dynamic, but according to PhysX document, kinematic controller has below advantages:\nDirect control. For dynamic rigidbody, Adding force/impulse/velocity to move player to final position is impossible. Built-in CCD. No jitter in a corner. No friction to finetune. Standing on a slope requires +inf friction, but walking on a slope without slowing down requires 0 friction. It\u0026rsquo;s difficult to tune. No restitution. For dynamic rigidbody, even with 0 restitution, it will bounce a bit, due to imperferct nature of linear solver, and the way of recovering from penetration. Easy to stick to ground. Easy to keep standup and never rotate. For dynamic rigidbody, it\u0026rsquo;s difficult to really constrain that way. Joints are often used, but with less robustness and speed. Above advantages apply to all kinematic CCT, not exclusive to PhysX\u0026rsquo;s CCT implementation. You can just use a real kinematic rigidbody to implement your own CCT (Ground detection, Collide-and-Slide algorithm), which can be even more flexibile than PhysX\u0026rsquo;s CCT.\nCCT Setup There are two shapes of CCT, AABB and Capsule. Take Capsule as example.\n⚠ In Unity, the height in inspector equals to CCT.height + 2 * CCT.radius.\nTo change height, use setHeight or resize. Skin Width To avoid numerical issue, there is a skin around character, defined by PxControllerDesc::contactOffset. E.g. 0.08, or 10% of the radius. Also called \u0026ldquo;Contact Offset\u0026rdquo;.\nFoot Position Foot position is helpful in some cases, access via PxController::getFootPosition/setFootPosition\nIf need to keep foot position when change height, use PxController::resize.\nUp Direction Up direction can be arbitrary, defined by PxController::setUpDirection().\n⚠ Unity locks up direction to (0,1,0), which makes no sense and they have not changed that for years. Unity Forum\nExample Below is the creation process of a CCT, using the layer defined in \u0026ldquo;PhysX物理引擎（2）Collision\u0026rdquo;.\nbool PhysXManager::AddCCT(ActorWrapper \u0026amp;actor, float radius, float height, float skinWidth, float stepOffset, float slopeLimit, int layer, int layerAgainst) { if (!m_pxControllerManager) { m_pxControllerManager = PxCreateControllerManager(*m_pxScene, true); //m_pxControllerManager-\u0026gt;setOverlapRecoveryModule(true); m_pxControllerManager-\u0026gt;setPreciseSweeps(false); } PxCapsuleControllerDesc desc; desc.scaleCoeff = 1; desc.position = actor.GetPosition(); desc.contactOffset = skinWidth; desc.stepOffset = stepOffset \u0026gt; height ? height : stepOffset; // [0, height] desc.slopeLimit = slopeLimit \u0026gt; 0? slopeLimit : 0; // cos(theta), [0, 1] desc.radius = radius; // [0, ] float h = height - 2 * radius; desc.height = h \u0026lt; 0 ? 0 : h; // [0,] desc.upDirection = PxVec3(0, 1, 0); desc.material = m_pxMaterial; desc.climbingMode = PxCapsuleClimbingMode::eCONSTRAINED; desc.reportCallback = actor.GetCCTHitReportHandler(); //desc.behaviorCallback = NULL; currently we don\u0026#39;t need this //create controller PxController* controller = m_pxControllerManager-\u0026gt;createController(desc); controller-\u0026gt;getActor()-\u0026gt;userData = \u0026amp;actor; actor.SetPxActor(controller); actor.SetActorType(EActorType_CCT); //set query info PxShape* shape = NULL; controller-\u0026gt;getActor()-\u0026gt;getShapes(\u0026amp;shape, 1); if (shape) { PxFilterData filterData; filterData.word0 = layer; shape-\u0026gt;setQueryFilterData(filterData); PxFilterData simFilterData; simFilterData.word0 = layer; simFilterData.word1 = layerAgainst; shape-\u0026gt;setSimulationFilterData(simFilterData); } return false; } CCT Move Rather than PxController::setPosition, PxController::move uses a \u0026ldquo;collide-and-slide\u0026rdquo; algorithm. Internally, it use sweep tests in required direction. If found obstacle, CCT will slide smoothly against it.\nflags = PxController::move(disp, minDist, elapsedTime, filters, obstacles=NULL); name type description flags PxControllerCollisionFlags combination of eCOLLISION_SIDES, eCOLLISION_UP, eCOLLISION_DOWN disp PxVec3 displacement, or delta. You need to apply gravity yourself. minDist PxF32 epsilon for moving. you would better keep it 0. elapsedTime PxF32 how much time passed since last call to move. filters PxControllerFilters customize how CCT collides against the world, and with other CCT. obstacle PxObstacleContext user-defined obstacles (can be moving), only for CCT, without shape object in scene. Collide-And-Slide Here are basic ideas on implementing \u0026ldquo;collide and slide\u0026rdquo; algorithm yourself.\nCall a Sweep from the current position of the CCT shape to its goal position. If no initial overlap is detected, move the CCT shape to the position of the first hit, and adjust the trajectory of the CCT by removing the motion relative to the contact normal of the hit. Repeat Steps 1 and 2 until the goal is reached, or until an Sweep in Step 1 detects an initial overlap. If a Sweep in Step 1 detects an initial overlap, use the Penetration Depth computation function to generate a direction for depenetration. Move the CCT shape out of penetration and begin again with Step 1. Custom Gravity There is no internal Gravity applied on CCT. Calculate yourself and add it when calling cct-\u0026gt;move. You may want to modify up-direction at the same time.\nCCT Interaction Hit Callback ⚠This is called when the CCT moves and hits a shape. This will not be called when a moving shape hits a non-moving CCT.\nHere is how to add hit callback.\nInherit class PxUserControllerHitReport and override its onShapeHit, onControllerHit, onObstacleHit to record hitinfo. E.g., in onShapeHit, you can apply forces to other rigidbody, play sounds, etc. In Unity, it records this and throw OnControllerColliderHit event. class MyControllerHitReport : public PxUserControllerHitReport { public: virtual void onShapeHit(const PxControllerShapeHit\u0026amp; hit); virtual void onControllerHit(const PxControllersHit\u0026amp; hit); virtual void onObstacleHit(const PxControllerObstacleHit\u0026amp; hit) {} // don\u0026#39;t need this void Clear(); bool GetResult(PhysXCCTHitReportList\u0026amp; out); private: PhysXCCTHitReportList m_HitRecords; }; void MyControllerHitReport::onShapeHit(const PxControllerShapeHit\u0026amp; hit) { PhysXCCTHitReport result; PxRigidActor* controllerActor = hit.controller-\u0026gt;getActor(); PxRigidActor* otherActor = hit.shape-\u0026gt;getActor(); result.Controller = controllerActor == NULL? NULL : (ActorWrapper*)controllerActor-\u0026gt;userData; result.Other = otherActor == NULL? NULL : (ActorWrapper*)otherActor-\u0026gt;userData; result.HitNormal = hit.worldNormal; result.HitPosition = physx::toVec3(hit.worldPos); result.Dir = hit.dir; result.Distance = hit.length; m_HitRecords.push_back(result); } void MyControllerHitReport::onControllerHit(const PxControllersHit\u0026amp; hit) { //similiar to onShapeHit } //other functions When creating CharacterController, create a object of above class and assign to PxCapsuleControllerDesc.reportCallback Behaviour Callback This is called after CCT hit different objects to define three different behaviours:\nflag meaning eCCT_CAN_RIDE_ON_OBJECT Travel horizontally with the object it is standing on. eCCT_SLIDE Slide when standing on the object. It can be used to make CCT fall off a platform\u0026rsquo;s edge if you think it\u0026rsquo;s not enough to climb on it. eCCT_USER_DEFINED_RIDE Disable all built-in logic Here is how to add a behaviour callback.\nInherit class PxControllerBehaviorCallback and overide its \u0026ldquo;getBehaviorFlags\u0026rdquo;. Notice there are 3 different functions with same name, used for against shapes, CCTs and internal obstacles. Return above flag. When creating CharacterController, create a object of above class and assign to PxCapsuleControllerDesc.behaviorCallback. CCT vs Rigidbody As for CCT pushes Rigidbody, it\u0026rsquo;s difficult to control by applying force at contact points. You should use onShapeHit just mentioned.\nAs for Rigidbody pushes CCT, as mentioned above, when a CCT is not moving, hitcallback won\u0026rsquo;t even called. There is no official way to solve this, see next section.\nCCT vs CCT Inherit class PxControllerFilterCallback and override its \u0026ldquo;filter(const PxController\u0026amp; a, const PxController\u0026amp; b)\u0026rdquo; to determin if two CCTs can interact.\nIf you simply want two CCTs just overlap each other, return false. Return true means they will collide-and-slide. You can add custom logic using their shapes\u0026rsquo; PxFilterData.\nclass ControllerFilterCallback : PxControllerFilterCallback { public: virtual bool filter(const PxController\u0026amp; a, const PxController\u0026amp; b) { PxShape* shapeA = NULL; a.getActor()-\u0026gt;getShapes(\u0026amp;shapeA, 1); PxShape* shapeB = NULL; b.getActor()-\u0026gt;getShapes(\u0026amp;shapeB, 1); const physx::PxFilterData filterData0 = shapeA-\u0026gt;getQueryFilterData(); const physx::PxFilterData filterData1 = shapeB-\u0026gt;getQueryFilterData(); if ((0 == (filterData0.word0 \u0026amp; filterData1.word1)) || (0 == (filterData1.word0 \u0026amp; filterData0.word1))) { return false; } return true; } } In each CCT.Move, create a object of above class and pass as filter. Golden Tips CCT\u0026rsquo;s slopeLimit must be [0, 1] ([0, 90] in degree). Otherwise PhysX crashes.\nCCT\u0026rsquo;s stepOffset must be [0, height]. Otherwise PhysX crashes.\nUnity does not draw skinWidth in scene view, but it exists as part of volumn.\nUnity locks upvector to (0,1,0) so you can\u0026rsquo;t rotate controller.\nUnity\u0026rsquo;s IsGrounded is NOT reliable. It\u0026rsquo;s just implemented as collisionFlags \u0026amp; eCOLLISION_DOWN::eCOLLISION_DOWN, but very often when running uphill, the flag is eCOLLISION_SIDES so you get wrong result that IsGrounded == false. You should use capsule raycast or anything else to implement your own Ground-Detection algorithm instead. When CCT is not moving, it loses the ability to collide with non-dynamic physical objects. That is, it will penetrate into a kinematic moving platform. There is NO official way to solve it since it\u0026rsquo;s as designed by PhysX. To solve this, add a minor movement every frame on your CCT, e.g., 0.001 * Forward * deltaTime. See more discussion.\n","permalink":"https://pps43.github.io/posts/using_physx_cct/","summary":"本文主要介绍PhysX角色控制器相关的内部机制和使用方法。 PhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code,","title":"PhysX物理引擎（4）Character Controller"},{"content":" 本文主要介绍PhysX刚体动力学相关的内部机制和使用方法。\nPhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code, Doc）的一些经验，有不少对官方资料的补充。\nWarm-up We already know that\nKinematic and Dynamic rigidbody are both PxRigidDynamic in PhysX. Use PxRigidBody::setRigidBodyFlag(PxRigidBodyFlag::eKINEMATIC, true) to turn a dynamic actor into kinematic at runtime, and vice versa. Kinematic and Static actors always stay in the same location unless you move them in your code. When moving Static actors, their collisions with dynamic actors can be wrong. When moving Kinematic actors, you should always use PxRigidDynamic::setKinematicTarget in each frame rather than PxRigidActor::setGlobalPose to achieve correct collisions with other dynamic actors. In this post we focus on dynamic rigidbody movement, e.g., force and torque, gravity, sleeping and so on.\nHere are necessary physical concepts with math formulas.\nTranslation formular Rotation formular Position \\(\\vec{x}\\) Orientation (3x3 matrix) \\(\\mathbf{R}\\) Linear Velocity \\(\\vec{v}=\\frac{d\\vec{x}}{dt}\\) Angular Velocity \\(\\vec{\\omega}=\\frac{\\vec{v}\\times\\vec{r}}{\\lVert{\\vec{r}}\\rVert^2}\\) Linear Acceleration \\(\\vec{a}=\\frac{d\\vec{v}}{dt}\\) Angular Acceleration \\(\\vec{\\alpha}=\\frac{d\\vec{\\omega}}{dt}\\) Mass \\(M=\\sum{m_i}\\) Intertia tensor \\(\\mathbf{I}=\\mathbf{R}\\mathbf{I}_0\\mathbf{R}^T\\) Linear momententum \\(\\vec{p}=M\\vec{v}\\) Angular momententum \\(\\vec{L}=\\mathbf{I}\\vec{\\omega}\\) Force \\(\\vec{F}=\\frac{d\\vec{p}}{dt}=m\\vec{a}\\) Torque \\(\\vec{\\tau}=\\frac{d\\vec{L}}{dt}\\) Setup Rigidbody Dynamic actor has 3 mass related properties: mass, center of mass, inertia tensor.\nLet mass be 0 means it cannot move. Although the doc says 0 is ok for mass, but actually it will cause random crash. See Golden Tips.\nCenter of mass is the position where force applys upon to generate a translation without rotating. It\u0026rsquo;s defined in local space as Vector3, default value is (0,0,0).\nMoment of inertia is a single number that describes how hard it is to rotate an object about a particular axis, while the inertia tensor is a 3x3 matrix that describes how hard it is to rotate an object about any axis. Let inertia tensor be (0,0,0) means it cannot rotate by any axis.\nThe easiest way to calculate mass properties is to always use the PxRigidBodyExt::updateMassAndInertia. You don\u0026rsquo;t need setMassAndUpdateInertia.\nIn Official demo North Pole (PhysX_3.4\\Samples\\SampleNorthPole\\SampleNorthPoleDynamics.cpp), all of them are low center of mass, but with different config to achieve different feeling.\nHere are my code snippet to initialize a dynamic rigidbody.\nvoid ActorWrapper::InitRigidbody(bool useGravity, float mass, const PxVec3\u0026amp; centerOfMass, const PxVec3\u0026amp; interiaTensor, const PxVec3\u0026amp; velocity, float drag, const PxVec3\u0026amp; angularVelocity, float maxAngularVelocity, float angularDrag, int constrainFlags) { PxRigidDynamic* actor = PxActorAs\u0026lt;PxRigidDynamic\u0026gt;(); if (actor == NULL || actor-\u0026gt;getScene() == NULL) { return; } actor-\u0026gt;setActorFlag(PxActorFlag::eDISABLE_GRAVITY, !useGravity); actor-\u0026gt;setMaxAngularVelocity(maxAngularVelocity); //SetConstrains (todo) actor-\u0026gt;setLinearDamping(drag); angularDrag = PxMax(0.01f, angularDrag); // 0 is unstable for angular drag actor-\u0026gt;setAngularDamping(angularDrag); actor-\u0026gt;setMass(mass); actor-\u0026gt;setCMassLocalPose(PxTransform(centerOfMass)); actor-\u0026gt;setMassSpaceInertiaTensor(interiaTensor); // not kinematic if (!(actor-\u0026gt;getRigidBodyFlags() \u0026amp; PxRigidBodyFlag::eKINEMATIC)) { // if has constrain, modify velocity here actor-\u0026gt;setLinearVelocity(velocity); actor-\u0026gt;setAngularVelocity(angularVelocity); } } void ActorWrapper::SetDensity(float density) { PxRigidDynamic* actor = PxActorAs\u0026lt;PxRigidDynamic\u0026gt;(); if (actor == NULL || actor-\u0026gt;getScene() == NULL) { return; } density = PxClamp(density, 0.0f, 1e6f); PxRigidBodyExt::updateMassAndInertia(*actor, density); } Add Force \u0026amp; Torque addForce causes a translation, addTorque causes rotation, and addForceAtPos causes linear and rotation if pos is not the center of mass.\nBoth force and torque support 4 modes:\nPxForceMode physics equivalent eFORCE \\(ma\\) eIMPULSE \\(mat\\) eVELOCITY_CHANGE \\(at\\) eACCELERATION \\(a\\) Here are my code snippet to implement addForce and addTorque:\nvoid ActorWrapper::AddForce(const PxVec3\u0026amp; force, int forceMode) { // validation if (force.IsAllZero() || !force.IsFinite()) { return; } PxRigidDynamic* actor = GetRigidDynamicActor(); if (actor == NULL || actor-\u0026gt;getScene() == NULL || (actor-\u0026gt;getRigidBodyFlags() \u0026amp; PxRigidBodyFlag::eKINEMATIC)) { return; } actor-\u0026gt;addForce(force, forceMode); } void ActorWrapper::AddRelativeForce(const PxVec3\u0026amp; force, int forceMode) { // same validation as AddForce PxVec3 globalForce = actor-\u0026gt;getGlobalPose().rotate(localForce); actor-\u0026gt;addForce(globalForce, forceMode); } void ActorWrapper::AddTorque(const PxVec3\u0026amp; torque, int forceMode) { // same validation as AddForce actor-\u0026gt;addTorque(torque, forceMode); } void ActorWrapper::AddRelativeTorque(const PxVec3\u0026amp; torque, int forceMode) { // same validation as AddForce PxVec3 globalTorque = actor-\u0026gt;getGlobalPose().rotate(torque); actor-\u0026gt;addForce(globalForce, forceMode); } void ActorWrapper::UGCAddForceAtPosition(const PxVec3\u0026amp; force, int forceMode, const PxVec3\u0026amp; position) { // same validation as AddForce if (!position.IsFinite()) { return; } // Only eFORCE and eIMPULSE are supported! if (forceMode == PxForceMode::eFORCE || forceMode == PxForceMode::eIMPULSE) { PxRigidBodyExt::addForceAtPos(*actor, force, position); } } Change Gravity Gravity is scene-wide for dynamic rigidbodies in the scene, by PxScene::setGravity().\nWe can let some dynamic actors are not influenced by scene-wide gravity by PxActor::setActorFlag(PxActorFlag::eDISABLE_GRAVITY,true). Then addForce each frame manually to make your customized gravity on this actor.\nSleep \u0026amp; Awake Why Sleep Matters Sleeping rigidbodies almost cost nothing. You can put less important rigidbodies to sleep, which can significantly lower CPU cost when you have thousands of them.\nSleep Mechanism An actor goes to sleep when: its mass-normalized kinetic energy (a.k.a. \\(\\frac{1}{2}v^2\\)) is below a given threshold for a certain time (Internally they use a wake-counter, when counter reaches 0, actor is a candidate to sleep).\nDefault threshold is \\(5∗10^{−5}∗v^2\\), where \\(v\\) is PxTolerancesScale.velocity. Thus the logic of the formular is that when actor\u0026rsquo;s velocity is below 1% of PxTolerancesScale.velocity, they are allowed to go to sleep. Set threshold by PxRigidDynamic::setSleepThreshold\nYou can also set wake-counter value to control sleeping, by PxRigidDynamic::setWakeCounter.\nCommon APIs about sleeping (only for dynamic actors!):\nName Notes PxRigidDynamic::setSleepThreshold PxRigidDynamic::setWakeCounter Calling on a sleeping rigidbody will auto-wakeup PxRigidDynamic::isSleeping() PxRigidDynamic::wakeUp() Force wakeup. PxRigidDynamic::putToSleep() Force sleep. PxSimulationEventCallback::onWake/onSleep To receive these events, set flag on actor: PxActorFlag::eSEND_SLEEP_NOTIFIES Awake Mechanism Overall, these actions wake an actor up:\nPxRigidDynamic::setKinematicTarget() for kinematic actor. PxRigidActor::setGlobalPose(), if the autowake parameter is set to true (default). Raising PxActorFlag::eDISABLE_SIMULATION Calling PxScene::resetFiltering(). Calling PxShape::setSimulationFilterData() and cause a different filtering result. Touch with an actor that is awake. A touching rigid actor gets removed from the scene. Contact with a static rigid actor is lost. Contact with a dynamic rigid actor is lost (awake in the next simulation step). Actor gets hit by a two-way interaction particle Golden Tips When calling PxRigidBodyExt::setMassAndUpdateInertia(actor, mass), make sure mass is above zero for dynamic rigidbody, otherwise it crashes randomly when collision happens. When calling PxRigidDynamic::setAngularDamping(value), make sure value is above zero otherwise it goes unstable for rotating. Changing scene-wide gravity value will NOT auto-wake sleeping rigidbody. Call PxRigidDynamic::wakeUp() manually if required. ","permalink":"https://pps43.github.io/posts/using_physx_rigidbody/","summary":"本文主要介绍PhysX刚体动力学相关的内部机制和使用方法。 PhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code,","title":"PhysX物理引擎（3）Rigidbody Dynamics"},{"content":" 本文主要介绍PhysX碰撞检测的一些内部机制和使用方法。\nPhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code, Doc）的一些经验，有不少对官方资料的补充。\nWarm-up Static, Kinematic \u0026amp; Dynamic Static colliders are non-movable. In fact, they are not rigidbody, just PxRigidStatic. Kinematic and dynamic rigidbody are both PxRigidDynamic, and can switch to each other at runtime by setRigidBodyFlag(PxRigidBodyFlag::eKINEMATIC, true/false). The biggest difference is that kinematic rigidbody behaves like infinite mass, and will not move by external force. Instead, you call MovePosition on it. Dynamic rigidbody is the only type we can AddForce to, which has mass, center of mass, and inertia tensor to simulate a natural movement with Newton\u0026rsquo;s laws of motion. classDiagram PxRigidActor \u0026lt;-- PxRigidStatic PxRigidActor \u0026lt;-- PxRigidBody PxRigidBody \u0026lt;-- PxRigidDynamic PxRigidActor *.. PxShape class PxRigidBody { PxRigidBodyFlag } class PxShape { PxShapeFlag } We cannot make a rigidbody without a shape. Shapes are tangible, with a real size.\nOne actor can have multiple shapes as children, each with a local position and rotation.\nEach shape have a flag PxShapeFlag::eSIMULATION_SHAPE to determine it\u0026rsquo;s a collider or a trigger.\nWhen two shapes overlap (collide), a \u0026ldquo;triggering event\u0026rdquo; or a \u0026ldquo;contacting event\u0026rdquo; may raise.\nWhen two shapes are both collider, the event is contacting, otherwise triggering. Contacting event happens only when at least one of the shape\u0026rsquo;s belonging actor is Non-Kinematic PxRigidDynamic. Triggering event happens only when at least one of the shape\u0026rsquo;s belonging actor is PxRigidbody. Two shapes must pass a filtering logic, which is usually user-defined rules. This is called \u0026ldquo;collision filtering\u0026rdquo;. Unity Engine also has a doc to explain these rules, see here.\nFilter mechanism Basically, collision filter decides whether a collision pair should get processed, temporarily ignored or discarded.\nHistorically, there are 3 different implementation in PhysX.\nThe simpliest way is to use filter callback function to determine if two object can interact. Flexible, but too expensive/slow. Next is Simple group based filtering rules in PhysX 2.0 (Dividing actors into predefined groups and decide if two groups can collide). Not flexible enough. Now is Shader system+CPU callback in Physx 3+. Users can tradeoff between speed and flexiblity. Shader System, basically, is a custom function injected to the PxSceneDesc, and called for all pairs of shapes that come near each other \u0026ndash; more precisely: for all pairs of shapes whose AABB in world space are found to intersect for the first time.\nShader System runs on the vector processor and is therefore not able to access any eventual game data base in main memory, so any pointer dereference is FORBIDDEN.\nSimulation loop To put it simple, here is what happens each frame:\nvoid FixedUpdate(float timestep) { if (m_MySimCallback) { m_MySimCallback-\u0026gt;Clear(); // see later in Collision Callback section } if (m_pxScene) { m_pxScene-\u0026gt;simulate(timeStep, NULL, m_ScratchBlock, SCRATCH_BLOCK_SIZE); // your shader function are called inside simulate, when AABB intersect each other. m_pxScene-\u0026gt;fetchResults(true); // your CPU callbacks are called inside fetchResults. } } See details on Official Manual: Simulation and Broadphase algorithm\nSetup Rigidbody To create a static actor:\npxActor = sPxPhysics-\u0026gt;createRigidStatic(actorTransform); pxActor-\u0026gt;setName(\u0026#34;xxx\u0026#34;); pxActor-\u0026gt;userData = \u0026amp;myActorWrapper; // bind to your game actor m_pxScene-\u0026gt;addActor(*pxActor); To create a dynamic/kinematic actor:\npxActor = sPxPhysics-\u0026gt;createRigidDynamic(actorTransform); pxActor-\u0026gt;setName(\u0026#34;xxx\u0026#34;); pxActor-\u0026gt;userData = \u0026amp;myActorWrapper;// bind to your game actor m_pxScene-\u0026gt;addActor(*pxActor); ((PxRigidDynamic*)pxActor)-\u0026gt;setRigidBodyFlag(PxRigidBodyFlag::eENABLE_CCD, true); // set more property, e.g., linear damping, actor flag... if (isKinematic) { ((PxRigidDynamic*)pxActor)-\u0026gt;setRigidBodyFlag(PxRigidBodyFlag::eENABLE_CCD, false); ((PxRigidDynamic*)pxActor)-\u0026gt;setRigidBodyFlag(PxRigidBodyFlag::eKINEMATIC, true); } To set collision data on each shape for filtering later:\nPxFilterData collisionFilter; collisionFilter.word0 = layer; // ePlayer collisionFilter.word1 = layerAgainst; // eGroud | eWall | eObstacle shape-\u0026gt;setSimulationFilterData(collisionFilter); if (isTrigger) { shape-\u0026gt;setFlag(PxShapeFlag::eSIMULATION_SHAPE, false); shape-\u0026gt;setFlag(PxShapeFlag::eTRIGGER_SHAPE, true); } To register collision filtering and collision callback to all rigidbodies (when creating a PxScene):\nm_MySimCallback = new MySimulationEventCallback(); PxSceneDesc sceneDesc(sPxPhysics-\u0026gt;getTolerancesScale()); ... sceneDesc.filterShader = MyFilterShader; sceneDesc.simulationEventCallback = m_MySimCallback; m_pxScene = sPxPhysics-\u0026gt;createScene(sceneDesc); Collision Filtering Just define a function that meets type PxSimulationFilterShader (function pointer).\nPxFilterFlags MyFilterShader( PxFilterObjectAttributes attributes0, PxFilterData filterData0, PxFilterObjectAttributes attributes1, PxFilterData filterData1, PxPairFlags\u0026amp; pairFlags, const void* constantBlock, PxU32 constantBlockSize) { // for triggers if (PxFilterObjectIsTrigger(attributes0) || PxFilterObjectIsTrigger(attributes1)) { pairFlags = PxPairFlag::eTRIGGER_DEFAULT; // here I do not examine PxFilterData and just let it pass return PxFilterFlag::eDEFAULT; } else // for collisions { // Use word0 as self layer, word1 as layerAgainst. // word2 and word3 are not used here. // In most cases, collision rules are symmetric. // Here we use `||` just in case rules are asymmetric, which is equal to \u0026#34;Collision only happens if A can collide with B and B can collide with A.\u0026#34; if ((0 == (filterData0.word0 \u0026amp; filterData1.word1)) || (0 == (filterData1.word0 \u0026amp; filterData0.word1))) { return PxFilterFlag::eSUPPRESS; // no collide } // use contact solver, and use CCD detection pairFlags = PxPairFlag::eCONTACT_DEFAULT | PxPairFlag::eDETECT_CCD_CONTACT; // notify onContact event pairFlags |= PxPairFlag::eNOTIFY_TOUCH_FOUND | PxPairFlag::eNOTIFY_TOUCH_CCD | PxPairFlag::eNOTIFY_TOUCH_LOST; return PxFilterFlag::eDEFAULT; } } PxFilterObjectAttributes param is 32bit flag.\nvalue meaning 0 eRIGID_STATIC 1 eRIGID_DYNAMIC \u0026hellip; \u0026hellip; 15 as low mask 1\u0026laquo;4 eKINEMATIC 1\u0026laquo;5 eTRIGGER PxFilterFlags is return value tells PhysX what to do next.\nenum value meaning eDEFAULT 0 Processed with NO filter callback eKILL 1\u0026laquo;0 Ignore the collision pair as long as the bounding volumes of the pair objects overlap. Until one of these conditions are met:The bounding volumes of the two objects overlap again (after being separated) The user enforces a re-filtering eSUPPRESS 1\u0026laquo;1 Similar to eKill, except one more condition that filter data or attribute changes.✅Usually use this. eCALLBACK 1\u0026laquo;2 Use PxSimulationFilterCallback::pairFound(). ⚠SLOW! eNOTIFY 1\u0026laquo;3 | eCALLBACK Besides eCALLBACK, plus pairLost(). ⚠SLOW! PxPairFlags is modifiable, to provide more info on what to do next.\nenum value meaning eSOLVE_CONTACT 1\u0026laquo;0 need process contact for this pair eMODIFY_CONTACTS 1\u0026laquo;1 eNOTIFY_TOUCH_FOUND 1\u0026laquo;2 Raise event when \u0026ldquo;enter\u0026rdquo; eNOTIFY_TOUCH_PERSISTS 1\u0026laquo;3 Raise event when \u0026ldquo;stay\u0026rdquo; eNOTIFY_TOUCH_LOST 1\u0026laquo;4 Raise event when \u0026ldquo;exit\u0026rdquo; \u0026hellip; \u0026hellip; eNOTIFY_CONTACT_POINTS 1\u0026laquo;9 Add contact points info eDETECT_DISCRETE_CONTACT 1\u0026laquo;10 Discrete detection algorithm. Valid when eSOLVE_CONTACT eDETECT_CCD_CONTACT 1\u0026laquo;11 Continous detection algoritym. Valid when eSOLVE_CONTACT, PxSceneFlag::eENABLE_CCD, and PxRigidBodyFlag::eENABLE_CCD are all set. \u0026hellip; \u0026hellip; eCONTACT_DEFAULT \u0026hellip; eSOLVE_CONTACT | eDETECT_DISCRETE_CONTACT eTRIGGER_DEFAULT \u0026hellip; eNOTIFY_TOUCH_FOUND | eNOTIFY_TOUCH_LOST | eDETECT_DISCRETE_CONTACT Collision Callback This is also how Unity OnTriggerEnter/Exit and OnCollisionEnter/Stay/Exit are based on.\nCreat a new class inheriting PxSimulationEventCallback and override its functions. In below example, we only implement onTrigger and onContact.\nclass MySimulationEventCallback : public PxSimulationEventCallback { public: //from PxSimulationEventCallback //ignore virtual void onConstraintBreak(PxConstraintInfo* constraints, PxU32 count) {}; virtual void onAdvance(const PxRigidBody*const* bodyBuffer, const PxTransform* poseBuffer, const PxU32 count) {}; virtual void onWake(PxActor** actors, PxU32 count){}; virtual void onSleep(PxActor** actors, PxU32 count){}; //implement virtual void onTrigger(PxTriggerPair* pairs, PxU32 count); virtual void onContact(const PxContactPairHeader\u0026amp; pairHeader, const PxContactPair* pairs, PxU32 nbPairs); public: // clear m_TriggerRecords and m_ContactRecords. Called before every time you called m_pxScene-\u0026gt;simulate void Clear(); // Copy m_TriggerRecords. Called after every time you called m_pxScene-\u0026gt;fetchResults(true) bool GetTriggerResult(PhysXArray\u0026lt;MyTriggerResult\u0026gt;\u0026amp; out); // Copy m_ContactRecords. Called after every time you called m_pxScene-\u0026gt;fetchResults(true) bool GetContactResult(PhysXArray\u0026lt;MyContactResult\u0026gt;\u0026amp; out); private: PhysXArray\u0026lt;MyTriggerResult\u0026gt; m_TriggerRecords; PhysXArray\u0026lt;MyContactResult\u0026gt; m_ContactRecords; }; void MySimulationEventCallback::onTrigger(PxTriggerPair* pairs, PxU32 count) { for (PxU32 i = 0; i \u0026lt; count; ++i) { const PxTriggerPair\u0026amp; curPair = pairs[i]; // ignore pairs when shapes have been deleted if (curPair.flags \u0026amp; (PxTriggerPairFlag::eREMOVED_SHAPE_TRIGGER | PxTriggerPairFlag::eREMOVED_SHAPE_OTHER)) continue; MyTriggerResult result; result.Trigger = (MyActor*)curPair.triggerActor-\u0026gt;userData; result.Collider = (MyActor*)curPair.otherActor-\u0026gt;userData; result.Lost = (curPair.status == PxPairFlag::eNOTIFY_TOUCH_LOST); // onTriggerEnter/Exit m_TriggerRecords.push_back(result); } } void MySimulationEventCallback::onContact(const PxContactPairHeader\u0026amp; pairHeader, const PxContactPair* pairs, PxU32 nbPairs) { if (pairHeader.flags \u0026amp; PxContactPairHeaderFlag::eREMOVED_ACTOR_0 || pairHeader.flags \u0026amp; PxContactPairHeaderFlag::eREMOVED_ACTOR_1) { return; } PxRigidActor* actorA = (PxRigidActor*)pairHeader.actors[0]; PxRigidActor* actorB = (PxRigidActor*)pairHeader.actors[1]; for (PxU32 i = 0; i \u0026lt; nbPairs; i++) { const PxContactPair\u0026amp; curPair = pairs[i]; // ignore pairs when shapes have been deleted if (curPair.flags \u0026amp; (PxContactPairFlag::eREMOVED_SHAPE_0 | PxContactPairFlag::eREMOVED_SHAPE_1)) { continue; } if (curPair.events \u0026amp; PxPairFlag::eNOTIFY_TOUCH_PERSISTS) { // do nothing when contact persists } else { MyContactResult result; result.Lost = curPair.events \u0026amp; PxPairFlag::eNOTIFY_TOUCH_LOST; // OnCollisionEnter/Exit result.ColliderA = actorA == NULL ? NULL : (MyActor*) actorA-\u0026gt;userData; result.ColliderB = actorB == NULL ? NULL : (MyActor*) actorB-\u0026gt;userData; //result.ContactPoints= //need open eNOTIFY_CONTACT_POINTS if (result.ColliderA != NULL \u0026amp;\u0026amp; result.ColliderB != NULL) { m_ContactRecords.push_back(result); } } } } Golden Tips PhysX can only handle maximum 65535 total interactions on ONE actor in broadphase. Since it\u0026rsquo;s in broadphase, \u0026ldquo;total interactions\u0026rdquo; is regardless of actor type or filter flags.\nWhen implementing onContact method,\nMust check actors\u0026rsquo; state via PxContactPairHeaderFlag::eREMOVED_ACTOR_0 and eREMOVED_ACTOR_1 Must check shapes\u0026rsquo; state via PxContactPairFlag::eREMOVED_SHAPE_0 and eREMOVED_SHAPE_1 Must access actor via pairHeader.actors, NOT pairs[i].shapes[0]-\u0026gt;getActor(), which may be invalid. Be careful about whether you bind userData to shape, or actor. ","permalink":"https://pps43.github.io/posts/using_physx_collision/","summary":"本文主要介绍PhysX碰撞检测的一些内部机制和使用方法。 PhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code, D","title":"PhysX物理引擎（2）Collision"},{"content":" 本文是PhysX物理引擎系列的番外篇，其实要弄明白一个3D数学问题：如何处理父节点带有非均匀缩放和旋转时，子节点的最终大小和形态。 问题源自笔者在修改物理引擎为其添加scale属性时遇到的一个bug。解决后对WorldScale为什么叫做LossyScale、空间变换和基变换有了更深的理解。\n背景 PhysX引擎中，场景内的Actor之间并没有父子层级关系，仅有的层级是Shape可以绑定到Actor作为子节点。 PhysX引擎中并没有Scale的概念，即PxTransform只包含Position和Rotation，而大小只反映在最底层Shape的尺寸上（比如球形碰撞盒有半径这个属性）。所以设置缩放比例的实现方式是改变物体尺寸。 问题的出现 Actor下可以有若干Shape，这里只讨论一个Shape。增加属性Actor.Scale，修改该属性时要保证Shape.Dimension的正确性。\nclassDiagram direction LR class Actor { PxVec3 Position PxQuat Rotation PxVec3 Scale } class Shape { PxVec3 LocalPosition PxQuat LocalRotation PxVec3 OriginalDimension PxVec3 Dimension } Actor o-- Shape 经实践，更方便的方式是记录Shape的初始尺寸OriginalDimension，然后乘上改变Actor.Scale后对应Shape的“缩放系数”。而不是为了节省存储通过Actor.Scale改变前后的比例来计算。总之，问题简化为实现GetShapeScale()。\nShape.Dimension = Shape.OriginalDimension * GetShapeScale(); 当Shape相对Actor没有旋转，即Shape.LocalRotation = (0,0,0,1)时，容易发现：\nPxVec3 GetShapeScale() { return actor.Scale; } 但是当同时存在旋转和非均匀缩放呢？简单来说，当Actor.Scale=(1,4,1)，而Shape绕z轴转了90度，那么预期的结果应该是(4,1,1)，即Shape相对于自己在横向上扩大到2倍。若绕z轴转了45度，那么预期的结果是(2,2,1)。要如何达到这种效果呢？\n似是而非的算法 一个很自然的想法是，要达到上面的效果，其实是将Actor.Scale像方向矢量那样旋转到Shape空间内。\nPxVec3 GetShapeScale() { PxTransform shapeSpace = shape-\u0026gt;getLocalPose(); return shapeSpace-\u0026gt;rotate(actor.Scale); } 然而反例是：actorScale=(1,1,1)经过旋转后可能不再是(1,1,1)，即Shape叠加了一个缩放。这是与事实违背的。针对(1,1,1)特殊处理也并不正确，因为对于任意(X,Y,Z)，总有一种旋转让其某个分量为0。\n这种算法的错误之处在最后一节会额外讨论。\n正确的算法 在Unity的文档中，对于Transform.LossyScale这样说明：\nThe global scale of the object (Read Only).\nPlease note that if you have a parent transform with scale and a child that is arbitrarily rotated, the scale will be skewed. Thus scale can not be represented correctly in a 3 component vector but only a 3x3 matrix. Such a representation is quite inconvenient to work with however. lossyScale is a convenience property that attempts to match the actual world scale as much as it can. If your objects are not skewed the value will be completely correct and most likely the value will not be very different if it contains skew too.\n理解，但不完全理解。直到找来源码分析了一番。去粗取精，根据代码提炼出公式：\n$$ R_{world}=R_1R_2\u0026hellip;R_N $$ $$ W_{world}=R_1S_1R_2S_2\u0026hellip;R_NS_N $$ $$ S_{world}=R_{world}^{-1}W_{world} $$ $$ s = diag(S_{world}) $$\n上式中，\\(1\u0026hellip;N\\)是根节点到叶子节点的编号，所有矩阵均采用列优先矩阵。\\(R_i\\)是只包含自身旋转信息的3x3旋转矩阵。\\(S_i\\)是质保函自身缩放信息的3x3对角矩阵。最终结果\\(s\\)是3x1列矢量，取自\\(S_{world}\\)的对角线元素。\n算法推导与解释 为什么是这样呢？这要从TRS变换矩阵说起。在3D中间中的姿态、运动和坐标系都可以用矩阵表达。 一般\\(T\\)表示位移，\\(R\\)表示旋转，\\(S\\) 表示缩放。\n贴心提示：\n用欧拉角表示则需要规定旋转轴次序否则有歧义（感兴趣可以搜索万向节死锁）。经过实验Unity使用YXZ，即对于\\((\\theta_x,\\theta_y,\\theta_z)\\)，先按照Y轴转 \\(\\theta_y\\) ，再按照转动后的X轴转\\(\\theta_x\\)，再按照转动后的Z轴转\\(\\theta_z\\)。 有时使用4x4而不是3x3矩阵只是一个数学上的技巧，为了让所有变换都可以用矩阵乘法串联起来。 有时使用分块矩阵也只是一个数学上的技巧，为了简化公式发现规律。 对于列优先矩阵，将列矢量\\(v_1\\)先按照\\(M_1\\)再按照\\(M_2\\)变换到\\(v_2\\)写作\\(v_2=M_2M_1v_1\\)。 更多资料如《游戏引擎架构》、任何讲解3D游戏开发或图形学的书籍。 $$ T= \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; Tx \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; Ty \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; Tz \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}= \\begin{pmatrix} I \u0026amp; \\bar{T}\\\\ 0 \u0026amp; 0 \\end{pmatrix} $$\n$$ R_x= \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\cos\\theta_x \u0026amp; -\\sin\\theta_x \u0026amp; 0 \\\\ 0 \u0026amp; \\sin\\theta_x \u0026amp; \\cos\\theta_x \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\n$$ R_y= \\begin{pmatrix} \\cos\\theta_y \u0026amp; 0 \u0026amp; \\sin\\theta_y \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ -\\sin\\theta_y \u0026amp; 0 \u0026amp; \\cos\\theta_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\n$$ R_z= \\begin{pmatrix} \\cos\\theta_z \u0026amp; -\\sin\\theta_z \u0026amp; 0 \u0026amp; 0 \\\\ \\sin\\theta_z \u0026amp; \\cos\\theta_z \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} $$\n$$ R = R_zR_xR_y= \\begin{pmatrix} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; 0 \\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; 0 \\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}= \\begin{pmatrix} \\bar{R} \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} $$\n$$ S= \\begin{pmatrix} s_x \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_y \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_z \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}= \\begin{pmatrix} \\bar{S} \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} $$\n则对于一个节点，其本地坐标系的TRS变换矩阵可以写作：\n$$ M=\\begin{pmatrix} r_{11}S_x \u0026amp; r_{12}S_y \u0026amp; r_{13}S_z \u0026amp; T_x \\\\ r_{21}S_x \u0026amp; r_{22}S_y \u0026amp; r_{23}S_z \u0026amp; T_y \\\\ r_{31}S_x \u0026amp; r_{32}S_y \u0026amp; r_{33}S_z \u0026amp; T_z \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}= \\begin{pmatrix} \\bar{R}\\bar{S} \u0026amp; \\bar{T} \\\\ 0 \u0026amp; 1 \\end{pmatrix} $$\n则对于父子节点\\(M_1\\)中的子节点\\(M_2\\)，其相对于世界坐标系的变换矩阵可以写作：\n$$ M=M_2M_1= \\begin{pmatrix} \\bar{R_2}\\bar{S_2} \u0026amp; \\bar{T_2} \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\bar{R_1}\\bar{S_1} \u0026amp; \\bar{T_1} \\\\ 0 \u0026amp; 1 \\end{pmatrix}= \\begin{pmatrix} \\bar{R_2}\\bar{S_2}\\bar{R_1}\\bar{S_1} \u0026amp; \\bar{R_2}\\bar{S_2}\\bar{T_1}+\\bar{T_2} \\\\ 0 \u0026amp; 1 \\end{pmatrix} $$\n从另一个角度思考，若要将\\(M\\)拆分成TRS三个分量，是否就对应世界坐标系下的位移、旋转、缩放（全局缩放正是我们所需要的）呢？容易观察到全局位移是：\n$$ T_{world}= \\begin{pmatrix} \\bar{R_2}\\bar{S_2}\\bar{T_1}+\\bar{T_2} \\end{pmatrix} $$\n而全局旋转由于其物理意义，必然是：\n$$ R_{world}= \\bar{R_2}\\bar{R_1} $$\n则全局缩放只能是：\n$$ S_{world}=R_{world}^{-1}\\bar{R_2}\\bar{S_2}\\bar{R_1}\\bar{S_1}=(\\bar{R}_1^{-1}\\bar{S_2}\\bar{R_1})\\bar{S_1} $$\n一般来说，\\(S_{world}\\)是一个非对角矩阵，即主对角线之外也有非零值。若只将主对角线元素取出来作为scale，则缩放信息是有损的，这便是Unity中LossyScale名字的由来。\n当父节点是均匀缩放时（即\\(\\bar{S_2}=s_2I\\)），缩放系数可以化简为\\(S_{world}=\\bar{S_2}\\bar{S_1}=[s_x,s_y,s_z]^T\\)，则整体世界变换可以独立分解为TRS三个维度分别做世界变换的组合！而且此时\\(S_{world}\\)是一个对角矩阵，即LossyScale包含了完整的缩放信息。\n$$ M_{world}=T_{world}R_{world}S_{world} $$\n问题的终结 回到最初的问题上来，对Actor设置缩放时要更新所属Shape的尺寸，为此要实现的GetShapeScale()完整版：\nShape.Dimension = Shape.OriginalDimension * GetShapeScale(); PhysXVec3 GetShapeScale() { PxQuat worldRot = actor.Rotation * shapeTransform.q; PxMat33 inverseWorldRotMat = PxMat33(worldRot.getConjugate()); PxMat33 localRotMat = PxMat33(shapeTransform.q); PxMat33 actorRotMat = PxMat33(actor.Rotation); PxMat33 actorScaleMat = PxMat33().createDiagonal(actor.Scale); PxMat33 worldRotAndScaleMat = actorRotMat * actorScaleMat * localRotMat /* * localScaleMat*/; // localScaleMat is always Identity Matrix, so we omit it PxMat33 worldScaleMat = inverseWorldRotMat * worldRotAndScaleMat; PhysXVec3 worldscale = PhysXVec3(worldScaleMat[0][0], worldScaleMat[1][1], worldScaleMat[2][2]); // can have extra logic to make sure all values in worldscale are non-zero return worldscale; } 额外的讨论 已知正确的算法是：\n$$ S_{world}=(\\bar{R}_1^{-1}\\bar{S_2}\\bar{R_1})\\bar{S_1} $$\n而似是而非的算法其实是：\n$$ S_{world}=(\\bar{R_1}s_2)\\bar{S_1} $$\n本质错误在于：\n正确的算法是将缩放本身视为一个空间变换\\(S_2\\)，进行基变换。 错误的算法是将缩放系数视为一个普通矢量\\(s_2\\) ，进行空间变换。 这些在大一的课堂上早已学过。往往正向解释很简单，难的是反向思考，即遇到实际问题怎么选择合适的概念去解决。\n纸上得来终觉浅，绝知此事要躬行。\n","permalink":"https://pps43.github.io/posts/thinking_of_lossyscale/","summary":"本文是PhysX物理引擎系列的番外篇，其实要弄明白一个3D数学问题：如何处理父节点带有非均匀缩放和旋转时，子节点的最终大小和形态。 问题源自笔","title":"深入 LossyScale"},{"content":"这本书带给我的，除了诸多耳熟能详的作品背后的逸闻趣事、诸位大师的才华风貌，还有不少对创作思路、团队协作、宣传发行的启发，相信能常看常新。同时这本书里记录的东西，让人有一种难以名状的触动。一定要形容的话，可以说是一种谁也抑制不了的、忍不住要生长的气魄 1。\n这种气魄和是否是“天才”无关。“才能这种东西，本来就是该靠自己挖掘创造的。我也不是什么天才，我只是比任何人都拼命工作，一步一个脚印走过来了。那帮懒惰的人只顾在山脚念叨着：谁叫那家伙是天才。开什么玩笑……” 2。\n那么，这些人类究竟是抱着怎样的信念，可以如此不知疲倦的创作，打磨，再创作的呢？也许这一切可以从影响宫老先生一生的一本书中找到答案 3。而那个答案，足以让每个还有赤子之心的人相见恨晚。总之，先让我们回到铃木笔下的吉卜力往事上来吧。本来只想摘一些只言片语，结果越摘越多，把书又看了一遍……\n铃木敏夫其人 本书的作者，也是吉卜力社长及多部动画的制作人。人称知道吉卜力秘密最多的男人、天才背后的策划师（驯兽师）。看完书有人感叹地总结到：一生操心的铃木君，一边要督促懒惰随性的的高畑老爷子，一边要哄傲娇的控制狂宫先生）。下面是一些书摘。\n不打寻常牌的铃木：\n我跟龟山商量说‘仅靠我们两个肯定不行，得找个人来当拉拉队长’，然后盯上了时任宣传部长的和田丰。他这个人喜欢赌博，我心想可以陪他玩一晚上骰子，两人各输5万日元，这样他就会为我们工作了。……当时在出版社工作的都是些无法适应社会的“混混”，而赌博是这类人的必修课……我发了疯的练习，也渐渐悟出了“输”的窍门。……反正现在时效已到，说出来也没关系了，电影《风之谷》诞生的契机就是那场赌博。\n我见宫崎骏哭成那样，只好去说服高畑。可他翻来覆去还是那句‘不想接’。无奈之下，我这辈子第一次也是最后一次抬高嗓门冲他吼道……大概很少有人敢这么吼他，这招立竿见影，他终于答应了我。一旦下定决心，高畑立刻变身成一位现实主义者，张口就问在哪里做。\n制作《哈尔的移动城堡》时，宫先生小脾气上来了，坚持不要搞多余的宣传，对此铃木的反应：\n被宫先生这么一说，我就想到了“无为而宣”的策略。……第一轮报导只用了城堡的素材，外加一句“这座城堡会动”的文案。没想到这样反而引起热议。此外，电影由于种种原因推迟，媒体的报道和猜测也为这部电影推波助澜。公布配音阵容的时候，更是引起轩然大波：木村拓哉的影响力真是不得了啊。\n善于从身边所有人汲取灵感的铃木：\n我从没看过他主演的电视剧，于是问女儿：木村的表演风格是怎样的？结果女儿只用一句话就解释清楚了：我觉得他能表现出男人那种不靠谱的感觉。我一听便想：找他给哈尔配音不是很合适吗！\n……“阿宫，你知道木村拓哉吗”“少瞧不起我，SMAP组合呗”“让他给哈尔配音怎么样”“啊？”于是我复述女儿的话，宫先生表示赞成：要的就是这个效果！\n高畑勋其人 高畑是细节狂魔，为了求证细节甚至亲自跑去种田，以及导出寻访小说或古籍中记载的事物，还会研究最新的制作技法。诸如此类的习惯总是导致项目严重滞后，当年《萤火虫之墓》一延再延，甚至到最后去菜园偷菜的重要场景以不上色的方式交付了……后来铃木想了一个“歪招”，在制作《平成狸合战》时，和发行方东宝联合起来一起骗高畑，计划上映时间是春天。果不其然，项目进度之后了，铃木看准时机说：事已至此，我就横下心跟东宝求情，想办法让他们改到夏天放映吧。……结果一算还是根本做不完，铃木试探道“干脆改到冬天？”高畑一反常态“呃，还是夏天吧”。铃木猜测：可能是高畑过意不去，也可能那时的高畑还残留着一丝严守制作计划的意识。对于这样风格的天才，铃木是这样想的：与高畑一起制作电影，虽然总会遇到各种困难，但在这个过程中我也学到了很多，真的很有意思。制作电影本身就好比一部纪录片，是一种知性的娱乐方式。\n现实主义的高畑：\n宫崎骏说：“时隔十年，能再次见到原彻，一起做点什么，也算是有缘了”这时高畑打断了他：“阿宫，别说这些无关紧要的废话了。作品又不是靠缘分做出来的，关缘分什么事呢。”那一刻，我深切地意识到，眼前这些人可不是什么半吊子。\n高畑用心力挺《龙猫》的企划：\n结果山下直言不讳地说：“《龙猫》这样的企划很难啊，观众还是想看《风之谷》《天空之城》这种片名中带外语单词的作品吧。” “山下副社长刚才所说，一定代表了宫崎骏影迷的意见。‘片名中带外语单词’这种表述还挺独特的，其实您想表达的是‘动作奇幻片’吧。观众的确想看这样的作品。”高畑继续说道，“可如果是这样，那要到何时才能拍上阿宫想做的《龙猫》那样的电影呢？”这话说得巧妙，山下副社长一听完就有点慌了……（但还只是答应做录像带而不是电影，于是铃木想到一个主意：如果只做一部《龙猫》不够，那就请高畑也做一部，两部同时推进不就行了吗？哈哈哈）\n……高畑对《龙猫》则是赞不绝口，说：“《龙猫》是我和阿宫这些年一起努力追求的东西的顶点。”而“这些年一起努力追求的东西”是什么呢？也许就是从普通人所遇到的平凡点滴、喜怒哀乐中挖掘出故事吧。\n《我的邻居山田君》票房惨淡，铃木在员工大会上背锅道歉：\n我横下心来，走上了讲台。“一切正如社长所说！”此时只见社长在台下一脸坏笑。我顿时火冒三丈：“失败的原因在于我们把发行公司从东宝换成了松竹……”\n我在员工大会上出了大丑，高畑却在庆功宴上当着大家的面发表了精彩的演讲。“就算这部电影没有火，我们也要以参与了它的制作为光荣！”这也太酷了吧。\n宫崎骏其人 完成《风之谷》后，宫崎骏宣布：我再也不做导演了，我不想再失去朋友了。……明明已经成为一名可以独立创作的导演，却想再做回“工作人员”，这也是宫崎骏不同于常人的地方。带领大家奋勇拼搏的确开心，然而当战斗结束，只剩下一副硝烟散尽、夕暮寂寥的光景时，留在心中的失落感让他渴望再次体验与大家一起创作的乐趣。\n负责作画的近藤喜文更是成了宫先生和高畑争夺的对象，但如果没有近藤，高畑确实没法开展，于是铃木说服了近藤加入了高畑的组，宫崎骏大为生气：\n“结果第二天早上八点左右，我接到了宫先生的电话。他突然说道：“我揍了阿近一顿。”这可把我吓坏了。细问之下才知道，他是在梦里揍的人。不过他也说：“气也消了，做就做吧。”这就是《龙猫》的起点。”\n总之宫先生是个很好的人。他重情重义，爱管闲事，明明自己很忙，却时时关注着《萤火虫之墓》的进度。据说他每天都会把那边的制作人员叫过来打听。回家后也是三句话不离《萤火虫之墓》，惹得他太太忍无可忍，责问道：“你成天把《萤火虫之墓》挂在嘴边，到底你在公司做什么啊？”这桩趣事也能体现出宫先生的为人。\n在《千与千寻》拿奖拿到手软后，最在意的却是一个小女孩的看法。小女孩一句看似的批评，却更让宫老开心：\n宫先生受不了一个人独占功劳，所以想尽可能的和他人分享，让自己轻松一点儿。……但是他从来没有因为这些事情浮躁过，电影观念也从未因此而动摇。……对我们来说，还有一件事比票房和奖项更重要。促使我们制作这部电影的千晶看完后会作何感想？首映会当天，宫先生格外紧张。……千晶是最后走出来的。宫先生战战兢兢的问：“怎么样？”千晶笑了笑：“真有意思”。这一句话，让宫先生和我们如释重负。\n……听说千晶回家后对父亲说：只有一个地方不太对。在电影最后的“完”时不是画了一只鞋子吗？那只鞋子画的不对。原来是有一次，千晶在木屋的河边玩耍时不小心把运动鞋掉进了河里，大家追着鞋子跑了一路。宫先生肯定是想起了那件事，所以才画了这幅画。千晶也是个很敏感的孩子，一看就意识到画的是那次的事情。只是鞋子的图案好像画错了。\n“应该是美少女战士的鞋子。”\n宫先生听说后，笑得可开心了。\n如何与宫崎骏这样特殊的天才打交道：\n还有一位“例外”是编剧丹羽圭子。在创作《借东西的小人阿莉埃蒂》的剧本时，宫先生也是想法日日变（好多编剧就是因为宫先生的这种习惯“壮烈牺牲”了），她却说：“这样能了解天才的思考过程啊，还有比这更有意思的吗！另一方面，《幽灵公主》和《千与千寻》的作画监督安藤雅司则非常享受和宫先生的激烈交锋。他会在和宫先生相抗衡的过程中追寻自己理想中的动画。美术专家男鹿和雄老师也有这样的倾向。要么积极接受并乐在其中，要么拿出匠人精神对抗到底——要和宫崎骏这位特殊的天才打交道，也许只有这两条路可走。\n和儿子宫崎吾郎的别扭相处：\n“我想让宫崎吾郎执导这部片子（《地海战记》）。”我一开口就遭到宫先生的激烈反对：“他怎么行啊，你疯了”反对的理由只有一条：他不擅长画画。……结果吾郎在分镜中展示出超出所有人预期的能力。 甚至在吉卜力之外，宫先生的 师傅大冢康生也惊叹到“虎父无犬子啊。” 庵野秀明也说:“这完全是宫崎骏的风格嘛!为什么不早点让他做啊?\n吾郎表现出的另一项能力是统率团队的领导力。这是连他父亲都不具备的天赋。宫崎骏是那种用过人的才华“统治”员工的导演……但工作人员难免会在这个过程中身心俱疲。而吾郎却用细心和体贴牢牢抓住了员工的心。作画工作启动后，他立刻理解了作画现场的金字塔形组织结构在精准掌控指挥系统的同时切实推进各项工作。如果动画师交上来的画出色，他不会音惜表扬，有问题时也会用明确易懂的话语下达指示。多亏了他，员工们都能以轻松愉快的心情投入工作，发挥出比平时更多的活力。另外，每逢周六他都亲自下厨犒劳大家。……虽然他没有多说，但我觉得他潜意识里终究想要赶超父亲。\n吾郎要是无法摆脱他对父亲的自卑情结，就不可能出人头地。为了他，那场戏（王子刺杀国王）也是必不可少的。后来有位心理学家和吾郎聊天，赞叹道：“我心想你有宫崎骏这样一位伟大的父亲，拍电影时肯定吃了不少苦头。结果跑去电影院一看，来就是弑父的场景，看得我格外激动。无论有没有缘由，父亲都是非‘杀’不可的。就因为他是父亲，所以才值得‘杀’。父亲越是伟大，越有‘杀’的价值。”对此吾郎如是回答道:“听到铃木说得让亚刃弑父，我觉得很有道理。倒不是说我非要超越父亲或必须冲破自己身上的束缚，只是结合当今年轻人的心境这是一种忍无可忍的感觉吧。\n……电影大功告成，放映结束后，宫先生自言自语道：即便还我来做，我也会做成这样。可作品的完成度确实让宫先生吃了一惊。但他却说道：“若要模仿，就不要让别人看出来你模仿的是谁！”\n当311大地震发生瞬间，宫老在做什么：\n《起风了》里面关于关东大地震的描写，我当时一直在琢磨地震的声音是什么样的呢?“3·11”大地震发生的时候，我在工作室里竖起耳朵听地震的声音，可怎么听都只有架子上的东西掉下来的声音，让我产生了“地震非常安静”的印象。\n高烟:听你这么一说，我就想起来了。当年遇到空袭的时候，总会有一段空白的时间和一段神奇的停顿。……\n对于工作这件事：\n“原来他不喜欢工作啊，这点我倒是完全可以理解……”\n不过人们的确会以工作的形式和社会接轨，获得精神世界的平静。失去“工作”这份属于自己的职责，对一个人来说是多么痛苦啊。\n关于创作 坚持与放弃\n接下来的一年，宫先生便窝在自己的工作室埋头绘制《画烟囱的小玲》……只见墙壁上贴满了Image Board，没人知道总共有多少张。可我没有多看这些画稿，只是说……（委婉的表达这个主打20岁年轻女性的动画企划不适合已经60岁的宫崎骏）……宫先生一边听我说，一边迅速起身，开始一张接一张地揭下墙上的Image Board。然后，他把画纸摞在一起，当着我的面咚一声扔进了垃圾桶。我至今都忘不了那一幕。“你的意思是这个项目行不通，对吧铃木？”我没有直说，但他一定读懂了我的神情。\n接着，他突然说道：“为千晶拍一部电影吧。”……（然后立即开始构思）……《小玲》遭到了我的反对，宫先生心里憋着一股气，于是发动了精彩的反击。他料定，只要搬出千晶和江户东京建筑园，我就绝对不会提出反对。他毫不犹豫地抛弃了酝酿了整整一年的项目，从零开始构思新的项目。并且，这一过程只花了五分钟。他的爽快和专注，真让我佩服得五体投地。这就是《千与千寻》的起源。\n使用套路还是冒险\n无脸男的提案的确有趣。只不过，会不会有小观众透过无脸男看到心灵的阴暗面呢？另一种方案便是联合小白达到钱婆婆，这么一来便是通俗易懂的奇幻动作大片。也许娱乐电影本该这么简单明了……“你定吧！”选打倒汤婆婆的……话都到嘴边了，可我最后还是下意识的说了“选无脸男”……说实话，我虽然担心，但认为能卖座的是“无脸男版”。早在制作《幽灵公主》的时候，我就观察到，单纯讲述惩恶扬善的故事已经吸引不了观众，娱乐电影也需要有哲学思想的时代已经到来了。\n回顾过去，日本电影的主题几乎都是贫穷与战胜贫穷。然而，经历了经济高速发展期，贫穷已经不足以成为主题了。之后又经历了泡沫经济的崩塌，很多人出现了心理问题。电影主题逐渐转变为心理问题与如何克服心理问题。《千与千寻》就诞生于这样的大环境下。\n对“俄狄浦斯情结”的现代意义的思考\n俄狄浦斯情结是一个很古典的主题，但我觉得它也能触动现代的观众。想当年，孩子们会在父母看不到的地方塑造只属于自己的世界，在那里学到各种各样的知识找到自我。\n但如今的孩子在被过度保护的环境中成长起来。换句话说，他们的举手投足都在父母的监视之下有时候，这种监视甚至会持续到他们成年。对孩子们来说，这是一个令人煎熬的时代。所谓的“寻找自我”之所以大行其道，出现心理问题的孩子之所以层出不穷，这恐怕就是背景原因之一。想要摆脱监视重获自由，想要找到自我——《地海战记》能够为这样的孩子指明前进的道路。\n对“社会变得浮躁和疯狂”的历史原因的思考\n时隔二十年，宫先生再次提出做《虞美人盛开的山坡》吧，他是这么想的：进入二十一世纪后，世界为什么变得越来越疯狂？经济的高速发展是不是使日本走向疯狂的催化剂呢？如果将故事设定在哪个时代，作品就有了审视现代社会的意义。\n……二十世纪六十年代的我还很年幼，但我记得那是一段非常光辉的岁月。……收入倍增计划这个词让我记忆犹新……另一件令我印象深刻的事情是，淫猥怪诞的东西从我们身边消失了，让人感觉“纯净的时代来临了”。学校里不存在霸凌现象，打架归打架，但从小学到高中我从没有看到过任何阴险的欺凌。身边的一切都是积极向上的，当然，负面消极的东西应该也是存在的，但人在埋头前进的时候是不会回头看过去的。于是我们就看不到任何令人不愉快的东西了。再加上在那个时代，全社会都很积极，很多问题也许就这样被掩盖住了吧。……可我们盼来的未来是怎么样的呢? 生活的确变得富足了，然而在这个物质泛滥的社会，很多人为自己该怎么生存而烦恼，出现了心理问题。这就是今天的日本。\n对“小朋友适合看什么样的东西”的思考\n《龙猫》也是如此，观影期间也有小朋友吓哭了。他们害怕的是龙猫本身。大约到了四五岁就能克服这种恐惧心理了。年纪更大一些的孩子则会迷上龙猫，一头栽进电影中的世界。 我认为这绝非巧合。当然，面向小朋友的电影最好能让人感觉被善良温暖的柔情所包裹着。然而世界上的确存在着可怕的东西，这些东西也必须被认真描写出来——宫先生正是怀着这样的意图创作的。《龙猫》这部作品的神髓便是在呈现温暖的同时也表现出恐惧。……这就是龙猫背负着的历史。它不仅是一种可爱的生物，也兼具骇人的元素。小朋友们会本能地感受到这一点。我和宫崎骏打了四十年的交道，深感他最大的特征就是童心未泯。他总能像孩童一样自由自在地发挥想象力，且想象出来的东西不总是光明的，也带有阴暗的一面。这也是他的魅力之一。\n对“年轻人的喜好”的思考\n我认为票房难创佳绩的原因除了影片的质量和宣传方式之外，还有一个更为本质的问题，那就是我察觉到社会已经开始逐渐转变了。说得再具体些，就是大家已经开始厌倦所谓的“寓言式电影”了，厌倦了用动画去描绘出一个奇幻的世界，再透过它来回顾现实世界。……反过来说，年轻人变得越来越强大，我们很难用这样的作品抓住他们的心，他们的目光更多地投向了现实。如今，这种倾向变得愈发明显，他们开始全盘否定上一辈人所做的一切了。当下的年轻人不想要电视剧中的“谎言”。换作以前，让人惊呼“什么”的意外转折是观众喜闻乐见的，然而时至今日，这种元素已经无法让他们感到惊讶了也许他们根本就不相信故事本身。他们只相信推动现实的原则，不相信流于幻想的原则。\n奇幻作品将走向何方? 我也不知道。不过，我们之所以制作《记忆中的玛妮》，是为了鼓励那些在现实世界中饱受折磨的人，为他们注入活下去的力量。也许我们并没有给出现实的解决方案。但我们可以陪伴你，告诉你“并非只有你一个人在受苦。”\n关于制作 因为各种原因需要对原来的想法删减时，如何考量？\n“诚如高畑所说，缩短结尾，可能会让观众认为电影想要表达的是老一套的“保护自然”。但高畑原来的方案也包括了“把保护自然挂在嘴边是不是也有问题”的疑问。这个世界没有那么简单，人类背负着更为复杂的使命活着——按他的思路制作，观众可以思考到这一深层。\n团队里有不合群的天才怎么处理？\n吉卜力有几位不太合群的动画师，技术水平自不必言，只是有的早上不来上班，有的特别任性，有的不听导演安排……把他们安排在作画现场可能会引发各种问题。我在工作室附近又租了一间房子，将“大侠”们专门养在那里。而且我明确规定，能入住的不限于员工，大侠们想带谁来就带谁。于是身怀绝技的无赖豪侠们反而纷至沓来。我们将那栋房子正式命名为“第四工作室”，专门用于处理需要优秀动画师完成的高难度工作。一方面，制作电影最讲究的就是团队合作；一方面，这些豪侠们一直都是吉卜力的中流砥柱，也毋庸置疑。\n写实还是抽象？\n高畑：在这次的《辉夜姬物语》中我们没有仔细描绘所有的空间和阴影，而是采用了带有速写色彩的手法，以此激发观众的想象力与记忆。看普通的动画时，观众会把自己看到的“画”当成是真的。可看速写画的时候，人会努力去想象“画图的人可能看到的对象本身”。\n关于发行 带有深意的作品刚完工时，是否要立即交给发行商讨论？\n不过我没有立刻通知东宝。因为我很清楚，如果在这个时候拿给他们，一定会引发各种关于内容的讨论。要是负面意见占了上风，说不定千辛万苦搭建起来的发行体系也会被彻底推翻。所以我告诉相关人员影片还没完成，连试映会都是拖到最后一刻才办。\n……果不其然，看完试映的相关人员的反应并不理想。“这部电影对孩子来说太难理解了。”还有人对宣传文案“活下去”提出质疑——情节已经很复杂了，还配上这种哲学味很浓的文案，不仅孩子提不起兴趣，女性观众、成年观众也不会买账的。\n但我打定主意宣传文案不作他想。因为我觉得，我们已经迎来了“电影也需要具备哲学内涵的时代”。我脑海中一直萦绕着从高畑那里听说的加里·库尔茨（《星球大战》的制片人）的观点。加里认为，以往好莱坞电影的首要主题是爱情。而《星球大战》的问世改写了历史，哲学也成了电影主题。”……既然如此，在宣传电影时提及哲学，也就是“生与死”的主体应该也是没问题的。……（还有各种容易引发争论的细节）……我早就料到会有人提出诸如此类的问题了。当然，只要认真看完这部电影，就会明白它绝不宣扬暴力，也没有涉及歧视。……肯定是因为我们做了很多前所未有的尝试，每个参与其中的人心里都没有把握。……为此，我们在各个城市展开了空前的宣传攻势……一部电影能否取得成功，绝不是听天由命的（酒香也怕巷子深），要尽的人事是在太多了。\n这样的合作方值得信赖吗？\n（和迪士尼谈合作时）别人都一个劲儿夸吉卜力的作品有多好，星野先生却只谈生意，态度又非常坦诚，这让我觉得他是值得信赖的人。经多方打听，我得知的是你有上百位销售专员，比其他公司强大得多。一九九六年，吉卜力和迪士尼正式签约，《幽灵公主》在全美上映一事也尘埃落定。\n如何确定宣传文案？\n（为《千与千寻》指定宣发主题时，很多工作人员都误以为主要讲的是千寻和小白的爱情故事）\n我决定掐表计算每个角色登场秒数……毫无疑问，第二名是无脸男。主推无脸男的方针对宣传文案也产生了影响……“你不是说过吗？‘好的文案往往诞生于偶然，并且大多数情况下都是最先说出来的那句话。但在反复锤炼中大家也许会忘记最初的表述。遇到这种情况，我们必须回归原点’。”市川建议用“唤醒活下去的力量”。……反响远超预期。广告行业就不用说了，“活下去的力量”甚至被应用在了教育和其他领域。\n脚注 在看《DOOM启示录》《Blood, Sweat, and Pixels》《硅谷传奇》时也有类似感受。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这句台词出自《Legal High》第二季第七集。片中导演名叫宇都宫（和宫崎骏的故乡同名）。片中被气走的同事和差点崩溃的原告不难让人对应到细田守、押井守、宫崎吾朗和庵野秀明（原出处）……看完书会觉得，往往现实比电视剧还要精彩。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n即吉野源三郎的《你想活出怎样的人生》，出版于1937年，后面还会写文章展开说说（见这里）。宫崎骏在小学时就在教科书上读过。“我至今还记得第一次读到《你想活出怎样的人生》时受到的冲击。”这也是宫崎骏老爷子将在一个月后上映的同名动画电影，据传是最后一部作品（希望不是）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://pps43.github.io/posts/notes_on_genius_in_ghibli/","summary":"这本书带给我的，除了诸多耳熟能详的作品背后的逸闻趣事、诸位大师的才华风貌，还有不少对创作思路、团队协作、宣传发行的启发，相信能常看常新。同时","title":"吉卜力的天才们（铃木敏夫）"},{"content":" 本文是PhysX物理引擎系列的特别篇，记录了影响近一周的物理引擎底层概率性Crash的定位过程和修复方法，具有很高的实践参考价值。“有多高？”“三四层楼那么高啦！”\n发现问题 运维同事发现体验服和某区在新版本上线一小段时间后，会出现概率不高但持续出现的进程Crash。这里先简单说明一下：我们会在一台机器上部署多个GameServer实例，每个GameServer实例进程同时进行着多场不同的Match，如果某一场Match出现了业务层Crash，并不会影响其他Match。但如果是C++物理库内出现Crash，则会同时中止其他正常运行的Match，对玩家的影响较大。\n虽然看不到完整的堆栈，但从中还是发现和物理引擎的SceneQuery有关。 其实还有另一个很相似的堆栈（忘记保存截图了）在prefilter上方还有一行NpShape vtable...。由于prefilter实现逻辑中确实调用了shape.getFlags()，所以怀疑是跟shape相关逻辑中出现空指针引用。但由于shape是引擎内部维护的，所以上述怀疑并不能提供明确的修复方法。\n// prefilter implementaion PxQueryHitType::Enum PhysxQueryFilterCallback::preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags) { bool isTrigger = shape-\u0026gt;getFlags() \u0026amp; physx::PxShapeFlag::eTRIGGER_SHAPE; if (isTrigger \u0026amp;\u0026amp; !m_IncludeTrigger) { return PxQueryHitType::eNONE; } PxFilterData shapefilterData = shape-\u0026gt;getQueryFilterData(); if (shapefilterData.word0 \u0026amp; filterData.word0) { // m_HitType should be PxQueryHitType::eBLOCK or PxQueryHitType::eTOUCH return m_HitType; } return PxQueryHitType::eNONE; } // shape-\u0026gt;getFlags() implementation (fron physx source code) PxShapeFlags NpShape::getFlags() const { NP_READ_CHECK(getOwnerScene()); return mShape.getFlags(); } 由于同期发布的还有其他物理相关功能，先通过开关这些功能来对Crash做初步判定。有一个延迟删除Actor的优化较为可疑，关闭该功能后，Crash确实减少了一小部分。\n另外也尝试通过分析日志对这些Crash发生的情境做猜测，便于复现。前面提到过，物理层一旦Crash会影响到该进程下其他正常进行的比赛。所以只好抓取了Crash发生前200ms的所有相关比赛的日志，大致能看出最后几秒内玩家进行了哪些操作：商店购买、切换武器、伤害扣血、扔手雷。这里只有扔手雷和物理层有关联，进一步找到该场比赛的模式信息，告诉QA同学尝试复现。\n第一次尝试修复 测试环境中一直未能复现，所以先尝试了一些防御性判空修复。比如在每帧驱动物理更新时，对Scene的判空；又比如在OnContact时，增加了对actor和shape失效状态的判断，并且获取actor的方式由pairs[i].shapes[0]-\u0026gt;getActor()改为pairHeader.actors[0]。这些修复确实能加强程序的健壮性，但可惜对本次的Crash并没有直接帮助。\nvoid PhysxSimulationEventCallback::onContact(const PxContactPairHeader\u0026amp; pairHeader, const PxContactPair* pairs, PxU32 nbPairs) { // add actor validation if (pairHeader.flags \u0026amp; PxContactPairHeaderFlag::eREMOVED_ACTOR_0 || pairHeader.flags \u0026amp; PxContactPairHeaderFlag::eREMOVED_ACTOR_1) { return; } PxRigidActor* actorA = (PxRigidActor*)pairHeader.actors[0]; PxRigidActor* actorB = (PxRigidActor*)pairHeader.actors[1]; for (PxU32 i = 0; i \u0026lt; nbPairs; i++) { const PxContactPair\u0026amp; curPair = pairs[i]; // add shape validation if (curPair.flags \u0026amp; (PxContactPairFlag::eREMOVED_SHAPE_0 | PxContactPairFlag::eREMOVED_SHAPE_1)) { continue; } if (curPair.events \u0026amp; PxPairFlag::eNOTIFY_TOUCH_PERSISTS) { // do nothing when contact persists } else { PhysXContactResult result; result.Lost = curPair.events \u0026amp; PxPairFlag::eNOTIFY_TOUCH_LOST; result.ColliderA = actorA == NULL ? NULL : (PhysXActor*) actorA-\u0026gt;userData; result.ColliderB = actorB == NULL ? NULL : (PhysXActor*) actorB-\u0026gt;userData; if (result.ColliderA != NULL \u0026amp;\u0026amp; result.ColliderB != NULL) { m_ContactRecords.push_back(result); } } } } 第二次尝试修复 为了获取更多信息，决定费一些周章，在一台线上机器部署Debug版物理库。事实证明，这个努力是值得的、立竿见影的。\n平时服务器程序打Release包时，链接到Release版的物理库工程（底层使用了physX）：\npackage physxgo /* #cgo CPPFLAGS: -Wno-attributes -I ./include -O3 -DNDEBUG #cgo LDFLAGS:-L ./lib -lPhysXWrapper_x64 -O3 */ import \u0026#34;C 现改为链接到Debug版物理库工程：\npackage physxgo /* #cgo CPPFLAGS: -Wno-attributes -I ./include -O1 -DNDEBUG #cgo LDFLAGS:-L ./lib -lPhysXWrapperDEBUG_x64 -O1 */ import \u0026#34;C\u0026#34; 在本地Windows上测试通过后，部署到Linux却失败了：发现依然链接到Release物理库。\n这里稍加说明：服务器是Linux的，但为了平时能在Windows上开发调试物理库，我们搭建了两套构建流程。在Linux上使用makefile编译出.so，在Windows上构建出.dll供cgo调用。注意cgo使用gcc，由于name mangling方式和MSVC不同，在使用MSVC编译时需要在.def文件中指定每个导出的函数编译后的名字，形如x=y，这样MSVC会把自己编出的y翻译成x，以便和gcc兼容。\n举例：_ZN10PhysXActor11SetPositionEfff=?SetPosition@PhysXActor@@QEAAXMMM@Z\n原因是go build是强制使用了配置的环境变量以及缓存。使用go build -d清空缓存即可。\n成功部署Debug包到一台服务器上后，过15min就Crash了，在最后一刻传递出了新的信息：\n创建shape时参数不合法，首先怀疑传入了0或NaN。进一步的排查发现，这个版本使用了新版的创建函数，确实相比老的创建流程少了一个参数合法性校验。当发现某个维度出现0值时，虽然不会立即Crash，但在后续被Raycast等操作访问到时，会概率性Crash。解决方法很简单，在业务层已有代码中，发现传入0时给出警告并return；在底层代码中创建shape时，强制将值为0的维度改为一个微小的值，如下ForceNonZero。\nvoid ForceNonZero() { //EPS = 1.192092896e-07F (machine epsilon for float) if (physx::PxAbs(x) \u0026lt; EPS) { x = x \u0026gt;= 0 ? EPS : -EPS; } if (physx::PxAbs(y) \u0026lt; EPS) { y = y \u0026gt;= 0 ? EPS : -EPS; } if (physx::PxAbs(z) \u0026lt; EPS) { z = z \u0026gt;= 0 ? EPS : -EPS; } } 部署该修复后，Crash归零。\n并且通过Debug版给出的警告，我们还对数值类参数做了NaN或Inf校验physx::PxIsFinite，并且对方向类参数做了强制归一化，进一步提高了物理系统稳定性。值此，修复完成。\n总结 对于传入physX的参数必须严格保证有效性。\n基本要求：数值类不可以是NaN, Inf 尺寸类不可以是0 方向类矢量必须模长为1 平时开发中多关注Debug版的输出，对于警告和错误都要高度重视。\n问题发生时，如果传统的定位、复现问遇到困难，不妨考虑部署Debug版到线上环境，获取更多信息。\n","permalink":"https://pps43.github.io/posts/using_physx_solving_zero_value_crash/","summary":"本文是PhysX物理引擎系列的特别篇，记录了影响近一周的物理引擎底层概率性Crash的定位过程和修复方法，具有很高的实践参考价值。“有多高？","title":"PhysX零值Crash"},{"content":" 当看到书名时，我承认内心有一丝挑战欲。受过系统的科研训练的人，往往对“真的”有“洁癖”。作者Robert Wright也认识到这一点，所以在开头便说，本书可以看作佛学观点的进化心理学解释。并不探讨轮回、神明这些超自然的部分，而探讨自然主义部分。并不探讨佛教各分支的细枝末节，而探讨核心思想。不要用佛教教义使自己成为更好的佛教徒，用它来做更好的自己。正是这句话让我有了继续读下去的意愿。\n进化心理学🐵 进化心理学研究的是大脑如何由自然选择设计来误导我们甚至奴役我们的。因为自然选择最终只关心有利于基因传播的基因特性。\n猴子实验表明，如果你尝试做一件事体验到一种新的快感，你会分泌大量多巴胺。但后续再次尝试做这件事，多巴胺分泌峰值会出现在你真正做之前。也就是说，期待的快感比实际体验的快感更强。作者将这种差异成为“幻觉”。\n幻觉的出现源自感觉。感觉最初的作用是趋利避害，是对所处环境中的事物做出评价：好还是坏。如果这种评价是准确的，就可以说某种感觉是真实的，否则称为幻觉。幻觉大致有两类：\n“环境错位”。自然选择是在特定的环境下设计出我们的感觉系统的，而如今外部人文环境突变，自然选择却没有及时更新设计而带来幻觉。例如，在原始部落以小规模、相对固定的族群为环境的生存规则下，自然进化让我们会特别在意他人怎么看待自己。但是在现代社会，一个人却能经常接触大量并不了解你的陌生人（尤其有了互联网以后），而保持别人眼中好印象的压力就会被放大。你可能急切想去确认对方对你的态度。也可能因为无法确认而痛苦或回避。 也叫“误报”，但往往关乎生死所以宁可误报不可漏报。 例如，我们至少身处于两种幻觉中，都属于上文提到的“环境错位”。我们以为自己对事物的掌控力比实际情况强；我们对自己的能力和品德的评价会明显偏离实际。自然选择设计出这两种幻觉，作用分别是帮助我说服外界世界我是始终如一的行为者值得信任；帮助我说服世界我应该得到赞扬而不是责备。\n总之，人类在自然选择的设计之下被植入一些幻觉而做一些特定的事情，比如吃饭、做爱、超越对手。现代社会也有大量的人为设计，利用同样的机制，似乎只要遵循三个原则：\n实现这些目标能带来某种快感。 但这种快感只能是短暂的。 调动动物性大脑更关注第一点而不是第二点。 作者认为进化心理学能够揭露真相，却并不能直接帮助人摆脱这种幻觉的控制。这也是作者从佛学、冥想中寻求切实对策的起因。\n佛教🙏 佛教分为小乘和大乘。内观禅修源自小乘佛教。大乘佛教中有一些关于幻觉的激进而广泛的理念，接近“唯心论”，即我们通过意识感知的事物实际上都是凭空的臆想。由此可以引申到佛学概念“空”。另一个佛学概念是“无我”，即所谓自我也是幻觉，并非真实的存在。本书大量篇幅都在探究这两种思想。\n正念冥想🧘‍♀️ “正念”和“正定”，都属于佛教中的“正道”。\n正定冥想通过专注于呼吸，使注意力集中。专注（入定）会带来宁静，如果持续足够长的时间，还会带来强烈的极乐和狂喜感。 正念冥想在注意力集中的基础上，用心体察自己的内心感觉，会让我们更好的体会美。和正念冥想相关的概念是“内观禅”。所谓内观，有三法印，用白话说就是三个标准： “诸行无常”（impermanence） “诸漏皆苦”（dukkha），苦在某些语境下指的是不满足。 “诸法无我”（notself/anatta），即自我是一种虚幻的错误的信念。 无我 说起“无我”，不得不提起《无我相经》中的佛陀梳理出的体验的五个种类，称为“五蕴”。\n色身。包括眼、耳这类感觉器官。 感觉。 辨识。可辨识的景象或声音。 心锁。复杂的情绪、思想等。 意识。特指对上述四点的认识。 接下来佛陀定义了所谓的“自我”，并试图证明其不存在。第一、所谓“自我”，应当具有一定程度的控制性。即如果存在“自我”，至少五蕴中有一蕴是受控的。然而我们并不能向其中任何一蕴发号施令，所以“自我”不存在。第二、所谓“自我”，应当具有一定程度的恒常性。即如果存在“自我”，至少五蕴中有一蕴具有历经岁月而不变的特质。然而稍加思辨就会发现五蕴都不是恒常的，所以“自我”不存在。\n对于非佛教徒，对于这种论证逻辑显然不太满意。而佛教徒对此更深的解读有会让人更加困惑。对此，作者的建议是从中汲取积极性的一面：\n不要想太多，去做。你有能力与自己的感觉、思想、冲动和辨识建立一种不同的关系，解决与其中几样的纠缠，从而实现某种程度的意识解放。 如果不再将这些我无法控制的感觉看作“自我”的组成部分，我就能从这些感觉中得到解脱。例如当五蕴之一受苦时，你的态度应该是：这不是我的，这不是我。 总之，关于佛陀对“自我”的真正理解的争论，是毫无意义的。但有一点启发：**我们对“自我”的概念有极大的误解。我们通常会将其与“控制”“恒常”联系在一起，但实际上会发现，自己收到的控制比想象中的少，身份也更具灵活性。**通过这个概念也许可以帮助一个人更好的冥想。\n现代心理学实验表明，可以通过控制实验对象的心理状态，改变其个人选择倾向。“感觉”决定了当下由哪个模块负责，决定你在那段时间里所做的事情、以及产生新的感觉。“意识”并没有自我控制权，大脑多个模块各自反应和相互作用塑造了我们的行为，意识最多是个众议院议长而不是总统。\n无相，不执于物 无相（The Formless）并不是说物质世界不存在，也不是否认物质世界有构架，而是指很多关于世间万物的意义都是我们后来强加上去的，比如价值观或某种体验，一旦强加了这种特别感，对其反应的强烈程度就会变高。\n人天生就有一种强烈倾向：归因事物的“本质”。这种倾向结合一些原始性的思考捷径，往往对本质和真相的解释造成偏差，常见的一种是错误归因：在评价某人的行为时会倾向于性格因素（即他是个怎样的人）而不是情境因素（他当时面对怎样的处境）。更进一步说明自然选择对人脑的影响。\n如果敌人或对手做了一件好事，我们倾向于将其归因为环境因素——他给乞丐钱只不过是为了取悦一位恰巧站在那里的女性；\n如果密友或盟友做了一件坏事，那么环境因素就会显得突出——他对要钱的乞丐大喊大叫是因为他工作压力太大。\n除非对我做了坏事，那么他将会被移出盟友这个分类。即使敌人对我做了好事，我依然对他心怀芥蒂。多么简单有效的生存法则啊。\n敌人的暴力行为归因于性格，因此可以进一步证明敌人天生好斗；敌人的亲善的行为归因于对情境的反应（某种技巧或是因弱势而做出的暂时性调整），因此也不需要对他们原本的形象做调整，因为敌人的本质是坏的。战争中，你最开始的想法是一个国家的领导人的本质是坏的。你会认为这个国家的本质是坏的——整个伊拉克或德国或日本。你转而又开始认为这个国家的所有士兵——甚至这个国家的所有人——本质都是坏的。这些人是坏人，那就么你杀了他们也不用遭受良心的谴责！\n谁让他们本质是坏的呢……\n佛教中的不执于物，并不是引导人变成一个毫无情感、没有情绪波动的机器人。作者认为，持续佛学修行，可以丰富情绪，使人的情感变得更敏锐，使人变得更幸福喜乐，对待世界的方式可以更自由（换言之，不给事物归因太强的本质，正是自由的源头）。比如，让父母之爱变得没有那么强的占有欲。或许这样一来父母不再那么焦虑，控制欲也有舒缓，结果父母和孩子都会变得更幸福。\n开悟，无源之见 开悟，真的是得到终极真理了吗？\n开悟，究竟是怎样一种体验？\n有人认为，佛教的开悟之路就是对自然选择下的人脑的反抗。有人认为是体验到无我。有人认为，开悟就是看到万物的无常、克服贪爱等枷锁。\n还有一种理解叫“无源之见”（The View From Nowhere）：不带任何自私偏见的洞察，而且在一定程度上甚至根本不是任何特定人类的视角，或是其他物种的视角，即任何个体的利益并不比其他任何个体的利益更重要。自然选择机制创造了许许多多不同的视角，而佛教的开悟就是要超越所有这些视角、抵制自我特殊性这种核心的进化价值观——这种价值观造成的扭曲力场所产生的敌意已经深深影响了人类社会：种族问题、宗教问题、国家问题和意识形态冲突等。\n靠冥想拯救世界看起来难免太过天真幼稚，作者想强调的是，通过培养冷静清明的大脑，才能在面对威胁时不至于反应过度、恶性循环。我们不需要去爱敌人，但想要清醒地认识敌人，就要抑制恐惧和嫌恶，还要超越更微妙的感知和认知扭曲。\n所以佛学为什么是真的？ 作者说，佛学为什么是真的？因为我们是自然选择造就的动物。关于自然选择植入我们头脑中哪些倾向性，早期佛学思想家在科学资源有限的条件下，做出来了不起的评价。如今，根据对生物学和脑科学的现代研究，我们可以为这些评价提供新的支持。\n其实看到这里，我也并不在意真假之辩了，这本书已经带给我一些新的有益的思考，便已足够。\n后记 在讨论“无我”的时候写到，心理学研究和佛学思想中对人脑决策的模块化建模方式，可以为游戏AI尤其是拟人化AI行为提供启发。即预先设计模块化的专门系统，然后设计多种情绪触发器，每种触发器可以触发不同组合的专门系统。曾有心理学家给出粗糙的7个模块：自我保护、吸引配偶、保住配偶、友好关系、关爱亲属、社会地位、预防疾病。\n最后，本文使用了诸多佛教用语，并不一定是“正统解读”，所以希望佛教中人莫要见怪。一定要说的话，大概算是窥见一丝精神内涵后赋予的现代社会和心理的一种理解。话又说回来，什么又是正统呢。\n","permalink":"https://pps43.github.io/posts/notes_on_why_buddhism_is_true/","summary":"当看到书名时，我承认内心有一丝挑战欲。受过系统的科研训练的人，往往对“真的”有“洁癖”。作者Robert Wright也认识到这一点，所以在开","title":"洞见·为什么佛学是真的"},{"content":" 本文主要介绍PhysX场景查询的一些内部机制和使用方法。\nPhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code, Doc）的一些经验，有不少对官方资料的补充。\nWarm-up A Scene in PhysX engine is a container of objects in a hierachical manner.\n--- title: Scene Hierachy --- classDiagram direction LR class world class scene { Flags Gravity ... } class actor { ActorFlags Name GlobalPose ... } class shape { Flags GeometryType LocalPose QueryFilterData SimulationFilterData ... } class geometry { ... } class material { friction restitution damping } world \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;scene scene \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;actor actor \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;shape shape o--geometry shape o--material There are only position and rotation in GlobalPose and LocalPose, no \u0026ldquo;scale\u0026rdquo;. Scale only reflects on geometry\u0026rsquo;s actual size.\nScene Query Basics Three kinds of scene query:\nraycast sweep overlap In general, each SceneQuery traverses a culling structure containing the scene objects, performs a precise test using the GeometryQuery functions, and accumulates the results.\nYou can customize filtering logic via prefilter and postfilter.\nflowchart LR BP(Broad Phase) --\u0026gt; prefilter --\u0026gt; MP(Mid phase) --\u0026gt; NP(Narrow Phase) --\u0026gt; postfilter Broad phase traverses the global scene spatial partitioning structure to find the candidates for mid and narrow phases. midphase traverses the triangle mesh and heightfield internal culling structures, to find a smaller subset of the triangles in a mesh reported by the broad phase. Narrow phase performs exact intersection tests Pre-filtering happens before midphase and narrow phase and allows shapes to be efficiently discarded before the potentially expensive exact collision test. Post-filtering happens after the narrow phase test and can therefore use the results of the test (such as PxRaycastHit.position) to determine whether a hit should be discarded or not. More on Traversal A scene uses two query structures, one for \u0026ldquo;static\u0026rdquo; objects (PxRigidStatic), one for \u0026ldquo;dynamic\u0026rdquo; objects (PxRigidBody). Each structure can use different culling algorithms, see PxPruningStructureType.\nPxPruningStructureType Explaination eNone Based on grid. Full rebuild when changed. eDYNAMIC_AABB_TREE Based on tree. Full rebuild when changed. Only choose this if all static actors in your scene are not modified after initialization. eSTATIC_AABB_TREE Based on grid and tree. Incremental rebuild when changed, unless by force. Choose this if frequently add/remove geometry, at the cost of higher memory More on Filtering To make custom filtering logic works, there are 3 steps.\nAttach data for filtering on the shape (a.k.a. QueryFilterData) Define custom filtering logic Attach filter in your scene query Step 1. Attach data for filtering on the shape Attach PxFilterData to each shape\u0026rsquo;s QueryFilterData. It has four 32bit words to hold custom data, e.g., we use word0 as layer of this shape. Here is an example:\nPxShape* shape = PxRigidActorExt::createExclusiveShape(*pxActor, PxBoxGeometry(extV), *pxMaterial); PxFilterData queryFilter; queryFilter.word0 = layer; shape-\u0026gt;setQueryFilterData(queryFilter); Step 2. Define custom filtering logic Define callback function for prefilter and postfilter. See PxQueryFilterCallback. The logic is totally depend on yourself, just return PxQueryHitType to tell if this shape can pass.\nPxQueryHitType Explanation eNONE Shall not pass. eTOUCH Pass, but does not stop the raycast or sweep. eBLOCK Pass, but stop here. Here is an example:\nclass PhysxQueryFilterCallback : public PxQueryFilterCallback { public: PhysxQueryFilterCallback(); PhysxQueryFilterCallback(bool isBlocking, bool includeTrigger); virtual PxQueryHitType::Enum preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags); virtual PxQueryHitType::Enum postFilter(const PxFilterData\u0026amp; filterData, const PxQueryHit\u0026amp; hit); private: PxQueryHitType::Enum m_HitType; bool m_IncludeTrigger; }; PxQueryHitType::Enum PhysxQueryFilterCallback::preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags) { bool isTrigger = shape-\u0026gt;getFlags() \u0026amp; physx::PxShapeFlag::eTRIGGER_SHAPE; if (isTrigger \u0026amp;\u0026amp; !m_IncludeTrigger) { return PxQueryHitType::eNONE; } PxFilterData shapefilterData = shape-\u0026gt;getQueryFilterData(); if (shapefilterData.word0 \u0026amp; filterData.word0 || shapefilterData.word1 \u0026amp; filterData.word1) { return m_HitType; } return PxQueryHitType::eNONE; } PxQueryHitType::Enum PhysxQueryFilterCallback::postFilter(const PxFilterData\u0026amp; filterData, const PxQueryHit\u0026amp; hit) { const PxLocationHit\u0026amp; impactHit = static_cast\u0026lt;const PxLocationHit\u0026amp;\u0026gt;(hit); if (impactHit.distance \u0026gt; 0.0f) return m_HitType; return PxQueryHitType::eNONE; } Step 3. Attach filter in your scene query PxQueryFilterData has two fields:\nfield Explaination PxQueryFlags Supported flags are in PxQueryFlag::Enums, e.g. raise ePREFILTER means all shapes need to pass prefilter you defined. PxFilterData Has four 32bit words for you, e.g. use word0 as the \u0026ldquo;layermask\u0026rdquo; of the query. Raycast example Here is an example of raycast (return multiple objects).\n// save result and use it later struct MyRaycastHitResult { PhysXVec3 Position; PhysXVec3 Normal; float Distance; PhysXActor* Collider; }; typedef PhysXArray\u0026lt;MyRaycastHitResult\u0026gt; MyRaycastHitResults; // make sure \u0026#34;direction\u0026#34; is non-zero and normalized! bool PhysXManager::MyRaycast(MyRaycastHitResults\u0026amp; hitResults, const PhysXVec3\u0026amp; startPos, const PhysXVec3\u0026amp; direction, float distance, unsigned int layerMask, bool includeTrigger) { const PxU32 bufferSize = 256; PxRaycastHit hitBuffer[bufferSize]; PxRaycastBuffer buf(hitBuffer, bufferSize); PxQueryFilterData filterData = PxQueryFilterData(); filterData.flags |= PxQueryFlag::ePREFILTER | PxQueryFlag::ePOSTFILTER; // will call both prefilter and post filter filterData.data.word0 = 0; filterData.data.word1 = layerMask; PhysxQueryFilterCallback filterCallback(false, includeTrigger); bool status = m_pxScene-\u0026gt;raycast(startPos, direction, distance, buf, PxHitFlag::eDEFAULT, filterData, \u0026amp;filterCallback, NULL); if (status) { hitResults.clear(); for (PxU32 i = 0; i \u0026lt; buf.nbTouches; i++) { PxRaycastHit touch = buf.touches[i]; MyRaycastHitResult hitResult; hitResult.Position = touch.position; hitResult.Normal = touch.normal; hitResult.Distance = touch.distance; hitResult.Collider = (PhysXActor*)touch.actor-\u0026gt;userData; hitResults.push_back(hitResult); } if (buf.hasBlock) { MyRaycastHitResult hitResult; hitResult.Position = buf.block.position; hitResult.Normal = buf.block.normal; hitResult.Distance = buf.block.distance; hitResult.Collider = (PhysXActor*)buf.block.actor-\u0026gt;userData; hitResults.push_back(hitResult); } } return status; } Overlap example Here is an example of sphere overlap. Other geometries are similar.\n// make sure radius is above zero! bool PhysXManager::MySphereOverlap(PhysXRaycastHits\u0026amp; hitResults, float radius, const PhysXVec3\u0026amp; position, const PhysXQuat\u0026amp; rotation, unsigned int layerMask, bool includeTrigger) { const PxU32 bufferSize = 256; PxOverlapHit hitBuffer[bufferSize]; PxOverlapBuffer buf(hitBuffer, bufferSize); PxQueryFilterData filterData = PxQueryFilterData(); filterData.flags |= PxQueryFlag::ePREFILTER;// no postfilter logic since we SHOULD NOT EXAMINE DISTANCE in overlap query filterData.data.word0 = 0; filterData.data.word1 = layerMask; PhysxQueryFilterCallback filterCallback(false, includeTrigger); bool status = false; PxTransform trans(position, rotation); status = m_pxScene-\u0026gt;overlap(PxSphereGeometry(radius), trans, buf, filterData, \u0026amp;filterCallback); if (status \u0026amp;\u0026amp; buf.nbTouches \u0026gt; 0) { hitResults.clear(); for (PxU32 i = 0; i \u0026lt; buf.nbTouches; i++) { PxOverlapHit hit = buf.touches[i]; PhysXRaycastHit hitResult; hitResult.Collider = (PhysXActor*)hit.actor-\u0026gt;userData; hitResults.push_back(hitResult); } } return status; } Golden Tips Make sure shape dimension and queryshape dimension have positive values, recommended minimum value is 1.192092896e-07F. Or random crash may happen. In Raycast or Sweep, make sure direction is normalized and not zero. Or random crash may happen. In Overlap, do not check hit.distance (it\u0026rsquo;s always zero) in post-filtering logic. ","permalink":"https://pps43.github.io/posts/using_physx_scenequery/","summary":"本文主要介绍PhysX场景查询的一些内部机制和使用方法。 PhysX物理引擎系列记录了在实际项目中使用Nvdia PhysX 3.4物理引擎（Code, D","title":"PhysX物理引擎（1）Scene Query"},{"content":"上一篇我们聊了AI模型的分类和大致架构，目前最火的两类模型LLM和Diffusion都基于Transformer架构训练而成，ChatGPT则是LLM家族中最出色的产品。本文就来聊聊最近出现的基于ChatGPT的创新架构和应用。由于ChatGPT的训练数据截止到2021年9月，且笔者并没有Plus会员才能使用的ChatGPT Plugin，所以这次请到的是Dr. Phind（The AI search engine for developers），大家欢迎。\n系列文章\n跟AI老师学AI（一） 跟AI老师学AI（二） 课堂问答 Auto-GPT Me:\nwhat is autogpt and how does it work\nDr. Phind:\nAuto-GPT is an experimental open-source project built on the latest ChatGPT model i.e GPT-4. It is a variant of ChatGPT developed by Significant Gravitas and it uses the ChatGPT API to work. Specifically, Auto-GPT utilizes the GPT-4 API, though it seems like it should work with the GPT-3.5 API as well. Auto-GPT is an AI that does the work for you by setting a goal and then generating and completing tasks. It does all the follow-up work for you, asking and answering its own prompts. It automates multi-step projects that would have required back-and-forth prompting if you worked directly with GPT-4. In other words, it \u0026ldquo;chains together LLM \u0026rsquo;thoughts\u0026rsquo;, to autonomously achieve whatever goal you set.\u0026rdquo;\nThere are new apps that have emerged to make Auto-GPT even easier to use, like AgentGPT and GodMode, which provide a simple interface where users can input what they want to accomplish directly on a browser page.\nMe:\nhow does AutoGPT work exactly? Find the techniques behind.\nDr. Phind:\nAuto-GPT is a Python application built on top of GPT-4 that allows the AI to function independently without requiring prompts from the user for each action.\nAuto-GPT breaks down the AI’s actions into “thoughts,” “reasoning,” and “criticism,” which enables users to understand precisely what the AI is doing and why it’s doing it.\nAuto-GPT uses long and short-term memory management, which is similar to how humans learn from their mistakes. Auto-GPT has the ability to assess its work, improve upon past experiences, and leverage its history to generate more precise results. Auto-GPT\u0026rsquo;s integration with vector databases, a memory storage solution, allows it to retain context and make informed decisions.\nMe:\nlist some autonomous agent besides autogpt\nDr. Phind:\nhere are several other autonomous AI agents besides Auto-GPT, some of which are Camel, BabyAGI, and Westworld simulation.\nCamel is an autonomous AI agent that uses LangChain to store and retrieve information. It can learn and understand human language and is capable of making decisions based on the context. Camel can generate content for social media and can perform simple tasks like booking a restaurant reservation or scheduling a meeting (Reference).\nWestworld simulation is an autonomous AI agent that simulates the Westworld environment. It uses reinforcement learning to improve its decision-making ability and can learn from its mistakes. Westworld simulation can interact with other agents in the environment and can perform tasks like navigating through the environment, recognizing objects, and even interacting with human.\n⚡Me: In fact it\u0026rsquo;s an interactive sandbox environment created by researchers from Stanford and Google, the paper is here (PDF) and this is the basic architecture. BabyAGI is another autonomous AI agent that uses GPT-4, Pinecone, LangChain, and Chrome to generate and execute tasks. It can make human-like cognitive decisions and can write and run codes to meet certain objectives. BabyAGI can retain long-term memory of tasks and events with the help of Pinecone and LangChain, making fetching information faster. Its decisiveness makes it an effective tool for cryptocurrency trading, robotics, and autonomous driving.\n⚡Me: add info from Github: the script works by running an infinite loop that does the following steps: These autonomous AI agents have different capabilities and approaches to accomplishing tasks. Camel is useful for generating content for social media and performing simple tasks, Westworld simulation is useful for simulating environments and interacting with other agents and BabyAGI is useful for decision-making sectors like autonomous driving and robotics.\nBabyAGI Me:\nwhat is babyagi and compare with autogpt?\nDr. Phind:\nBabyAGI and Auto-GPT are autonomous AI agents designed to generate and perform tasks based on a given objective. The primary difference between the two is the way they approach the objective, the techniques they use, and their purpose.\nBabyAGI uses a combination of GPT-4, Pinecone, LangChain, and Chrome to create and execute tasks. It creates a task list based on the objective entered and executes them one by one. The results of one task determine the next one. It can retain a long-term memory of tasks and events with the help of Pinecone and LangChain, which makes fetching information faster. Since it uses a trial-and-error basis to decode the results of previous tasks, BabyAGI can make complex decisions without losing track of the predefined objective.\nAuto-GPT, on the other hand, generates and runs multiple tasks at once using GPT-4 and creates an artificial memory space using GPT-3.5 to store results from previous tasks. It has access to internet services like apps, websites, books, documents, and articles and can use them to complete tasks required to reach the objective. Auto-GPT is good at generating text-rich content and images. It can also provide human-like text responses, making it useful for generating content, summarizing texts, and translating text into more than a dozen languages\nVector Database Me:\nwhat is vector database and how does it integrate into above system?\nDr. Phind:\nVector database is a type of database that is designed to handle high-dimensional data such as embeddings, which are numerical representations of objects or concepts in a high-dimensional space. In the context of Auto-GPT, vector databases are used to store the embeddings of documents that are generated by OpenAI\u0026rsquo;s language model. The vector database is used to enable similarity search over these embeddings, which is used by Auto-GPT to find the most relevant documents to complete a task.\n(Me: Below is just one example to integrate pgvector to auto-gpt, there are other choice, e.g., weaviate)\nThe vector database is integrated into the Postgres database using pgvector, an open-source vector similarity search extension for Postgres. In more detail, open-source vector similarity search extension pgvector allows for storing and querying vector embeddings in a Postgres database. OpenAI\u0026rsquo;s text-embedding-ada-002 model outputs 1536 dimensions, so that is the size of the vector used in Auto-GPT. The pgvector extension introduces a new data type called vector, which is used to create a column named embedding with the vector data type in the Postgres database. The similarity search over these embeddings is performed using a function named match_documents, which takes a query embedding, a match threshold, and a match count as inputs and returns a table of document IDs, content, and similarity. The similarity is calculated using vector math operations like cosine distance. (Reference)\nMe:\nlist some popular vector database product or repo\n(Dr.Phind\u0026hellip;. Google and Google Bard comes to help):\nSure, here are some popular vector database products or repos:\nFAISS (Facebook AI Similarity Search) is a library for efficient similarity search of dense vectors. It is open source and available on GitHub. Milvus: Milvus is a vector database designed for high-performance similarity search and machine learning.Milvus 2.0 is a cloud-native vector database with storage and computation separated by design. All components in this refactored version of Milvus are stateless to enhance elasticity and flexibility. Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data. weaviate These are just a few of the many vector database products and repos that are available. The best product or repo for you will depend on your specific needs and requirements.\nLangChain Me:\nwhat is langchain and how does it work?\nDr. Phind:\nLangChain is a Python package that provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications [3]. It is designed to assist in four main areas, which are listed here in increasing order of complexity: prompts, long-term memory (LLMs), tools, chains, agents, and memory [2].\nLangChain is a framework for developing applications powered by language models. It is created to be data-aware and agentic, meaning that it connects a language model to other sources of data and allows a language model to interact with its environment.\n(\u0026hellip;) LangChain can be used in a variety of ways, including autonomous agents, personal assistants, question answering, chatbots, querying tabular data, code understanding, interacting with APIs, summarization, and evaluation.\nMe:\ndoes AutoGPT use Langchain?\nDr. Phind:\nThere is no mention of AutoGPT using LangChain in the provided context. While LangChain is a framework that makes it easier to build scalable AI/LLM apps and chatbots, there is no clear indication that AutoGPT specifically uses LangChain.\n课后笔记 Autonomous agents (e.g., AutoGPT, BabyAGI) can auto-create tasks towards the GOAL you give them, and auto-find the way to complete task based on GPT models and other data sources (websites, your documents, etc), and even auto-improve themselves from previous mistakes. The basic structure is somehow similar to GOAP in video game\u0026rsquo;s AI design. They can achieve these because they can have long-term and short-term memory powered by Vector Database. ( memory size is much more than single GPT4\u0026rsquo;s context which is 32K tokens, ~24K words.) Vector Database is a specialized db to handle vectors (read, write, calculate similarities, etc). they are useful for text because text can be transformed into numberic vectors by embedding technique. Similarity between embeddings (vectors) reflect relevance between text. Hence above agents can retrieve relevant data from large datasource and sending to GPT as context. Frameworks like LangChain makes it easier to implement autonomous agents because it provides a standard interface to handle different tools and data sources. 拓展阅读\nTutorial: ChatGPT Over Your Data (using LangChain) Create your own chat bot with GPT-Index \u0026amp; LangChain ","permalink":"https://pps43.github.io/posts/asking_ai_about_ai_2/","summary":"上一篇我们聊了AI模型的分类和大致架构，目前最火的两类模型LLM和Diffusion都基于Transformer架构训练而成，ChatGPT","title":"跟AI老师学AI（2）"},{"content":"按照目前AI在学术界、工业界推陈出新的速度，感兴趣的同学们纷纷表示：真的学不动啦！不要慌，AI老师来了~ 在本系列中，我不会提前学习预备知识，以门外汉的视角去追问AI并了解一些基本概念。既可以锻炼提问技巧，还可以在听完高屋建瓴的讲解后有针对性的去学习感兴趣的部分，效率倍增！\n系列文章\n跟AI老师学AI（一） 跟AI老师学AI（二） 课堂问答 Generative Models, Transformers, Large Language Models (LLM) ? Me:\nexplain generative models and list some types of it. And explain the relationship and differences with LLM.\nChatGPT:\nGenerative models are a type of machine learning model that learns to generate new data that is similar to the training data it was trained on. Unlike discriminative models, which learn to classify input data into specific categories, generative models focus on learning the underlying patterns and structure of the input data, allowing them to generate new data that is similar to the input data.\nThere are several types of generative models, including:\nVariational Autoencoders (VAEs) Generative Adversarial Networks (GANs) Autoregressive models, such as PixelCNN and PixelRNN Boltzmann Machines Restricted Boltzmann Machines (RBMs) Deep Belief Networks (DBNs) Language Models (LMs) are a type of generative model that specifically focuses on generating natural language. Like other generative models, LMs are trained on a large corpus of text data and learn to generate new text that is similar to the input data. However, LMs differ from other types of generative models in that they specifically focus on generating text and incorporate language-specific features, such as grammar, syntax, and vocabulary.\nOne key difference between LMs and other types of generative models is that LMs are typically based on probabilistic models, such as n-gram models or neural network models, whereas other types of generative models, such as GANs and VAEs, are typically based on deep learning architectures. Additionally, LMs may incorporate language-specific constraints, such as word order and grammar, that are not present in other types of generative models.\nMe:\nDo you mean generative models are unsupervised learning, while discriminative models are supervised learning?\nChatGPT:\nGenerative models can be trained using both supervised and unsupervised learning approaches, depending on the specific task and the type of data being used. In unsupervised learning, the model is trained on a dataset without explicit labels, and the goal is to learn the underlying structure and patterns in the data. In supervised learning, the model is trained using labeled data, and the goal is to learn to predict the label of new, unseen data.\nHowever, it is true that discriminative models are typically used for supervised learning tasks, such as classification, regression, and sequence labeling. These models learn to map input data to specific output labels or categories. In contrast, generative models are more commonly used for unsupervised learning tasks, such as density estimation, data synthesis, and anomaly detection. These models learn to generate new data that is similar to the training data, without necessarily mapping input data to specific output labels or categories.\nMe:\nGot it. Now explain what transformers is about in deep learning, and the relationship between generative models and the transformers.\nChatGPT:\nTransformers are a type of deep learning model that has become increasingly popular in natural language processing (NLP) tasks, such as language modeling, machine translation, and sentiment analysis. The Transformer architecture was introduced in a 2017 paper titled \u0026ldquo;Attention Is All You Need\u0026rdquo; by Vaswani et al., and it has since become the foundation for many state-of-the-art NLP models.\nThe key innovation of the Transformer architecture is the use of self-attention mechanisms to replace traditional recurrent or convolutional neural networks. Self-attention allows the model to weigh the importance of different parts of the input sequence when generating each output token. This enables the model to capture long-range dependencies and context more effectively, resulting in better performance on many NLP tasks.\nGenerative models, such as language models, can be built using the Transformer architecture. In fact, many of the most successful language models, such as OpenAI\u0026rsquo;s GPT series and Google\u0026rsquo;s BERT, are based on the Transformer architecture. These models are trained on large amounts of text data using unsupervised learning techniques, such as autoregressive language modeling or masked language modeling. Once trained, the models can be used to generate new text that is similar to the training data, or to perform downstream NLP tasks, such as sentiment analysis or machine translation.\nIn summary, the Transformer architecture is a powerful tool for building deep learning models for NLP tasks, including generative models such as language models. By leveraging self-attention mechanisms, Transformers are able to capture long-range dependencies and context more effectively, resulting in better performance on many NLP tasks.\nModels, Architectures? Me:\nDo you mean one model can be implemented in different architectures, and transformers are just one of the architectures to build models? If this is true, list some other architectures popular.\nChatGPT:\nYes, that\u0026rsquo;s correct. A single machine learning or deep learning model can be implemented using different architectures, depending on the specific task and the type of data being used. The choice of architecture can have a significant impact on the performance and efficiency of the model.\nIn addition to the Transformer architecture, there are several other popular deep learning architectures for building models in various domains. Here are a few examples:\nConvolutional Neural Networks (CNNs): These are commonly used in computer vision tasks, such as image classification, object detection, and segmentation. Recurrent Neural Networks (RNNs): These are often used for sequence modeling tasks, such as natural language processing, speech recognition, and time-series prediction. Long Short-Term Memory Networks (LSTMs): These are a type of RNN that are designed to handle long-term dependencies in sequences. Autoencoders: These are used for unsupervised learning tasks, such as feature extraction, data compression, and anomaly detection. Generative Adversarial Networks (GANs): These are used for generative modeling tasks, such as image synthesis, data augmentation, and video prediction. Variational Autoencoders (VAEs): These are also used for generative modeling tasks, such as image synthesis and data generation, but they incorporate probabilistic models and encode latent variables. These are just a few examples of the many deep learning architectures that are available for building models. The choice of architecture will depend on the specific task and the characteristics of the data being used.\nDiffusion, Stable Diffusion, latent space? Me:\nExplain stable diffusion to me. Is it a model, architecture, or others?\nChatGPT:\nStable Diffusion is a probabilistic model for generative modeling and data synthesis, and it is related to the concept of Generative Diffusion Models (GDMs).\nAt a high level, the Stable Diffusion model uses an iterative diffusion process to model the distribution of the data. The diffusion process involves iteratively adding noise to the input data and then progressively filtering out the noise using a series of conditional distributions. This process results in a sequence of probability distributions, each of which approximates the true data distribution more closely than the previous one.\nUnlike traditional diffusion models, which can suffer from instability due to the accumulation of noise over time, the Stable Diffusion model incorporates a stability regularization term to prevent this from happening. This regularization term encourages the diffusion process to converge to a stable distribution, resulting in more accurate and reliable generative models.\nThe Stable Diffusion model is closely related to Generative Diffusion Models (GDMs), which also use a diffusion process to model the data distribution. However, GDMs suffer from instability due to the accumulation of noise over time, and they require careful tuning of the learning rate to avoid this. The Stable Diffusion model addresses this issue by incorporating a stability regularization term, which improves the stability and reliability of the model.\nIn summary, Stable Diffusion is a probabilistic model for generative modeling and data synthesis that uses an iterative diffusion process and a stability regularization term to improve the stability and reliability of the model. It is closely related to Generative Diffusion Models (GDMs) but incorporates additional features to address the instability issues that can arise in traditional diffusion models.\nMe:\nDo you know stable diffusion 2? It\u0026rsquo;s said to be a latent diffusion model, what does it mean?\nChatGPT:\nYes, Stable Diffusion 2 is a follow-up work to the original Stable Diffusion model and is a latent diffusion model.\nA latent diffusion model is a type of generative model that learns a low-dimensional representation (or latent space) of the data that captures the underlying structure of the data. In other words, instead of modeling the data directly, the model learns to generate the data by sampling from a distribution in the latent space and then mapping the samples back to the data space.\nStable Diffusion 2 extends the original Stable Diffusion model by introducing a latent diffusion process. The model learns a diffusion process in the latent space, which allows it to generate data that has a more complex and structured distribution than the original model. The latent diffusion process involves iteratively adding noise to the latent space and then filtering out the noise using a series of conditional distributions. This process results in a sequence of probability distributions in the latent space, each of which approximates the true data distribution more closely than the previous one.\nOnce the model has learned the diffusion process in the latent space, it can generate new data by sampling from the final distribution in the latent space and then mapping the samples back to the data space. By using a latent diffusion process, Stable Diffusion 2 is able to generate data that has a more complex and structured distribution than the original model, making it suitable for more challenging generative modeling tasks.\nIn summary, Stable Diffusion 2 is a latent diffusion model that learns a diffusion process in the latent space to generate complex and structured data distributions. It is a more advanced version of the original Stable Diffusion model and is suitable for more challenging generative modeling tasks.\nMe:\nexplain latent space like I\u0026rsquo;m 5\nChatGPT: 👍\nImagine you have a big box full of toys. Each toy has different colors, shapes, and sizes. Now, imagine you have a secret map that tells you how to mix and match the toys to create new toys that no one has ever seen before. This secret map is like a code, and it\u0026rsquo;s hidden inside the box.\nThe box of toys is like the data, and the secret map is like the latent space. The latent space is a hidden space that tells you how to mix and match the different parts of the data to create something new. It\u0026rsquo;s like a secret code that only the computer knows.\nWhen you train a model to learn the latent space, it\u0026rsquo;s like teaching the computer to find the secret map inside the box of toys. Once the computer knows the secret map, it can create new toys that no one has ever seen before by mixing and matching the parts of the toys in new and interesting ways.\nSummarize the talk Me:\nSummarize our conversation above to give a thorough and concise relationship among all the models, architectures, and other important concepts we talked about.\nChatGPT:\nIn summary, we discussed several important concepts in deep learning, including generative models, discriminative models, language models, and architectures such as Transformers and Generative Adversarial Networks (GANs).\nGenerative models are used for unsupervised learning and aim to model the data distribution to generate new data. Discriminative models, on the other hand, are used for supervised learning and aim to learn the decision boundary between different classes.\nLanguage models are a type of generative model that learns to predict the probability of a sequence of words, given some context. Transformers are a popular architecture used for language modeling, which use a self-attention mechanism to capture long-range dependencies in the input.\nGenerative Adversarial Networks (GANs) are a type of generative model that uses two neural networks, a generator and a discriminator, to generate new data that is similar to the training data. The generator tries to create new data that can fool the discriminator, while the discriminator tries to distinguish between the generated data and the real data.\nFinally, we discussed Stable Diffusion and Stable Diffusion 2, which are probabilistic generative models that use a diffusion process to model the data distribution. Stable Diffusion 2 is a latent diffusion model that learns a diffusion process in the latent space to generate complex and structured data distributions. The latent space is a hidden space that tells the model how to mix and match the different parts of the data to create something new.\n课后笔记 We train a model by choosing an architecture. Some models are named after the architecture they use, e.g., GAN, VAE. We fine-tune existing model as a foundation to generate a new model, with less training effort. LLM and Diffusion are both generative, probabilistic models. ChatGPT is a product powered by GPT model (belongs to LLM family). Stable Diffusion is a Diffusion model, and its sevond version learns pattern in a hidden box called latent space. Above models all use Transformers as the architecture. Transformers are a type of attention-based neural network. mindmap root((Deep Learning)) models discriminative model K nearest Logistic regression SVM Random Forest ... generative model deterministic CNN ... probabilistic VAE GAN LLM Diffusion architectures CNN LSTM GAN VAE ... Transformer BERT GPT ... ","permalink":"https://pps43.github.io/posts/asking_ai_about_ai/","summary":"按照目前AI在学术界、工业界推陈出新的速度，感兴趣的同学们纷纷表示：真的学不动啦！不要慌，AI老师来了~ 在本系列中，我不会提前学习预备知识，","title":"跟AI老师学AI（1）"},{"content":" \u0026ldquo;There are decades where nothing happens; and there are weeks where decades happen.\u0026rdquo; ― Vladimir Lenin\n基于大语言模型的AI在这个月带给人们的感受，用列宁的这句话概括再贴切不过了。作为普通人，去拥抱这些AI工具，就像会使用智能手机和搜索引擎；对人类来说，就像学会用电，学会用火。\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39; } }%% timeline title Era of AI comes in 2023 2-7: Microsoft New Bing 3-12: Open AI ChatGPT 90% cheaper 3-15 : Open AI GPT-4 3-16 : Microsoft Copilot : Midjourney V5 : Google PaLM API 3-21 : Adobe FireFly : Nvdia GTC 3-22 : Github Copilot X : Google Bard 3-24 : Open AI ChatGPT Plugins 更新：4月以来，AI应用的新概念、新架构、新产品如寒武纪大爆炸一般涌现（AutoGPT 首当其冲），非人力所能穷举。 这个网站收录了大量AI工具，本文也会持续更新笔者常用、觉得好用的工具。\nFor General Purpose Open AI\u0026rsquo;s ChatGPT, and Plugins. GPT3.5 is free to use. Microsoft\u0026rsquo;s NewBing. It\u0026rsquo;s said to be powered by GPT4 (internal version). Google\u0026rsquo;s Bard. Tips on chatting effectively:\nUse Englisih. Use precise verb. Only one topic at a time. No \u0026ldquo;thanks\u0026rdquo; and interrupt in time. Use role-play. Act as a travel guide. ... Chat to multiple GPT instances with slightly different views, as if they are expert team. After trying many LLM, ChatGPT is still the best one to be professional and smart. But I still prefer asking different models to get different points of view. Some common tips when asking:\nRole play. act as .... Here is a collection of role-related prompts. Give template input-output. Tell chatgpt to anwser step by step. For Doc Edge + NewBing. Explain any webpage (including PDF) side by side. ChatDoc/ChatPDF, upload PDF and analyze. ⏳Microsoft\u0026rsquo;s Copilot. For Software Development Github Copilot (based on OpenAI\u0026rsquo;s CODEX), costs $10/mo after 60d trial. Github Copilot Chat, previous named Github Copilot X, based on GPT4. Integrated to many IDE as plugins. Updated on December 4, 2023. Use @workspace to query information about your VS Code workspace! Use #file:xxx to inquire about a specific file! Analyze terminal errors by clicking the \u0026ldquo;sparkling\u0026rdquo; icon! Summarize PR (Pull Request) for you! Ask questions by speaking, via the VS Code Speech plugin.\nphind, the AI search engine for developers. Cursor editor, or vscode plugin CodeCursor, read/write current document/code, FREE to try. ⏳Copilot for Docs, used to learn a SDK/framework/API, can based on private content. The gist to generate code is, to describe a single-responsibility function to let AI generate, rather than a function with long description of chained operations.\nFor 3D/2D Art Stable-Diffusion (SD) web-ui, totally free and opensource, run model locally on PC.\nDownload/Share models on civitai/Hugging face Use ControlNet (Github )to add more controll on specific SD model. Use LoRA (Low-rank adaption) to train faster with less memory. Use Text Inversion to train with amazingly small output. Use [DreamBooth] to train if you need to be really expressive. Midjourney, famous for its artistic style, 25 times FREE try.\nBuilt on Discord Bot, thus you can use Official API or thirdparty lib to automate the flow. Adobe\u0026rsquo;s Firefly\nOpen AI\u0026rsquo;s DALL-E-2, generates image with natural language and long prompts, but limited-access and less control.\nBing\u0026rsquo;s Image Creator, generate image with natural language, and free to try.\n[Only preview] styledrop\n[Only preview] muse\nFor Music Mubert Soundraw.io Want more power? If you want to:\ntrain your own AI based on these models know the strength and weakness of current AI models know why \u0026amp; how Generative AI works, mathematically Here are my personal ideas:\nFor text, play with LLaMA/llama.cpp, or its fined tuned version Alpaca/Alpaca-LoRA. For image, play with Stable-Diffusion and its plugins. They can run on PC/Mac. Weakness of current LLM models: math; chain of decision. But they are improving. \u0026ldquo;Dive into Deep Learning\u0026rdquo; by 李沐。中文版《动手学深度学习》 Hardware considerations Training on cloud is cheaper and least effort to start. (Google\u0026rsquo;s Colab is even FREE) Training on local hardware, if use multiple GPUs (with NVLink), traffic bandwidth between GPUs is the botthleneck. (DGX A100 specs: 8xA100 GPUs, total 640GB VRAM, 600GB/s GPU-to-GPU bandwidth.) ","permalink":"https://pps43.github.io/posts/ai_toolkit_for_dev/","summary":"\u0026ldquo;There are decades where nothing happens; and there are weeks where decades happen.\u0026rdquo; ― Vladimir Lenin 基于大语言模型的AI在这个月带给人们的感受，用列宁的这句话概括再贴切不过了。作为普通人，去拥抱这些AI工具，","title":"Generative AI Toolkit (5.12)"},{"content":"市面上以DevOps为主题的书，以及和项目攻关的影视作品有不少了，但这本小说读起来依然紧张刺激。除了揭示管理现代IT组织与管理传统工厂的共通之处，书里更强调了以一种全局的视角来重新看待自己的工作环境，和自己在工作中扮演的角色，并且，其包含的问题分析方法、实践方法也或多或少可用于个人效能的优化。\n下面是一些书摘和简评。\n面对压力和现实 比尔，我知道你没有申请这个职位，但公司已命悬一线，我需要你来帮助我拯救这家伟大的公司。我能指望你吗？还没来得及再次礼貌谢绝，我突然听到自己说“可以，你可以指望我。”我慌了，强迫自己住嘴，以免做出更多愚蠢的承诺。……“我会尽力的，还有，能不能请你至少解释一下，为什么在这个位子上的人都干不长？你最希望我做什么，最不希望我做什么？”\n临危受命时，一定要弄清楚领导的意图和前任失败的原因。\n我还来不及回应，莎拉就大声说：这恰恰表明，比尔及其团队缺乏对于紧迫性的必要认知。追求完美是成事的大敌，比尔，我们可没有闲工夫为了迎合你的黄金标准而精雕细琢……难道这不是有点太轻率、太不公平了吗？但莎拉不屑一顾的说：我相信决定已经做出了。……接下来的9天里，我们所有人都要熬夜加班了。这种全员出动的工作状态是IT人生活的一部分，但是想到我们又得因为其他人疏于计划 而不得不奋力拼搏，我还是有些恼火。\n在市场负责人莎拉拿出一切为了项目这个尚方宝剑时，由于没有实际数据支撑，只因为道理而不去做是不够的，所以只能先忍下来。反倒是不顾实际情况强行推进项目发布的莎拉，得到了CEO史蒂夫的支持。因为CEO更关心股价、以及董事会对他的看法！\n我设法复述那些缜密理性、条理分明的论点，他们是我花了整个周末的事件排练的……我一边说一边不断观察史蒂夫，到目前为止，他一直面无表情。……史蒂夫愤怒的回答：什么优先级高不高的狗屁问题？……我在心里默数三下才开口：当然，我表达的不够清楚，……我们的基础架构太过脆弱……好吧，我们会尽最大努力，但我要郑重声明，我们的人手严重不足，无法高质量完成其中任何一项工作，更别说全部了。……比尔，凤凰项目已经超支1000万美元，我们必须马上得到正向现金流。增加任何预算都是不可能的，如果会有什么调整，我们可能还得在你的部门减掉几个人。……离开的时候，我把花费整个周末时间准备的演示稿扔进了垃圾桶。\nCEO有他的计算方式和危机视角，所以主角上来摆事实说道理不会得到支持。\n公司高管强迫我的工程师优先执行他们的命令，这完全是胡闹。我（对布伦特）补充道：如果有人为了凤凰项目之外的事和你联系，就把他们推给韦斯（直接上级），让他去对付那帮笨蛋。无论如何都要想办法改正大家直接来找你帮忙的坏习惯。我允许你把电话设成静音，把状态设置成你没空。随便怎么样都行。\n布伦特是一名对系统了解最多的明星员工，所有重要的系统变更都需要他帮忙，但他就像一个黑盒，是系统的约束点。首要任务是发现并保护系统的约束点。\n我惊讶的目瞪口呆：史蒂夫，情况得有多糟糕你才肯推迟这次发布？我告诉你这次是运行将是非常鲁莽的冒险！……虽然很不情愿，但我还是觉得自己欠公司最后一搏，去阻止这次疯狂的行动。……（强行发布后）……史蒂夫停下脚步，用手指着我的前额说：我对责任的理解，比你一辈子学的还要多！我受够了你整天嚷嚷着天要塌了，时候在高高兴兴地说‘我早就告诉过你了’。你得带着实际的解决方案来找我。……我需要业务部门告诉我，他们不再受部门IT部门的钳制。我担任CEO依赖，一直都听到这样的投诉，IT拖累了每一项重要举措。\n原来是这种发酵很久的情绪带来了偏见，导致史蒂夫讨厌比尔的稳扎稳打，偏爱莎拉的冒进。\n（史蒂夫讲述自己的个人经历和弱点，并鼓励会上的每一个人都这么做）展现自己脆弱的一面有助于建立起信任的基础。\n约翰气急败坏的说：你以为你是谁？我在努力保持这家公司的安全，让那些审计师离得远远的！我……得了吧，只会帮倒忙的CISO先生。埃瑞克打断他说，正如你刚才看到的，不用你出手，这家公司就能让审计师离得远远的。你就像个管道工，不知道自己在为一架飞机服务，更别说了解飞行路线，或者航空公司的营业状况了。……你真的不明白，是不是？无极限零部件公司最大的风险是停业破产，而你似乎一心想用你那些不周全的考虑和无关紧要的技术细节，让他加速倒闭。怪不得你会被边缘化！……\n约翰身上有几个问题：\n没有理解总目标。数据安全是重要，但公司和业务都要黄了，保证数据安全只是空谈而已。 没有理解业务全流程。事后他才知道即使前序的IT系统中有各种安全隐患，但后续每一笔交易都有人工审核，所以审计师并不会来找麻烦。 没有真正理解自己的职责和能力如何助力于总体目标。所以他倡导的数据安全在其他人看来只有妨碍作用。他在该阶段真正的作用在于用经验和能力减少其他部门在安全上的无效努力。 克里斯首先回答：我之前就说过，就连次要的漏洞修复都问题重重，我们不能承受每月一次的发布……史蒂夫回答：成败就看这个季度。我们向世人许诺过，会在上个月把凤凰弄出来。……我们没有时间了。我对克里斯说，如果你说凤凰团队应该放缓速度，我不会有异议。但是，我们仍然要想办法满足史蒂夫的要求，如果我们不能再凤凰的框架内做成这件事，也许可以在凤凰之外做到。我提议可以从凤凰主团队分出一小队人，组建特别行动队。\n老项目老流程积重难返，这里提供了一种减少掣肘的思路。后文能看到，特别行动队奏效了，而且其优秀的工作流还被原凤凰团队逐步借鉴，形成反哺。\n建立认知 半成品是个隐形杀手。\n因此，管理任何一家工厂最关键的机制之一，就是工作任务和原材料的发布。没有这个机制就无法控制半成品。为了停止半成品在工作流中堆积，即使其他人闲得无聊，也不应该在瓶颈已经达到饱和的情况下，继续制造半成品。\n在瓶颈之外的任何地方做出的改进都是假象。\n作为IT运维部的副总裁，你的工作时确保形成一条迅速、可预测、持续不断的计划内工作流，从而向业务部门交付工作价值，同时尽可能降低计划外工作的影响和破坏。我不管每个人觉得自己的项目有多重要，我们要知道的是，该项目能否提高我们在约束点上的工作能力。\n实际上有四种类型的工作：来自业务方的、自身基础架构的、操作变更、计划外工作。\n计划外工作是恢复性工作（比如屎山代码出BUG后的救火），占用你的时间并阻碍完成前三种正向产出的工作。如果不加控制，技术债务将导致公司里能够唯一完成的工作就是计划外工作。\n流程是用来保护人的。\n根据韦斯讲的故事，我们甚至都不该让布伦特碰到键盘，他可以告诉大家应该输入什么，但在任何情况下，都不准他做哪些我们无法在时候记录的事……每解决一个问题，我们的知识库里就会多一篇关于如何解决某个疑难杂症的文章，而且能够实施修复的人会越来越多。\n这里提到的解决疑难杂症的文章，类似我在微软工作时组内特别重视的postmortem，也类似达利欧在桥水特别推行的错误日志。目的是同样的错误绝不犯两次，而且下次其他人也可以处理。\n一旦通清楚最常出现的任务是什么，就需要建立起工作中心和工作路径。\n你必须跳出原来的专有领域，才能弄清楚整体的成功需要你的哪些工作来达成。\nIT是一种技能，就像能读会算一样。理解技术能够做什么，不能做什么，已经成为一家公司每个部门必须具备的核心竞争力之一。\n创建一个让人感觉无能为力的系统，使我们能对人类同胞做的最具破坏性的一件事——我们剥夺了他人控制自己成果的能力，甚至营造了一种文化，让人们因为害怕遭受惩罚、失败而不敢做正确的事。这制造了“习得性无助”的环境，人们变得不愿或无法采取行动来避免未来遇到同样的问题。\n三步建立高效的工作流 目标：流量最大化；可灵活应对调整。\n第一工作法 明确价值链。从最终的绩效指标反推，其实现依赖什么，以及不满足时有什么风险。 最大优化正向工作流。 让等待时间可视化。在工厂的流水线上在哪里出现拥堵或空转显而易见，但IT或日常生活中很容易忽略（又叫暗时间）。Kanban是一种比较好的方式，注意只有在生产环境里成功运行起来、或开始产生价值的时候，才能算“完成”。 减少中断。生产中断在制造业里显眼且代价极高，所有的半成品都将报废。但技术工作者很容易被打断，因为后果不可见。通过严格限制多任务的数量、新任务的插入减少打断的次数。 杜绝缺陷向下游传递。发现难题，群策群力，建立机制而不是以后再说。 减小交付的内容大小和等待间隔。 持续识别、改善约束点。约束点可以是一个人，或者大家都倚赖的同一种资源、前置步骤。以DevOps为例，通常要依次优化以下约束点：1、环境搭建；2、代码部署；3、准备和执行测试；4、架构耦合。 消除浪费。例如：1、半成品（文档、变更单等）；2、多余工序（对后续流程无价值）；3、任务切换；4、资源竞争和等待；5、非标准或手动操作。 第二工作法 快速、持续的获得反馈。否则没人敢在一个复杂系统中放心的工作（不用担心自己的某一个操作在将来搞坏系统中另一个本不相关的功能）。 仍以微软为例，Substrate仓库里有多个M365相关的产品，约5000+工程师维护着上百GB的源码文件，即使穷尽一生也没有人能充分了解这样一个已经存活几十年的庞然巨物。然而这样一个系统的迭代速度却非常惊人，保障这个工作流的其中一个方式是：\n每一次代码提交都会触发多种自动化测试和模拟部署，在几分钟到几十分钟内告诉你是否可以将代码合入主干。 同时会自动@所有相关模块的负责人在DevOps系统中进行Review，直到所有利益相关者同意合入。 第三工作法 建立具有创意和高可信度的文化。它强调每个人都是持续学习者，必须勇于承担风险，通过科学的方式改进。彼此分享经验，经验积累沉淀。\n将日常工作的改进制度化 将局部的优秀经验全局化 领导层为团队创造学习条件，领导力并不体现在做出的所有决定都是对的。 ","permalink":"https://pps43.github.io/posts/notes_on_phoenix_project/","summary":"市面上以DevOps为主题的书，以及和项目攻关的影视作品有不少了，但这本小说读起来依然紧张刺激。除了揭示管理现代IT组织与管理传统工厂的共通","title":"凤凰项目"},{"content":" 近日读完《贫穷的质感·王梆的英国观察》（豆瓣读书，Goodreads）一书后，将只言片语组合起来作一点记录。此书说了很多英国普通人的事情、社会传统，但相信我们读起来应该不会太陌生。透过诸多生活中的观察，自然引申出养老、医疗、社会制度、逆全球化等话题。\n只言片语 英国政府曾推出大量廉租房，老旧了便拿来卖钱。由于是市面价格的一半，非常受炒房客的欢迎。有钱人买来继续出租，租金比原来还要高。但由于政府不再负责修缮，这些房子逐渐变得又贵又脏。\n斯蒂芬·阿姆斯特朗指出：穷人不是贫困的罪魁祸首，而是他的产物。贫困也不可能通过自律自愈，紧缩只会导向更极致的贫穷。\n19世纪的改革家为底层人民指出的通往中产之路是五条箴言：勤劳，坚持，节俭，审慎，自力更生。但社会现实是，权贵们掌握着话语权，一旦你陷入贫穷，此前的辛勤付出全不算数，社会对你的困顿只有一条解释：懒惰。\n私人养老院的比例急剧上升，很多投机商大举投资养老院，经营不善则扔给政府逃之夭夭。收费998英镑一周。没钱了怎么办？可以卖房或者先住，死后房子被收走。但一般房产的价格只够住四年。工资低雇员少，虐待事件因此极为普遍。几乎每所大学都在想方设法取消全职合约，因为这样就能省掉不少职业养老金了。\n脱欧是这么多年来他们（农民，流水线工人等等）所获得唯一一次发声，是他们想象中对抗外来移民、以及国际垄断资本主义带来的困苦生活状态的一种方式。\nKeep calm and carry on. 这句话自维多利亚时代就被英国社会视为行为典范。恐慌是可耻的。正如贫穷是可耻的。疫情后，一部分人过上了在家办公的生活，另一部分人则成为“前线工人”（看护，餐饮工作等）。自封城后，人们对前线工人的赞美开始层出不穷，但这改变不了他们的高死亡率。\n1863年，古典自由主义者引领了废奴运动并获得巨大成功。全球化后，新自由主义者（大资本家）却用自由市场重新包装了蓄奴主义，且丝毫不觉有任何道德负担。吊诡的是，许多被损害的人也保持着同样的道德优越感：自由市场是完美的，穷人是不幸的，因为他们天生智商低下。富人是优胜劣汰的产物，通过创造财富缴纳税金，连同穷人一起受益。\n相关阅读 Why Poverty? (PBS documentary films) Poor Economics (中译名《贫穷的本质》)，豆瓣读书, Goodreads ","permalink":"https://pps43.github.io/posts/notes_on_texture_of_hard_times/","summary":"近日读完《贫穷的质感·王梆的英国观察》（豆瓣读书，Goodreads）一书后，将只言片语组合起来作一点记录。此书说了很多英国普通人的事情、社","title":"贫穷的质感（王梆）"},{"content":" 本文总结自Ray Dalio: How the Economic Machine Works，原视频地址\n核心概念简述 什么是交易 买方使用货币、信用（也叫信贷）向卖方交换商品、服务、金融资产。 交易首先是由人的天性驱动的。 买方的支出总额（而不是其他）是经济的驱动力，所有的经济周期和动力都是交易造成的。 交易的主体通常有：个人、企业、银行、政府。 政府如何参与交易 政府其实同时是最大的买方和卖方。 政府主要由两部分：中央政府，负责收税；中央银行，负责花钱。 中央银行通过利率和印钱控制经济中的货币和信用（信贷数量）。 什么是信贷 信贷也是一种交易。比如你找银行借钱，你是借款人，银行是贷款人。你是交易的卖方，银行是买方。 这种交易的特殊之处在于：交易是延迟完成的，即借款人保证将来偿还贷款和利息，直到那个时候，交易才算完成。 交易未完成前的状态，即债务。债务是贷款人的资产，借款人的负债。 只有交易完成时，这些资产和负债才会消失。 什么是价格 $$ 价格 = \\frac{买方支出总额}{产销总量} =\\frac{(货币 + 信贷)} {产销总量} $$\n什么是市场 一种市场，包含同一种商品的所有买方和所有卖方。有千千万万种商品或服务对应千千万万种市场。\n什么是经济 全部市场的全部交易统称为经济。\n经济形势变化的三条脉络 这三个相对独立的因素叠加起来，构成经济形势的变化：\n生产率的提高（近似线性） 债务小周期（5~8年） 债务大周期（75~100年） 深入经济周期 经济为什么依赖信贷 简而言之，因为借款人通过信贷可以增加自己的支出，而支出是经济的驱动力。在没有信贷的社会中，增加支出的唯一方式是增加生产。由于信贷可以凭空产生，支出可以超越当前生产，所以能让经济注入更多活力。\n更具体的说，交易双方中，A的支出是B的收入，A支出增加等于B收入增加，B偿还能力提升，信贷能力也会提升，B的支出可以进一步增加，在另一场交易中，B的支出增加等于C的收入增加……产生循环。不过要注意，借钱实质是向未来某一时刻的自己借钱，即未来那个时刻自己的收入减掉消费必须足够还钱才能保证交易最终的有效性。\n经济上下起伏不取决于人们多么善于创新或勤奋工作，而看信贷的总量。而人的天性和借贷机制决定了信贷总量一定是波动的。\n短期信贷周期为什么出现 考虑单一商品的情况。如果支出（消费）或者收入（消费是另一个人的收入）的增长速度超过所出售商品的生产速度，商品价格就会上涨。央行为了防止通货膨胀，会提高利率。利率直接影响人们贷款和借款意愿——这也是限制支出的唯一因素。还贷增加造成可支配的支出减少，商品价格下跌。\n当信贷易于获得则经济扩张，信贷不易获得则经济收缩，这个周期由央行主导，一般5~8年。\n长期信贷周期为什么出现 经历一个短期周期后，经济增长和债务规模都会超过前一个周期。债务规模的扩大其实正是人的天性：更喜欢消费而不是还钱。因此经历若干短周期后，债务增加的速度其实会超过收入（虽然收入也在增加）。到那时，社会总债务会变得过重而导致支出、收入、信用的全面下降，这个过程叫去杠杆化。\n社会总债务：政府债务、居民债务、非金融企业债务。社会总债务过重一般是指超过GDP的好几倍。\n去杠杆化过程开始后，会出现资产价格下跌、企业破产、信贷消失、无法正常还贷，只能变卖资产，进一步造成资产价格下跌，形成恶性循环，且无法通过降低利率摆脱（降到0以后就到底了）。例如欧美1929年、2008年，日本1989年。\nflowchart LR 资产价格下跌 --\u0026gt; 企业或个人破产 --\u0026gt; 信贷消失 --\u0026gt; 变卖资产 -.-\u0026gt; 资产价格下跌 根据具体情况的不同，该周期可能持续75~100年。\n有哪些方式解决债务过重问题 衡量债务负担是否过重：\n$$ 债务负担 = \\frac{债务}{收入} $$\n解决债务过重的四种主要手段：\n所有交易主体削减支出；\n因为A的支出是B的收入，B收入也下降了，那么B的债务负担更重，所有人更痛苦。会演变成通货紧缩。 例如企业为了削减成本造成大量人口失业。 通过判定违约和重组减少债务；\n大量借款人无法偿还即债务违约后，贷款人会向银行索要存款，造成挤兑，银行自身也会违约。这就叫经济萧条，即所有人都感觉自己的资产蒸发了。 比债务违约稍好一点的方式是债务重组，即还款减少或延期或利率降低。 不论哪种方式，由于A的债务是B的资产，债务消失或减少，意味着B的资产减少。依旧会演变成通货紧缩。 政府财富再分配；\n在去杠杆化的过程中，伴随着失业人口上升，因此政府支出变多甚至财政赤字，必须收更多的税或发行债券。 向谁融资或征税呢？通常是富人。但容易引发社会动荡。如果是国与国之间，会引发战争。 央行增发货币；\n前文提到央行的两个手段：利率和印钞。利率降低到0之后，只能印钞。 印出的钞票只能用来买入金融资产和政府债券。前者可以保住金融资产（例如股市）的价格，因此提高了持有人的信用（但仅仅对持有金融资产的人有用），后者其实是把印出的钱借给政府，由政府通过救济金和刺激计划增加人们收入，实质上是将经济系统分散的总债务，集中到政府的头上。 增发的货币刚好抵消贷款的降幅，就不会引发通货膨胀（参考前文说过的价格公式）。温和的通货膨胀本身对经济并不有害。但增发太猛会导致恶性通货膨胀，例如1920年的德国。 结合具体情势，通过巧妙地组合使用上述四种手段，才可以做到和谐的去杠杆。也不要太悲观，因为和谐的去杠杆化过程比信用扩张时期更好，因为此时经济是在自然缓慢增长的，夯实根基。\n反思 上述模型大致可以判断当前我们处于哪个阶段，预测未来大趋势如何。 上述模型解释了经济周期的必然性：现代社会的经济发展依赖信贷，信贷依赖机制和人性，机制有一定的调整空间（但交易延迟完成和要连本带利是刚性的）、人性则不可避免的包含贪婪、恐惧、喜欢消费而不是还钱、从众…… 上述模型对于个人的启发意义： 不要让债务的增长速度超过收入，因为最终会让债务把你压垮； 不要让收入的增长速度超过生产率，因为最终这会让你失去竞争力； 尽一切努力提高生产率，因为它在长期起最关键的作用。 ","permalink":"https://pps43.github.io/posts/how_the_economic_machine_works/","summary":"本文总结自Ray Dalio: How the Economic Machine Works，原视频地址 核心概念简述 什么是交易 买方使用货币、信用（也叫信贷）向卖方交换商品、服务、金融资产。 交易首先","title":"经济机器如何运行（达利欧）"},{"content":" 本文简要介绍了利用GPU进行通用计算的历史和方法，以NVDIA家的CUDA架构为例，介绍GPGPU编程的重要概念和简单实践。希望能够在浩如烟海的网络资料之外，给像我这样的初学者提供一些清晰的参考。CUDA C的学习资料在文末给出，NVDIA的官方文档干净友好，听说社区生态也堪称典范，好感度+1。\n简介 历史 近30年来，提升CPU计算性能的主要方法就是提升时钟频率。此方法在目前的硅基半导体工艺下已进入瓶颈，转而用“并行”来提升计算性能（例如多核）。\n自1980年代起，一家名为Silicon Graphics的公司在业界推广OpenGL用于渲染三维图像，获得了广泛支持。NVIDIA、ATI等公司也开始对计算机3D图形计算加速进行研究，并将此硬件设备称为“GPU”以区别于传统的CPU。GPU由于其结构特点，具备强大的并行计算能力。（1999年，GPU的始祖GeForce 256横空出世。2002年完整支持DX9的王者Radeon 9700Pro正式开启了GPU的众神时代，Age of Gods. 显卡爱好者可以看看此文：视觉时代的回响 GPU十年历史追忆。）\n在很长一段时间里，利用GPU进行渲染是通过OpenGL和DirectX进行的。人们发现传给GPU进行像素点渲染的颜色、纹理坐标等信息其实可以是任何数据，即GPU可以用于通用计算（General Purpose GPU）。然而当时的GPU有着难以在显存任意位置进行读写、GPU本身也不具备浮点计算能力、GPU难以debug等严重缺点。更糟的是，必须使用一种叫“shader language”的编程语言，利用OpenGL和DirectX提供的API对GPU进行操作。利用GPU进行通用计算的巨大的学习成本，使得研究人员望而却步。\n对于上述硬件缺点，NVDIA推出了基于CUDA架构的GPU，如2006年上市的高端卡GeForce 8800 GTX；对于上述软件缺点，NVDIA在C的基础上进行扩展，并在GeForce 8800 GTX上市后几个月后公布CUDA C语言及相应的编译器用于GPGPU的编程。\n现状 性能上，就个人电脑来说，目前的顶级CPU，主频也很难上4GHz，而核心数量很难达到16个（计算能力约64GFLOPS）。而稍微高端点的GPU早就超过3000GFLOPS（以GTX 690为例，核心频率约1GHz，3072个核心）。国外已有建立在Tesla k40集群上的超算中心。下面一图以蔽之吧。 GPGPU应用：耳熟能详的GPU应用诸如：视景仿真、视频渲染。以及石油勘探、环境科学、航空航天和流体力学计算等领域。举几个典型且较新的例子：医疗成像。智能汽车。深度学习。 概念 硬件 Device, Host\nCPU和内存（host memory）是host，GPU和显存（device memory）是device。 SM, SP\nSM：Stream-Multiprocessor，俗称“大核”。SP：Stream-Processor，俗称“小核”。 Global memory\n就是通常意义上的显存。 Shared memory\n为同一个block中若干threads所共享。 存取非常快，可看作user-managed cache。CUDA C中的关键词：__shared__ Constant memory\n并行层次 并行是一个概念，可以用在不同的层面。CUDA中从高到低有以下几个层次：\nstream -\u0026gt; grid -\u0026gt; block -\u0026gt; (warp) -\u0026gt; thread.\n并行层次与存储层次是对应的。\nthread 并行的基本单位。同一个block中的thread通过shared memory共享数据。同一个block中的thread通过__syncthreads()同步，具体见下一节。 warp In the world of weaving, a warp refers to the group of threads being woven together into fabric. In the CUDA Architecture, a warp refers to a collection of 32 threads that are “woven together” and get executed in lockstep. At every line in your program, each thread in a warp executes the same instruction on different data. When it comes to handling constant memory, NVIDIA hardware can broadcast a single memory read to each half-warp. A half-warp—not nearly as creatively named as a warp—is a group of 16 threads: half of a 32-thread warp. If every thread in a half-warp requests data from the same address in constant memory, your GPU will generate only a single read request and subsequently broadcast the data to every thread. If you are reading a lot of data from constant memory, you will generate only 1/16 (roughly 6 percent) of the memory traffic as you would when using global memory. 最大支持512个threads。只用一个block不能充分利用GPU的资源。 grid We call the collection of parallel blocks a grid. 同一个grid中的所有thread都执行相同参数的核函数。 stream A stream is a sequence of commands (possibly issued by different host threads) that execute in order. this behavior is not guaranteed and should therefore not be relied upon for correctness (e.g., inter-kernel communication is undefined). 不同stream可执行不同参数的核函数。 通信机制 Inter-thread level\nshared memory. 要求是同一个block中的threads。 内置函数 __syncthreads(). 作用是当某个线程执行到该函数时，进入等待状态，直到同一线程块（Block）中所有线程都执行到这个函数为止。 原子操作。可以跨block。 Inter-block level\nInter-stream level\n实践 环境搭建 采用64位win7 + visual studio 2013 + cuda toolkit 6.5 。早期的cuda环境需要下载cuda toolkit、drivers、cuda SDK，现在全部打包到一起了。Cuda Toolkit 6.5下载地址：官网 关于软硬件兼容性的问题，2010年的老显卡（指笔者的Quadro NVS 3100M 。怎么样，没听过吧，说多都是泪……此卡介于G210M~G310M之间）仍可以安装 6.5 版，基本可以放心下载。 完整的搭建过程参考官方文档。\n常见问题 warning: C4819\n只需在项目properties-\u0026gt;configuration properties-\u0026gt;CUDA C/C++的Additional options那里加一行：-Xcompiler /wd4819 error：expected and expression (不能识别\u0026lt;\u0026lt;\u0026lt;)\n首先检查出错的代码文件的属性中，item type是不是CUDA C/C++。如果是，则没有大碍，只是VS的C/C++文本编辑器认为有错，编译器可以通过。这里提到一种自定义宏的方法消除这个小bug。 error：addKernel launch failed: invalid device function\n常用工具 deviceQuery.exe\nCUDA Samples中自带的检测显卡信息的工具。可以方便的查看显卡的基本信息。如： 型号 Compute capability 核心数量（大核SM、小核SP） 时钟频率 global memory const memory shared memory/block 最大threads、block、grid限制等 bandwidthTest.exe\nCUDA Samples中自带的测试Device和Host之间带宽的工具。还有个重要功能是搭建CUDA环境后验证是否正常运转（看最后Result是否为PASS） Visual Profiler\n","permalink":"https://pps43.github.io/posts/cuda_programming_gpgpu/","summary":"本文简要介绍了利用GPU进行通用计算的历史和方法，以NVDIA家的CUDA架构为例，介绍GPGPU编程的重要概念和简单实践。希望能够在浩如烟","title":"GP-GPU编程：CUDA介绍"},{"content":"更新了下Notepad++后，自动打开了一个文档，一个个单词自动蹦出来……差点吓尿——这是什么鬼？屏幕上的光标终于不动了……是一段话。我这里把原文贴出来：\nFreedom of expression is like the air we breathe, we don\u0026rsquo;t feel it, until people take it away from us. For this reason, Je suis Charlie, not because I endorse everything they published, but because I cherish the right to speak out freely without risk even when it offends others. And no, you cannot just take someone\u0026rsquo;s life for whatever he/she expressed. Hence this \u0026ldquo;Je suis Charlie\u0026rdquo; edition.\n我查了一下Je suis Charlie，意思是“我是查理”（这又是什么鬼）。\n继续搜，才发现《查理周刊》是一个法国讽刺漫画杂志。最近在世界掀起轩然大波的一部分穆斯林们，屠杀了此刊相关的12人，以及多人受伤。穆斯林接触的不多，伊斯兰教谁爱信信去是他的自由，只是听说伊斯兰教徒认为别人没有不信他们的先知的自由。以下文字摘自百度百科：\n《沙尔利周刊》又翻译为《查理周刊》，创建于1970年，法文名为 Charlie Hebdo，是法国的一家讽刺漫画杂志。2012年在法国发行量已达4.5万份。该周刊经常刊登宗教和政治类报道。\n2015年01月7日，位于巴黎的法国讽刺杂志《沙尔利周刊》总部遭到武装分子袭击，造成12人死亡，另有多人受伤。这是近些年来法国遭受的最为严重的恐怖袭击。法国检方调查人员说，当天，几名武装人员手持AK47以及火箭弹发射器，袭击了《沙尔利周刊》总部，造成包括周刊主编及4名漫画插图师在内的至少10人死亡。随后，一名枪手劫持了一辆汽车，在与警方交火时，还打死了2名警察。有视频显示，其中1名警察是在倒地后被补枪身亡。\n警官孔滕托说，有10人在这起事件中受伤，其中也包括1名警察。根据他的描述，事发现场如同“大屠杀”场面，有警车的玻璃上被打了十多个洞。\n当天，目击者布兰热接受当地电视台采访时说：“我看见两名用黑布蒙面的男子持冲锋枪进入杂志社大楼。几分钟后，我听到许多声枪响。”布兰热说，他看见这两名男子随后逃走。法国电视台援引警方的消息说，还有目击者在现场听到，一名武装分子在袭击时，曾高呼“我们已经为先知复仇了！”\n2015年1月14日，“基地”组织阿拉伯半岛分支发布视频称，该组织策划并发动了针对法国《沙尔利周刊》的袭击事件。这是惨剧发生一周以来，首次有组织宣称对此事负责。\n原来如此。先默哀下。苟活者在淡红的血色中，会依稀看见微茫的希望；真的猛士，将更奋然而前行。\n","permalink":"https://pps43.github.io/posts/je_suis_charlie/","summary":"更新了下Notepad++后，自动打开了一个文档，一个个单词自动蹦出来……差点吓尿——这是什么鬼？屏幕上的光标终于不动了……是一段话。我这里","title":"我是查理"},{"content":" 新学期，和诸位打个照面~既然没有什么干货，不妨先散扯一通，聊聊学习，聊聊 Scientific Thinking，聊聊经济学和通信，等等。\n一个学习能力强、效率高的人，相比常人更经常有意识地思考自己的核心技能是什么。从专业上来看，物理出身的人、经济出身的人、通信出身的人、法律出身的人等等——其专业提供其的认识论和方法论都不同。可能经济出身的人习惯于把一个现象通过供需平衡、边际成本、效率和公平等角度理解；搞通信的习惯于通过信息量、统计量、系统输入输出、相关性等角度来分析；搞物理的可能先@%^\u0026amp;$!#……“不论以后干什么，珍惜你的专业背景”这句话的含义就是不要舍弃专业带给你的这话总工具和视角。\n从多个领域分析问题——这种能力我做梦都想得到。我就琢磨，琢磨一个人从小学以来的学习经历中能提炼出什么“万金油”式的指导方针，使你按照这个方针成为一个博学而智慧、乐观而包容的学习者和教育者。下面两个词是我的结论：\nSystematic Adaptive 说白了，我们通过不断的学习，在构建属于自己的系统，容纳我们对这个世界的所有认知和对尚未认知领域的做法。从通信和信号处理的视角很好解释，意会意会。\n对了，什么叫Scientific Thinking呢？单单说其有四个阶段：inquiry, analysis, inference, argument似乎空洞了点，我就不成体系的先说我感受比较强烈的几点：\nHypothesis 一言以蔽之：\nThe art of scientific thinking is deciding which assumptions to make. \u0026ndash;Mankiw\nModeling 太重要了有木有！人人都知道好的模型可以事半功倍，但我想说的是不妨从简陋的模型开始，不要一口吃个胖子。学医学的朋友对教学时使用的人体模型再熟悉不过，没有人会把这种模型当成真人，但这说明这个模型的作用不大吗？恰恰相反，正是这种简化，使人们集中精力学习某一个器官或系统，而不用在于那些无关的毛细血管或者毛发。模型过于逼真，反而让人无从下手研究。\n在无线通信领域，无线信道建模是很头疼的，不像《通信原理》课程中的信道那么简单（但同样很有用）。目前理论分析中广为使用的statistic input/output model可以说其基于的假设简直让人坐立不安。但老鸟David Tse这么说了：\nWe need such models, even if they are quite inaccurate. Without models, systems are designed using experience and experimentation, and creativity becomes somewhat stifled. Even with highly over-simplified models, we can compare different system approaches and get a sense of what types of approaches are worth pursuing.\n\u0026ndash;David Tse\n所以，不要轻视简单模型作用。\n所以，当看到 rational people 这个有点反人类的假设时，也不要对经济学的那一套分析一笑而过。看了几天曼昆，发现经济学真是个宝贝。不知道多少人觉得经济学的标签是：金钱、市场、GDP、股票……不错，但我更欣赏这种观点：经济学是一门研究有限资源如何分配、系统内的角色如何决策、系统内的事件如何发展的科学。\n今天就扯到这里，院长一会回来看我还没吃药就完了。\n","permalink":"https://pps43.github.io/posts/thinking_of_scientific_thinking/","summary":"新学期，和诸位打个照面~既然没有什么干货，不妨先散扯一通，聊聊学习，聊聊 Scientific Thinking，聊聊经济学和通信，等等。 一个学习能力强、效率高的","title":"Scientific Thinking? No, 杂谈です"},{"content":"范数、内积空间的概念这里不做赘述，只点几个要点。\n粗糙的说，对于V(F)空间，定义内积为一种将两个属于此集合V的元素与一个来自数域F的元素对应起来，并且满足一些抽象但自然的性质（不举）。为了方便将两个元素的内积记为：\\(\u0026lt; \\bf{a,b} \u0026gt; ，\\bf{a,b}\\) 属于V。\n而范数也是一种将V中元素映射成F中的一个数的方法。映射后需要满足的性质如：非负性、齐次性、三角不等式。满足这种性质都可以叫求范数，常见的范数有：1-范数，2-范数，无穷范数。记为：\\({\\rm{||}}{\\bf{a}}{\\rm{||}}\\)\n二范数和内积的关系可以写为：\\({\\rm{||}}{\\bf{a}}{\\rm{||}} = \\sqrt { \u0026lt; {\\bf{a,a}} \u0026gt; }\\)\n可以证明一下几个运算在对应的空间中都满足内积的定义。\n内积空间满足以下两个重要的不等式：\n基础的准备工作就这么多。下面通过发现几个问题的相似性，提出用内积表示的统一形式，其实这些问题的解就是此形式在不同内积空间二范数下的特例。\nP阶线性预测 SimonHaykin《通信原理》P224\n求解一系列\\(w(k)\\)，其解是 wiener-hopf 方程，在以前的文章中有详细解释。\nLMMSE参数估计 很多书都会提及。根据观测数据\\(y\\)求某个待估参数\\(\\theta\\)。最一般的表达式如下：\n$${\\bf{\\theta }} = {C_y}^{ - 1}{C_{\\theta y}}$$\n其实是和P阶线性预测本质是一样的，但仔细看看却有差别。聪明的读者，你们看出来了吗？（此句是向某些恼人的书致敬，每当到关键点，作者就会这样搪塞过去）。\n最佳平方逼近 常见于数值分析。\n首先聊两句插值和逼近的区别。插值要求在插值点上与待插函数完全相同，用一个简单函数近似待插函数。逼近则不要求某些点上函数值一定相同，而是在一段区间上给出准则，然后最小化。其中一个准则便是最佳平方逼近。\n再多说一句，函数可以看做无穷维向量，用无穷维向量的角度看函数，有种触类旁通的感觉。\n统一形式 简写为：\n$${\\bf{Ax = b}}$$\nA矩阵式正定的，而且是Hermite阵。应该可以用 Levinson Dubin 递归法快速求解。\n整理到这里会觉得自己弱爆了，不过是数学上已经有的东西，好比我只是将其重新抄了一遍放在这里。\n这确实有点意思。\n","permalink":"https://pps43.github.io/posts/thinking_of_norm2/","summary":"范数、内积空间的概念这里不做赘述，只点几个要点。 粗糙的说，对于V(F)空间，定义内积为一种将两个属于此集合V的元素与一个来自数域F的元素对应","title":"二范数下几个问题的统一解释"},{"content":"在\u0026quot;信道仿真模型 3GPP-SCM\u0026ldquo;一文中提到了WINNER模型是诺西牵头搞的原3GPP SCM的增强版，关于WINNER模型的官方介绍参见：http://www.ist-winner.org/\n考虑到篇幅，官方将winner模型的文档分成上下两册。上册为主，着重介绍信道模型结构和参数，下册为辅，详细记录了信道测量和分析。另有一份关于matlab实现的说明文档。\nwinner模型是一种geometry-based、stochastic模型，自scm发展而来。其物理参数采集自欧美的城市、乡镇或农村。\n在仿真时：信道参数根据一定的统计分布随机生成；天线图样从模型中剥离，由用户决定；信道实现由详细的小尺度参数（delay、power、AoA、AoD）通过模拟多径叠加完成。这种多径叠加得到信道的方式体现了geometry-based，在MIMO规模较大时，仿真复杂度反而要比correlation-based方法低（参见文献1的3.6.3节）。这种叠加造成了：1、天线元间的信道相关性；2、依赖于地理环境的多普勒频谱的瞬时衰落（不太理解）。\nwinner模型框架又可以细分为：generic channel model（GCM）、 clustered delay line（CDL）等。主要特性通过GCM介绍，CDL作为链路级仿真时减小计算量所使用的替代模型，和Tapped Delay Line模型类似。\nGCM有三个层次的随机性。从上到下依次为：\n大尺度参数（LSP）根据分布函数表随机生成（LSP的空间相关性参见文献1的3.3.1节）； 小尺度参数根据大尺度参数和分布函数表随机生成； 各散射体的初相位随机生成。 从多径角度看信道构成，是rays–\u0026gt;paths(clusters)–\u0026gt;links。从时间上看信道构成，是drops–\u0026gt;segments。每个segment中的LSP是固定的。而每个drop中除了rays的相位，其他参数也都是固定的。drops之间一般彼此不相关，除非选择使用time evolution。time evolution的实现目前只提供一种Basic Method：在时间上划分出若干子区间，逐次用新cluster替换掉旧cluster。\n至此，模型完全非随机了。\n如果只看文献1，其所介绍的特性是明晰的、周全的。但目前的matlab实现代码提供的和文献1中规定的尚有出入，使用者必须亲自实践才能略知一二。而且文献2中涉及的数据结构、参数设置等话题过于庞杂，故以后有时间再续。被频繁关注的Antenna Field Pattern、扇区化、布局等话题在文献2中都有详细的介绍。下面列举一些实践中让我读了文档后仍感到困惑的地方：\n将100MHz带宽通过DownScaling转化成20MHz的具体步骤？matlab版本的代码实现支持此功能吗？\n文献2中没有讲到DownScaling\nDelaySamplingInterval的设定有哪些讲究？与系统带宽有关吗？\n从文献1中看，此参数只在DownScaling中被提及。文献2中对此参数的解释是：DelaySamplingInterval determines the sampling grid in delay domain. All path delays are rounded to the nearest grid point. It can also be set to zero.\nTimeSamplingInterval的设定受SampleDensity的影响，设定上有什么讲究？\n不详。\nDrop和Segment等效于多少个sample？（有时为了研究100个sample能不能将信道遍历，需要知道这个细节）\n可以在文献1的表4.5中查到Segment的折算后的长度。对于drop，其说法是：\nWhen obtaining channel parameters quasi-stationarity has been assumed within intervals of 10-50 wavelengths. Therefore we propose to set the drop duration corresponding to the movement of up to 50 wavelengths. In a simulation, the duration of a drop can be selected as desired. It is a common practice to use drops in the simulations…\n但文献2中并未对这一要求是怎么实现的加以说明，也没有明确给出drop和segment和sample的对应关系。所以samples之间的相关性和遍历性一概无从得知了。\n天线设置中有个rotation的参数，[Rotx，Roty，Rotz]的含义是？\n根据单位是rad，通过网上搜集资料，最为接近的理解来自这里。后来又翻出来rotate_vector.m这个函数，总算看明白了，同网页上讲的一致。\nIST-WINNER D1.1.2 P. Kyösti, et al., “WINNER II Channel Models”, ver1.1, Sept. 2007. Available: https://www.ist-winner.org/WINNER2-Deliverables/D1.1.2v1.1.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hentilä, P. Kyösti, M. Käske, M. Narandzic, and M. Alatossava. (2007, December.) MATLAB implementation of the WINNER Phase II Channel Model ver1.1 [Online]. Available: https://www.ist-winner.org/phase_2_model.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://pps43.github.io/posts/using_winner2_channel_model/","summary":"在\u0026quot;信道仿真模型 3GPP-SCM\u0026ldquo;一文中提到了WINNER模型是诺西牵头搞的原3GPP SCM的增强版，关于WINNER","title":"信道仿真模型 WINNER II"},{"content":"在\u0026quot;衰落信道小议\u0026ldquo;一文中提到了MIMO信道的建模。好的信道模型是以下三个标准的折中：\n准确性。 计算可行。 一般性（或者说灵活性）。 3GPP提出的SCM（Spatial Channel Model）是符合以上标准的、基于地理的概率模型（Geometry Based Stochastic Model），即使用若干传播路径进行叠加，每条路径的强度和延迟符合一定的随机分布。SCM被WP5小组采用研究室外的MIMO信道，后经其手拓展成SCM-E（E for Extension）模型，再后来WP5还是不满意，便另起炉灶开始了WINNER系列信道模型的制定及其实现，这是后话，放在下一篇里说。\n推荐从SCM开始学习信道仿真。说到底信道仿真就是通过一系列配置，生成某场景下的信道H。接下来的文字认为大家已经看过或正在看以下两个文档：\n[1] 3GPP TR 25.996 V10.0.0, “Spatial channel model for MIMO simulations”, Mar. 2011 [2] 3GPP TR 25.996, “MATLAB implementation of the 3GPP spatial channel model”, July 2006.\nSCM整个模型分为两块分别用于系统级仿真、链路校正。常用的是系统级仿真，即 [1] 第5章的内容。第4章那么长很容易让人找不到要领。\n要说的是，SCM信道参数很多，一定要全部掌握而不能依靠默认配置，从而获得似是而非的仿真结果。SCM为了计算方便，对很多参数有假设限定，例如认为用户之间即使靠的很近，也认为Shadow Fading的系数不相关。又例如：AS，DS，SF参数在所有扇区是一致的。天线配置理论上是自由的但实际只可以设定成0.5/4/10倍波长。假设延时抽头数目小于等于6，这就限制了最大带宽。等等。\n5.4节给出了计算subpath上的信道增益的原理，这正是前文提到的 Ray-Tracing 方法。\n5.5节是一些附加选项，如极化天线、视距信道等等。5.6节讲述了信道系数之间的相关性是怎么选取的。很遗憾，SCM中对inter-cell correlation的处理是给定一个B矩阵，这也表明了SCM是 入门级 的东西。\n5.7节，点了一下多小区干扰的建模。\n在提供的matlab仿真程序中，对参数 SampleDensity（信道生成使用的采样密度，和多普勒频移参数有关）和Ts（用于将delay从具体时间转化到抽头数，也是通信系统的采样率）说明的不完善，需要自己玩味。另外，最终生成的信道存储在\\(\\bf{H}\\)中，是一个五维的矩阵，每一维的具体含义文档里说的很清楚了： \\({\\bf{H}}(i,j,k,p,q)\\)表示第\\(q\\)条链路在第\\(p\\)个采样时间点上，从天线\\(i\\)到天线\\(j\\)之间delay为\\(k\\)秒的信道增益。\n","permalink":"https://pps43.github.io/posts/using_scm_channel_model/","summary":"在\u0026quot;衰落信道小议\u0026ldquo;一文中提到了MIMO信道的建模。好的信道模型是以下三个标准的折中： 准确性。 计算可行。 一般性（或者说灵","title":"信道仿真模型 3GPP-SCM"},{"content":" 无线通信中对信道建模一直是一件比较头疼的事。就现在无线通信物理层最活跃的Massive MIMO方向来说，有效的统计信道模型尚未有组织提出，可见对信道建模，不存在一劳永逸的方法。这里只谈像我这样的初学者入门时，学习衰落信道过程中的困惑和思考。虽然叫“小议”，就是想写尽量通俗，但估计条理还是会有点乱~\n常见的信道建模方法及其利弊 Ray-tracing Model 就是研究若干条反射路径在空间中某一点的叠加。优点：与物理世界结合紧密，比较贴合初学者的直观感受。由此导出的重要参量如多普勒频移、时延扩展具有很强的现实意义。相信看David Tse（Tse是一位文风飘逸的男子，是香农学生的学生）入门的一定有同感。 该模型缺点也很明显：太依赖物理参数导致工程上不可用。\n上面的斜体字，现在看来太武断了。时代在发展，为了充分空分复用，功耗低、天线架设低的微基站为显著特征的 small cell 概念正在冉冉升起。但是，当小区半径很小时，其他模型的统计特性很难满足，这时ray-tracing model的普适性就体现出来了。\n华为主办的研究生数学建模竞赛有一题就是让你做ray-tracing建模。研究空间相关性、频率相关性等等。也许华为要开始做small cell了吧。\nPower-Decay Model 这个A. Goldsmith的书讲得比较多，是基于实际测量发现的。我们知道在自由空间中球面电磁波的能量是按照距离的平方率衰减的：\\({P_r}(r)={P_t}/{r^b}\\)\n但在高楼林立的环境下，由于吸收、散射等影响，上式的参数b比2要大。在城市宏小区环境下甚至可以达到6.5（Goldsmith书表2-2）。这么差的性能，比学通信原理时介绍的AWGN信道要差很多（相干BPSK调制下为达到同样误码率，无线衰落信道中要比AWGN信道多付出17dB，证明见Tse Ch3.1）。回到这个模型上，由于是经验模型，而没有揭示其信道特性和物理规律，所以仅用来做估算。\nRandom walk Model 比较非主流，但是观点挺新。也可以导出些有意义的结果，详情wiki。\nStatistical Model 终于讲到正题。对于\\(y=hx+w\\)，将h建模为一个具有易于分析的统计特性的随机量，来包含衰落信道的影响：多径造成的时延和移动造成的多普勒扩展。基于一些不太准确的假设，再用大数定理来说服听众，最终导出的结果是：h是循环对称复高斯分布的。其模服从瑞利分布，相位服从均匀分布。对于习惯了教条式地分析AWGN信道的学究来说，对一个特性未知的物理信道这样建模是不负责任的，对此，David Tse做了很精彩的反驳：\nOn the other hand, we need such models, even if they are quite inaccurate. Without models, systems are designed using experience and experimentation, and creativity becomes somewhat stifled. Even with highly over-simplified models, we can compare different system approaches and get a sense of what types of approaches are worth pursuing.\n采用Tse的复数离散基带模型来讨论衰落信道，写作\\(h_L[m]\\)。\\(m\\)表示信道的时变性，\\(L\\)称作tap，表示信道的频率选择性或者时延特性。\n一些看书时的难点和疑问 信道时变性怎么就跟多普勒频移联系在一起？ 信道频率选择性怎么就跟多径时延联系在一起？ 信道带宽W越大，多普勒扩展越小，原因是？ 相干时间和相干带宽不是倒数关系。那它们有没有内在联系？ 信道时变的影响因素是什么？别急着答，其实分布在三个时间尺度上。 我的解答：\n多普勒频移（频域） ← 天线运动 → 信道变化（时域） 信号带宽（频域） ← 码元长度 → 路径延时（时域） 码元长度和路径延时扩展二者的大小直接决定了信道的特性：frequency-flat（窄带，即延时扩展远小于码元长度）还是frequency-selective。下图就反映了\\(W\u0026raquo;Bc\\)时的情况。 每个tap的间隔是\\(1/W\\)，\\(W\\)越大，(解析度越高)，每个tap中做出贡献的路径越少。当每个tap中路径变少后，信道的时变性其实是减弱的：例如观察\\(L=1\\)的tap，若有1条路径，则\\(h_1[m]\\)的变化只有这一条路径引起。若有2条路径，两条路径的时变相加（从而加重了时变，使得多普勒扩展减小）。所以路径比较少时时变比较小。 相干时间是从信道的时变性上得出的概念，与多普勒扩展呈倒数关系。相干带宽是从信道的频率选择性上得出的概念，与时延扩展呈倒数关系。非要画在一张图里，就是这个样子：（Simon Haykin, Communication Systems） 大致有三个尺度 即使接收天线位置不变，信道随着环境也会有微妙的改变；在环境变化不太快的情况下，通常是数秒。 当接收天线在较大范围内移动后，一条路径从一个tap转移到另一个tap造成\\(h_L[m]\\)发生改变；公式应该是这样的：\\(t=c/(W*v)\\)，c是光速，W是带宽，v是运动速度。以W为20MHz，v=15m/s，则时间尺度为1s。窄带情况下\\(W\u0026laquo;f_c\\)（载波），tap之间的时间距离较大，所以对时变特性影响弱于多普勒扩展。UWB时，tap跳动非常快，\\(h_L[m]\\)的随\\(m\\)的变化就很剧烈了。 MIMO信道建模方法利弊 也许大家看多天线的时候早已习惯了将接收天线数M，发射天线数N的MIMO信道写作MxN矩阵：\\({\\bf{H}}\\)。其实，要加上衰落信道中的时变因素和频率选择性因素，\\({\\bf{H}}\\)应该是4维矩阵。在学术讨论中，省去时变这个维度是不得已而为之，否则会带来很多不仅是计算而且是概念上的麻烦。比如：三维矩阵的协方差阵的含义？所以干脆规定讨论的这一段时隙内，MIMO信道是时不变的。\n但是，LTE动辄20MHz的带宽，怎么会没有频率选择性呢？读者可以估算一下20MHz的平衰落信道对时延扩展的要求是多么苛刻。所以省去频率选择性这个维度表面看起来并不合理。\n不过，OFDM的应用解决了这个问题。OFDM变换实际上是DFT变换，其将\\({\\bf{H}}\\)分解到大量并行子载波上，每一个子载波的带宽较小（记得LTE规定子载波频率间隔15kHz），其具有足够的频率平坦性。插一句，将信道分解成多个无相互干扰的并行子信道并不是OFDM的专利，简单的SVD分解也可以达到这一效果，不过SVD分解依赖信道\\({\\bf{H}}\\)的具体实现，而OFDM直接用DFT矩阵就好。OFDM这么厉害，但是单独使用性能却很糟糕，最大的问题就是峰均比没法控制。关于OFDM，本篇不展开，因为已经达到了目的：将MIMO和OFDM的手自然地牵到了一起。\n","permalink":"https://pps43.github.io/posts/thinking_of_fading_channel_models/","summary":"无线通信中对信道建模一直是一件比较头疼的事。就现在无线通信物理层最活跃的Massive MIMO方向来说，有效的统计信道模型尚未有组织提出，可","title":"衰落信道建模小议"},{"content":"今天老师上课提到“Cybernetics”这个词，中文翻译为：控制论。控制论的开山鼻祖是Wiener（上篇文章 提到过他）。 我就好奇，Cybernetics怎么能就翻译成“控制论”了呢？去图书馆查阅了一番，才从第一批引进此书的老一辈科技工作者的文字中，了解到了其中的曲折。\n其一：正名 上世纪50年代初，苏联给Cybernetics定性为“一种反动的伪科学，认为宣扬“人是机器”，进而组织批判。当时的中国，因为要学习和组织批判，就将这些理论加以翻译和引进。但私下里窃以为这些是先进的好东西。不过表面上还是带着或多或少的批判眼光，于是将Cybernetics翻译成：人脑机械论。\n没过几年，苏联开始为Cybernetics“平反”。译丛编译部就找到中宣部科学处的同志讨论这个词的新译法。考虑到其希腊语词源：κυβερνητικός 以及这门科学的现实内涵，龚育之等人将中文译法商定为“控制论”。之后这个译法普及开来，沿用至今。\n这一点在wiki里得到佐证：Cyber- is a prefix derived from “cybernetic,” which comes from the Greek adjective κυβερνητικός meaning skilled in steering or governing (Liddell and Scott, Greek-English Lexicon).\n其二：作者郝季仁 1956年，“百花齐放、百家争鸣”，特别提到自然科学是没有阶级性的。中宣部科学处果断任命罗劲柏、龚育之、侯德彭、陈平四人开始翻译《控制论》这部旷世著作。\n然而1957年，四人中有两人被打成右派，而按当时规定，右派不能用真名出版著作。几人商定，干脆用个假名：郝季仁。郝季仁者好几人也。\n回头看控制论这本中译本，突然有种来之不易的感觉。\n其三：姊妹篇 维纳在写下这部学术巨著的末尾，谈到了自动化社会的后果，表示了自动控制技术用于战争和自动化生产带来的失业问题的担忧和苦闷。不知是不是因为这个因素，维纳又出了本书：《人有人的用处》。可以当做《控制论》的姊妹篇来阅读吧。\n要说的是，控制论发展了这么多年，不但没有老迈，反而迸发出无比的活力（比如机器学习）。\n","permalink":"https://pps43.github.io/posts/notes_on_cybernetics/","summary":"今天老师上课提到“Cybernetics”这个词，中文翻译为：控制论。控制论的开山鼻祖是Wiener（上篇文章 提到过他）。 我就好奇，Cybe","title":"Cybernetics二三事"},{"content":" 摸清背景，理清思路，探讨意义。\n首先谈谈我关于滤波的理解：\n在频域内对信号中某些频率分量进行衰减。关键词：低通、高通。典型的场景是将一段音乐中不同频段的声音分别抽取出来。 空间域内对不同尺度的细节进行处理。关键词：图像处理，滤镜。典型的场景是将一张被关在笼子里的老虎的照片“还原”出被笼子挡住的部分，有点透视的意味。 已经超出了两个字的字面意思，甚至也超出了频域、空域的范畴，进入了统计的领域。 扑面而来的是维纳滤波。先说说维纳其人。\n维纳（Norbert Wiener, 1894-1964）有个智识超常的父亲，不仅会40多种语言，而且据说数学功夫了得。小维纳在其教导下，以至于维纳初出茅庐时就已经“满身神装”。 后来，维纳研究的领域覆盖哲学、数学、物理学、工程学甚至生物学，是控制论（cybernetics）的开山鼻祖，也是通信界二号祖师爷（一号是香农）。并且功勋卓著，令人发指。其写了一本小册子《人有人的用处》，其中对信息化对人和社会的影响和变革的讨论，今天读起来依然富有前瞻性。是我最佩服的人类之一。\n维纳不仅是个奇才，也是个奇葩。\n摘一段轶事。维纳在教学上的恶劣表现让学生痛不欲生。有一次维纳在讲解一个定理时，想到了一个很直觉的证明方法，于是只在自己的大脑中推演，一下跳过了很多步骤，只写下一个简单的结果。这当然是为他的学生们无法承受的，于是有人很策略地请求他是否能够用另一种方法再证一遍，他说“当然可以”，马上又在脑中推演，又忘了在黑板上书写，经过几分钟的静默之后，只见他在原来的结果处打了一个查对无误的记号，就下课走了。。。\n回归正题。\n在信号接收时，由于信道的不确定性和其他种种干扰，接收方需要对接收信号对应的发送信号进行估计。为了评价估计的性能，我们定义了不少指标。这里采用误差平方函数的期望作为指标，指标的值越小越好。这就是LMS最小均方算法的出发点。 以下是用极不严谨的符号定义来说明问题。\n发送信号\\(S\\)，接收信号为\n$$X=S+N$$\n则估计值为\n$$\\hat S = {\\bf{T}}\\left( X \\right)$$\n\\({\\bf{T}}\\left( \\bullet \\right)\\) 表示一种变换，\\({\\bf{E}}\\left( \\bullet \\right)\\) 表示求期望。根据LMS算法的结果，对发送信号的估计值应为：\n$$\\hat S = {\\bf{E}}(S|X)$$\n这显然是难以获知的后验信息，所以这个结果仅仅有理论指导意义。\n如果多加一层约束，规定：\\({\\bf{T}}\\left( \\bullet \\right)\\) 为线性运算，则运用信号与系统的观点，可以将这个线性系统用冲激响应\\(h(t)\\)（离散形式为\\(h[n]\\)）表示。这里以离散形式为例，这个系统的物理实现是一个\\(N\\)阶抽头滤波器，滤波器系数可以写成一个列向量:\n$${\\bf{h}} = {[h(0),h(1),…,h(N - 1)]^T}$$\n使指标逼近最小值的方法有很多种，如steepest descent（最佳梯度法）逼近。也就是对\\(C\\)关于\\({\\bf{h}}\\)求偏导，再令偏导为0那一套方法。\n通过迭代逼近最小值这是个大的话题，在自适应滤波的书中讨论的比较细致（如稳定性、收敛性等），这里做一个粗略的分类：\n基于梯度的：牛顿法逼近、最佳梯度法; 随机梯度法：LMS算法等; 维纳滤波的数学推导点这里，或者统计信号处理的教材。\n离散形式的结论是：\n$${\\bf{h}} = R_X^{ - 1}{r_{SX}}$$\n这就是维纳滤波器的真身了。看看，结论是如此的简洁！先别兴奋，再对照一下Simon Haykin的《通信系统》一书第三章关于线性预测的讨论：在没有信道影响和噪声的情况下，同样是最小均方准则，结论是：\n$${\\bf{w}} = R_X^{ - 1}{r_X}$$\n绝了！\nUpdate:\n线性预测和维纳滤波器的推导都是采用最小均方准则的，数学上可以说明这类问题的最优解都是上述形式。所以这不是巧合。 补充解释下为什么大家钟爱于“均方准则”: 理论推导和实现要比诸如绝对值准则简单； 由二次方构成的性能曲面上，偏导为0的点一定是最小值点。 Update:\n最后摘一段维纳滤波的限制因素：\n要求接收信号是平稳的随机过程。 要求知道互相关系数矩阵。 计算出的系统可能因不满足因果性而无法实现。 后来的卡尔曼滤波在这一点上有所改进。\n慢慢发现，与其是滤波，不如说是线性预测，与其说预测，不如说是均衡。这几个概念我一直比较混乱，以后再进行统一和归纳吧。\nUpdate:\n滤波和预测的区别在于功能而不是算法或结构。前者是为了得到更满意的当前量，后者是为了得到满意的未来值。\n但有一点，滤波已经不再是狭义的“滤波”了。\n","permalink":"https://pps43.github.io/posts/thinking_of_filter/","summary":"摸清背景，理清思路，探讨意义。 首先谈谈我关于滤波的理解： 在频域内对信号中某些频率分量进行衰减。关键词：低通、高通。典型的场景是将一段音乐中不","title":"谈谈滤波"},{"content":"本篇的话题是——5G：OFDM没有遗言。OFDM作为4G多载波技术最大的功臣，在10多年来得到了疯狂的应用和追捧，在很多领域展现了其优越的性能和简约之美，可谓风光无限。研究者们痴迷它、学生们膜拜它、工程师们只要它。如今，我的主要工作却是质疑OFDM在5G场景中是否合适，并尝试寻找继承者，探索更适合5G场景的多载波传输解决方案。\n本篇的思路是：\n吐糟一下介绍OFDM的教材，尤其是国内教材。抛出几个小问题。 关于5G的研究内容 OFDM在5G场景中难以胜任的原因 未来 OFDM 先讲一讲OFDM的坎坷而光辉的历史。20世纪60年代OFDM的雏形诞生，实现复杂度很高，运用于军事。随后研究者们积极探索简化OFDM复杂度的方法但一筹莫展。FFT在1965年被提出后，Weinstein和Ebert第一次发现应该能够利用FFT实现OFDM，并设定通过保护间隔的思想对抗多径信道。但保护间隔的设定破坏了子载波间的正交性，极大损伤了OFDM的性能。直到1980年，循环前缀（CP）才被提出成为理想的保护间隔方案。再后来，基于FFT-CP的OFDM硬件系统被实现，并被广泛应用：\n1988 第一代实验型数字电视 1995 DAB系统 1999 WiFi 2002 WLAN 2005 LTE 历史介绍就到这里。\n关于OFDM技术本身，默认读者是基本了解的。完全了解OFDM的原理和实现，个人认为几个关键点是：多速率系统理解真实的时序、循环卷积与DFT、结合ISI与ICI的产生理解CP的作用。而这些内容常见的教材恰恰没有强调，只给出了OFDM调制解调的数学表达式、OFDM优点与缺点之类，松散而空洞。这样的直接结果是看完一遍好像懂了，别人只要问一个问题自己就又不懂了。即使是基础较好的学生，也要研读各家说法后才能有完整的、正确的理解。为了证实这一点，我抛出几个小问题供思考：\nOFDM子载波时域正交的条件是什么，怎样破坏之 一个OFDM符号时间包含多少个码字信息 假设有很长一段码流需要通过OFDM传送，超过了FFT长度。那么每做一次FFT的时间是多少，和OFDM符号时间有什么关系 在OFDM时频资源块上画出循环前缀（CP）的位置 采用循环前缀（CP）作为保护间隔，怎样去除了ICI和ISI，去除的是哪些资源块间的干扰？ 为什么LTE上行没有采用OFDM而是SC-CP？ 写出含有频率偏移（CFO）的CP-OFDM接收信号表达式 这些是OFDM最基本的概念引申出的问题，更不要说导频设计、峰均比等大量实际需要解决的问题了。如果问题都会，你大概是一个勇于思考善于思考的人（而且极有可能单身）。\n2020年，5G 首先点明5G的需求和物理层的挑战。为了偷懒就用我汇报时的PPT吧。参考5GNOW组织的几篇论文。\n5G的愿景很美好，那么如果利用现有的系统架构，即LTE-A/OFDM系统对应的性能又是如何呢？\n首先是实时性方面，LTE-A系统所能达到的时延的下限是1ms（由OFDM资源分配中的TTI参数决定），远远超过了realtime的要求。而且，就算改了协议废了现有器件，帧短了，但信道延时并没有短，这使得CP长度没有短，进一步使得OFDM符号的带宽效率大打折扣，简直不能忍。\n接着对于物联网中大量的M2M（机器对机器）交流，对于同步要求较高的OFDM也是一头雾水。如果为了性能而同步，将耗费大量时频资源。更何况现实是这是个RACH信道，而且有大量的结点。OFDM的高速高效传送需要的是整齐、同步——像军队。而RACH就像是一大群丐帮小混混，OFDM在这里玩不转。\n最后是频谱利用方面。由于OFDM的旁瓣巨大（比主瓣低13dB）导致功率侧漏，目前LTE-A很奢侈地用频域保护带来防止不同用户或系统互相干扰。\n补充（来自X.G.Xia教授和X.Q.Gao教授在CNV讲座上的讨论）：5G的带宽在100MHz以上，如果不用多载波直接均衡，需要的tap数在80个以上（小技巧：一般一个tap对应1.28MHz，所以LTE中30.72M的采样速率对应最大24个tap）远超过均衡器的能力；所以必须要多载波转化到较窄的频带，若用OFDM，FFT点数会达到2万多点，这也是不能接受的硬件复杂度（LTE-A中用2048）。\nOFDM在5G中难以胜任的原因，大抵如此了。\n未来 这方面的研究正紧锣密鼓的进行着。不过这不是一个全新的领域，因为FBMC家族的研究甚至比OFDM还要久远……但是FBMC单凭现在这个样子还是不足以打垮OFDM，五花八门的研究自此展开。 （Filtered-OFDM，Wavelet-OFDM，Vector-OFDM，UFMC，GFDM，。。。）\n我相信，用不了多久，OFDM就会成为萧何，LTE-A的成败皆由于此。我们通信人将会看着OFDM力战群雄后满身是血的倒下去，而我趁乱也捅了OFDM一刀。5G的时代里，OFDM没有遗言。\n先烈们安息吧。诸位国庆节快乐。\n","permalink":"https://pps43.github.io/posts/thinking_of_5g_ofdm/","summary":"本篇的话题是——5G：OFDM没有遗言。OFDM作为4G多载波技术最大的功臣，在10多年来得到了疯狂的应用和追捧，在很多领域展现了其优越的性","title":"5G时代：OFDM没有遗言"}]