[{"content":" PhysX Engine Series 这个系列主要记录一些最近在游戏中使用Nvdia PhysX 3.4物理引擎的一些经验。 本文主要介绍场景查询的一些内部机制和使用方法。\nPrerequisites 官方资料：\nPhysX 3.4 source code Download PhysXGuide.chm A Scene in PhysX engine is a container of objects in a hierachical manner.\n--- title: Scene Hierachy --- classDiagram direction LR class world class scene { Flags Gravity ... } class actor { ActorFlags Name GlobalPose ... } class shape { Flags GeometryType LocalPose ... } class geometry { halfExtents for box ... } class material { friction restitution damping } world \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;scene scene \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;actor actor \u0026#34;1\u0026#34;*.. \u0026#34;N\u0026#34;shape shape o--geometry shape o--material There are only position and rotation in GlobalPose and LocalPose, no \u0026ldquo;scale\u0026rdquo;. Scale only reflects on geometry\u0026rsquo;s actual size.\nPhysX Scene Query Three kinds of scene query:\nraycast sweep overlap In general, each SceneQuery traverses a culling structure containing the scene objects, performs a precise test using the GeometryQuery functions, and accumulates the results.\nYou can customize filtering logic via prefilter and postfilter.\nflowchart LR BP(Broad Phase) --\u0026gt; prefilter --\u0026gt; MP(Mid phase) --\u0026gt; NP(Narrow Phase) --\u0026gt; postfilter Broad phase traverses the global scene spatial partitioning structure to find the candidates for mid and narrow phases. midphase traverses the triangle mesh and heightfield internal culling structures, to find a smaller subset of the triangles in a mesh reported by the broad phase. Narrow phase performs exact intersection tests Pre-filtering happens before midphase and narrow phase and allows shapes to be efficiently discarded before the potentially expensive exact collision test. Post-filtering happens after the narrow phase test and can therefore use the results of the test (such as PxRaycastHit.position) to determine whether a hit should be discarded or not. More on traversal A scene uses two query structures, one for \u0026ldquo;static\u0026rdquo; objects (PxRigidStatic), one for \u0026ldquo;dynamic\u0026rdquo; objects (PxRigidBody). Each structure can use different culling algorithms, see PxPruningStructureType.\nPxPruningStructureType Explaination eNone Based on grid. Full rebuild when changed. eDYNAMIC_AABB_TREE Based on tree. Full rebuild when changed. Only choose this if all static actors in your scene are not modified after initialization. eSTATIC_AABB_TREE Based on grid and tree. Incremental rebuild when changed, unless by force. Choose this if frequently add/remove geometry, at the cost of higher memory More on prefilter and postfilter To make prefilter works, there are 3 steps.\nfirst attach data (PxFilterData) for on shape. It has four 32bit words to hold custom data, e.g., use word0 as layer of this shape. Here is an example:\nPxShape* shape = PxRigidActorExt::createExclusiveShape(*pxActor, PxBoxGeometry(extV), *pxMaterial); PxFilterData queryFilter; queryFilter.word0 = layer; shape-\u0026gt;setQueryFilterData(queryFilter); second define callback function for prefilter. See PxQueryFilterCallback. The logic is totally depend on yourself, just return PxQueryHitType to tell if this shape can pass. PxQueryHitType Explanation eNONE Shall not pass. eTOUCH Pass, but does not stop the raycast or sweep. eBLOCK Pass, but stop here. Here is an realworld example:\nclass PhysxQueryFilterCallback : public PxQueryFilterCallback { public: PhysxQueryFilterCallback(); PhysxQueryFilterCallback(bool isBlocking, bool includeTrigger); virtual PxQueryHitType::Enum preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags); virtual PxQueryHitType::Enum postFilter(const PxFilterData\u0026amp; filterData, const PxQueryHit\u0026amp; hit); private: PxQueryHitType::Enum m_HitType; bool m_IncludeTrigger; }; PxQueryHitType::Enum PhysxQueryFilterCallback::preFilter(const PxFilterData\u0026amp; filterData, const PxShape* shape, const PxRigidActor* actor, PxHitFlags\u0026amp; queryFlags) { bool isTrigger = shape-\u0026gt;getFlags() \u0026amp; physx::PxShapeFlag::eTRIGGER_SHAPE; if (isTrigger \u0026amp;\u0026amp; !m_IncludeTrigger) { return PxQueryHitType::eNONE; } PxFilterData shapefilterData = shape-\u0026gt;getQueryFilterData(); if (shapefilterData.word0 \u0026amp; filterData.word0 || shapefilterData.word1 \u0026amp; filterData.word1) { return m_HitType; } return PxQueryHitType::eNONE; } PxQueryHitType::Enum PhysxQueryFilterCallback::postFilter(const PxFilterData\u0026amp; filterData, const PxQueryHit\u0026amp; hit) { const PxLocationHit\u0026amp; impactHit = static_cast\u0026lt;const PxLocationHit\u0026amp;\u0026gt;(hit); if (impactHit.distance \u0026gt; 0.0f) return m_HitType; return PxQueryHitType::eNONE; } third step is to add PxQueryFilterData when query PxQueryFilterData has two fields:\nfield Explaination PxQueryFlags Supported flags are in PxQueryFlag::Enums, e.g. raise ePREFILTER means all shapes need to pass prefilter you defined. PxFilterData Has four 32bit words for you, e.g. use word0 as the \u0026ldquo;layermask\u0026rdquo; of the query. Here is an realworld example of raycast (return multiple objects):\nstruct MyRaycastHitResult { PhysXVec3 Position; PhysXVec3 Normal; float Distance; PhysXActor* Collider; }; typedef PhysXArray\u0026lt;MyRaycastHitResult\u0026gt; MyRaycastHitResults; bool MyRaycast(MyRaycastHitResults\u0026amp; hitResults, const PhysXVec3\u0026amp; startPos, const PhysXVec3\u0026amp; direction, float distance, unsigned int layerMask, bool includeTrigger) { const PxU32 bufferSize = 256; PxRaycastHit hitBuffer[bufferSize]; PxRaycastBuffer buf(hitBuffer, bufferSize); PxQueryFilterData filterData = PxQueryFilterData(); filterData.flags |= PxQueryFlag::ePREFILTER | PxQueryFlag::ePOSTFILTER; filterData.data.word0 = 0; filterData.data.word1 = layerMask; PhysxQueryFilterCallback filterCallback(false, includeTrigger); bool status = m_pxScene-\u0026gt;raycast(startPos, direction, distance, buf, PxHitFlag::eDEFAULT, filterData, \u0026amp;filterCallback, NULL); if (status) { hitResults.clear(); for (PxU32 i = 0; i \u0026lt; buf.nbTouches; i++) { PxRaycastHit touch = buf.touches[i]; MyRaycastHitResult hitResult; hitResult.Position = touch.position; hitResult.Normal = touch.normal; hitResult.Distance = touch.distance; hitResult.Collider = (PhysXActor*)touch.actor-\u0026gt;userData; hitResults.push_back(hitResult); } if (buf.hasBlock) { MyRaycastHitResult hitResult; hitResult.Position = buf.block.position; hitResult.Normal = buf.block.normal; hitResult.Distance = buf.block.distance; hitResult.Collider = (PhysXActor*)buf.block.actor-\u0026gt;userData; hitResults.push_back(hitResult); } } return status; } ","permalink":"https://pps43.github.io/posts/using_physx_scenequery/","summary":"PhysX Engine Series 这个系列主要记录一些最近在游戏中使用Nvdia PhysX 3.4物理引擎的一些经验。 本文主要介绍场景查询的一些内部机制和使用方法。 Prerequisites 官方资料： PhysX","title":"PhysX Engine Series: Scene Query"},{"content":"上一篇我们聊了AI模型的分类和大致架构，目前最火的两类模型LLM和Diffusion都基于Transformer架构训练而成，ChatGPT则是LLM家族中最出色的产品。本文就来聊聊最近出现的基于ChatGPT的创新架构和应用。由于ChatGPT的训练数据截止到2021年9月，且笔者并没有Plus会员才能使用的ChatGPT Plugin，所以这次请到的是Dr. Phind（The AI search engine for developers），大家欢迎。\n系列文章\n跟AI老师学AI（一） 跟AI老师学AI（二） 课堂问答 Auto-GPT Me:\nwhat is autogpt and how does it work\nDr. Phind:\nAuto-GPT is an experimental open-source project built on the latest ChatGPT model i.e GPT-4. It is a variant of ChatGPT developed by Significant Gravitas and it uses the ChatGPT API to work. Specifically, Auto-GPT utilizes the GPT-4 API, though it seems like it should work with the GPT-3.5 API as well. Auto-GPT is an AI that does the work for you by setting a goal and then generating and completing tasks. It does all the follow-up work for you, asking and answering its own prompts. It automates multi-step projects that would have required back-and-forth prompting if you worked directly with GPT-4. In other words, it \u0026ldquo;chains together LLM \u0026rsquo;thoughts\u0026rsquo;, to autonomously achieve whatever goal you set.\u0026rdquo;\nThere are new apps that have emerged to make Auto-GPT even easier to use, like AgentGPT and GodMode, which provide a simple interface where users can input what they want to accomplish directly on a browser page.\nMe:\nhow does AutoGPT work exactly? Find the techniques behind.\nDr. Phind:\nAuto-GPT is a Python application built on top of GPT-4 that allows the AI to function independently without requiring prompts from the user for each action.\nAuto-GPT breaks down the AI’s actions into “thoughts,” “reasoning,” and “criticism,” which enables users to understand precisely what the AI is doing and why it’s doing it.\nAuto-GPT uses long and short-term memory management, which is similar to how humans learn from their mistakes. Auto-GPT has the ability to assess its work, improve upon past experiences, and leverage its history to generate more precise results. Auto-GPT\u0026rsquo;s integration with vector databases, a memory storage solution, allows it to retain context and make informed decisions.\nMe:\nlist some autonomous agent besides autogpt\nDr. Phind:\nhere are several other autonomous AI agents besides Auto-GPT, some of which are Camel, BabyAGI, and Westworld simulation.\nCamel is an autonomous AI agent that uses LangChain to store and retrieve information. It can learn and understand human language and is capable of making decisions based on the context. Camel can generate content for social media and can perform simple tasks like booking a restaurant reservation or scheduling a meeting (Reference).\nWestworld simulation is an autonomous AI agent that simulates the Westworld environment. It uses reinforcement learning to improve its decision-making ability and can learn from its mistakes. Westworld simulation can interact with other agents in the environment and can perform tasks like navigating through the environment, recognizing objects, and even interacting with human.\n⚡Me: In fact it\u0026rsquo;s an interactive sandbox environment created by researchers from Stanford and Google, the paper is here (PDF) and this is the basic architecture. BabyAGI is another autonomous AI agent that uses GPT-4, Pinecone, LangChain, and Chrome to generate and execute tasks. It can make human-like cognitive decisions and can write and run codes to meet certain objectives. BabyAGI can retain long-term memory of tasks and events with the help of Pinecone and LangChain, making fetching information faster. Its decisiveness makes it an effective tool for cryptocurrency trading, robotics, and autonomous driving.\n⚡Me: add info from Github: the script works by running an infinite loop that does the following steps: These autonomous AI agents have different capabilities and approaches to accomplishing tasks. Camel is useful for generating content for social media and performing simple tasks, Westworld simulation is useful for simulating environments and interacting with other agents and BabyAGI is useful for decision-making sectors like autonomous driving and robotics.\nBabyAGI Me:\nwhat is babyagi and compare with autogpt?\nDr. Phind:\nBabyAGI and Auto-GPT are autonomous AI agents designed to generate and perform tasks based on a given objective. The primary difference between the two is the way they approach the objective, the techniques they use, and their purpose.\nBabyAGI uses a combination of GPT-4, Pinecone, LangChain, and Chrome to create and execute tasks. It creates a task list based on the objective entered and executes them one by one. The results of one task determine the next one. It can retain a long-term memory of tasks and events with the help of Pinecone and LangChain, which makes fetching information faster. Since it uses a trial-and-error basis to decode the results of previous tasks, BabyAGI can make complex decisions without losing track of the predefined objective.\nAuto-GPT, on the other hand, generates and runs multiple tasks at once using GPT-4 and creates an artificial memory space using GPT-3.5 to store results from previous tasks. It has access to internet services like apps, websites, books, documents, and articles and can use them to complete tasks required to reach the objective. Auto-GPT is good at generating text-rich content and images. It can also provide human-like text responses, making it useful for generating content, summarizing texts, and translating text into more than a dozen languages\nVector Database Me:\nwhat is vector database and how does it integrate into above system?\nDr. Phind:\nVector database is a type of database that is designed to handle high-dimensional data such as embeddings, which are numerical representations of objects or concepts in a high-dimensional space. In the context of Auto-GPT, vector databases are used to store the embeddings of documents that are generated by OpenAI\u0026rsquo;s language model. The vector database is used to enable similarity search over these embeddings, which is used by Auto-GPT to find the most relevant documents to complete a task.\n(Me: Below is just one example to integrate pgvector to auto-gpt, there are other choice, e.g., weaviate)\nThe vector database is integrated into the Postgres database using pgvector, an open-source vector similarity search extension for Postgres. In more detail, open-source vector similarity search extension pgvector allows for storing and querying vector embeddings in a Postgres database. OpenAI\u0026rsquo;s text-embedding-ada-002 model outputs 1536 dimensions, so that is the size of the vector used in Auto-GPT. The pgvector extension introduces a new data type called vector, which is used to create a column named embedding with the vector data type in the Postgres database. The similarity search over these embeddings is performed using a function named match_documents, which takes a query embedding, a match threshold, and a match count as inputs and returns a table of document IDs, content, and similarity. The similarity is calculated using vector math operations like cosine distance. (Reference)\nMe:\nlist some popular vector database product or repo\n(Dr.Phind\u0026hellip;. Google and Google Bard comes to help):\nSure, here are some popular vector database products or repos:\nFAISS (Facebook AI Similarity Search) is a library for efficient similarity search of dense vectors. It is open source and available on GitHub. Milvus: Milvus is a vector database designed for high-performance similarity search and machine learning.Milvus 2.0 is a cloud-native vector database with storage and computation separated by design. All components in this refactored version of Milvus are stateless to enhance elasticity and flexibility. Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data. weaviate These are just a few of the many vector database products and repos that are available. The best product or repo for you will depend on your specific needs and requirements.\nLangChain Me:\nwhat is langchain and how does it work?\nDr. Phind:\nLangChain is a Python package that provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications [3]. It is designed to assist in four main areas, which are listed here in increasing order of complexity: prompts, long-term memory (LLMs), tools, chains, agents, and memory [2].\nLangChain is a framework for developing applications powered by language models. It is created to be data-aware and agentic, meaning that it connects a language model to other sources of data and allows a language model to interact with its environment.\n(\u0026hellip;) LangChain can be used in a variety of ways, including autonomous agents, personal assistants, question answering, chatbots, querying tabular data, code understanding, interacting with APIs, summarization, and evaluation.\nMe:\ndoes AutoGPT use Langchain?\nDr. Phind:\nThere is no mention of AutoGPT using LangChain in the provided context. While LangChain is a framework that makes it easier to build scalable AI/LLM apps and chatbots, there is no clear indication that AutoGPT specifically uses LangChain.\n课后笔记 Autonomous agents (e.g., AutoGPT, BabyAGI) can auto-create tasks towards the GOAL you give them, and auto-find the way to complete task based on GPT models and other data sources (websites, your documents, etc), and even auto-improve themselves from previous mistakes. The basic structure is somehow similar to GOAP in video game\u0026rsquo;s AI design. They can achieve these because they can have long-term and short-term memory powered by Vector Database. ( memory size is much more than single GPT4\u0026rsquo;s context which is 32K tokens, ~24K words.) Vector Database is a specialized db to handle vectors (read, write, calculate similarities, etc). they are useful for text because text can be transformed into numberic vectors by embedding technique. Similarity between embeddings (vectors) reflect relevance between text. Hence above agents can retrieve relevant data from large datasource and sending to GPT as context. Frameworks like LangChain makes it easier to implement autonomous agents because it provides a standard interface to handle different tools and data sources. 拓展阅读\nTutorial: ChatGPT Over Your Data (using LangChain) Create your own chat bot with GPT-Index \u0026amp; LangChain ","permalink":"https://pps43.github.io/posts/asking_ai_about_ai_2/","summary":"上一篇我们聊了AI模型的分类和大致架构，目前最火的两类模型LLM和Diffusion都基于Transformer架构训练而成，ChatGPT","title":"跟AI老师学AI（二）"},{"content":"按照目前AI在学术界、工业界推陈出新的速度，感兴趣的同学们纷纷表示：真的学不动啦！不要慌，AI老师来了~ 在本系列中，我不会提前学习预备知识，以门外汉的视角去追问AI并了解一些基本概念。既可以锻炼提问技巧，还可以在听完高屋建瓴的讲解后有针对性的去学习感兴趣的部分，效率倍增！\n系列文章\n跟AI老师学AI（一） 跟AI老师学AI（二） 课堂问答 Generative Models, Transformers, Large Language Models (LLM) ? Me:\nexplain generative models and list some types of it. And explain the relationship and differences with LLM.\nChatGPT:\nGenerative models are a type of machine learning model that learns to generate new data that is similar to the training data it was trained on. Unlike discriminative models, which learn to classify input data into specific categories, generative models focus on learning the underlying patterns and structure of the input data, allowing them to generate new data that is similar to the input data.\nThere are several types of generative models, including:\nVariational Autoencoders (VAEs) Generative Adversarial Networks (GANs) Autoregressive models, such as PixelCNN and PixelRNN Boltzmann Machines Restricted Boltzmann Machines (RBMs) Deep Belief Networks (DBNs) Language Models (LMs) are a type of generative model that specifically focuses on generating natural language. Like other generative models, LMs are trained on a large corpus of text data and learn to generate new text that is similar to the input data. However, LMs differ from other types of generative models in that they specifically focus on generating text and incorporate language-specific features, such as grammar, syntax, and vocabulary.\nOne key difference between LMs and other types of generative models is that LMs are typically based on probabilistic models, such as n-gram models or neural network models, whereas other types of generative models, such as GANs and VAEs, are typically based on deep learning architectures. Additionally, LMs may incorporate language-specific constraints, such as word order and grammar, that are not present in other types of generative models.\nMe:\nDo you mean generative models are unsupervised learning, while discriminative models are supervised learning?\nChatGPT:\nGenerative models can be trained using both supervised and unsupervised learning approaches, depending on the specific task and the type of data being used. In unsupervised learning, the model is trained on a dataset without explicit labels, and the goal is to learn the underlying structure and patterns in the data. In supervised learning, the model is trained using labeled data, and the goal is to learn to predict the label of new, unseen data.\nHowever, it is true that discriminative models are typically used for supervised learning tasks, such as classification, regression, and sequence labeling. These models learn to map input data to specific output labels or categories. In contrast, generative models are more commonly used for unsupervised learning tasks, such as density estimation, data synthesis, and anomaly detection. These models learn to generate new data that is similar to the training data, without necessarily mapping input data to specific output labels or categories.\nMe:\nGot it. Now explain what transformers is about in deep learning, and the relationship between generative models and the transformers.\nChatGPT:\nTransformers are a type of deep learning model that has become increasingly popular in natural language processing (NLP) tasks, such as language modeling, machine translation, and sentiment analysis. The Transformer architecture was introduced in a 2017 paper titled \u0026ldquo;Attention Is All You Need\u0026rdquo; by Vaswani et al., and it has since become the foundation for many state-of-the-art NLP models.\nThe key innovation of the Transformer architecture is the use of self-attention mechanisms to replace traditional recurrent or convolutional neural networks. Self-attention allows the model to weigh the importance of different parts of the input sequence when generating each output token. This enables the model to capture long-range dependencies and context more effectively, resulting in better performance on many NLP tasks.\nGenerative models, such as language models, can be built using the Transformer architecture. In fact, many of the most successful language models, such as OpenAI\u0026rsquo;s GPT series and Google\u0026rsquo;s BERT, are based on the Transformer architecture. These models are trained on large amounts of text data using unsupervised learning techniques, such as autoregressive language modeling or masked language modeling. Once trained, the models can be used to generate new text that is similar to the training data, or to perform downstream NLP tasks, such as sentiment analysis or machine translation.\nIn summary, the Transformer architecture is a powerful tool for building deep learning models for NLP tasks, including generative models such as language models. By leveraging self-attention mechanisms, Transformers are able to capture long-range dependencies and context more effectively, resulting in better performance on many NLP tasks.\nModels, Architectures? Me:\nDo you mean one model can be implemented in different architectures, and transformers are just one of the architectures to build models? If this is true, list some other architectures popular.\nChatGPT:\nYes, that\u0026rsquo;s correct. A single machine learning or deep learning model can be implemented using different architectures, depending on the specific task and the type of data being used. The choice of architecture can have a significant impact on the performance and efficiency of the model.\nIn addition to the Transformer architecture, there are several other popular deep learning architectures for building models in various domains. Here are a few examples:\nConvolutional Neural Networks (CNNs): These are commonly used in computer vision tasks, such as image classification, object detection, and segmentation. Recurrent Neural Networks (RNNs): These are often used for sequence modeling tasks, such as natural language processing, speech recognition, and time-series prediction. Long Short-Term Memory Networks (LSTMs): These are a type of RNN that are designed to handle long-term dependencies in sequences. Autoencoders: These are used for unsupervised learning tasks, such as feature extraction, data compression, and anomaly detection. Generative Adversarial Networks (GANs): These are used for generative modeling tasks, such as image synthesis, data augmentation, and video prediction. Variational Autoencoders (VAEs): These are also used for generative modeling tasks, such as image synthesis and data generation, but they incorporate probabilistic models and encode latent variables. These are just a few examples of the many deep learning architectures that are available for building models. The choice of architecture will depend on the specific task and the characteristics of the data being used.\nDiffusion, Stable Diffusion, latent space? Me:\nExplain stable diffusion to me. Is it a model, architecture, or others?\nChatGPT:\nStable Diffusion is a probabilistic model for generative modeling and data synthesis, and it is related to the concept of Generative Diffusion Models (GDMs).\nAt a high level, the Stable Diffusion model uses an iterative diffusion process to model the distribution of the data. The diffusion process involves iteratively adding noise to the input data and then progressively filtering out the noise using a series of conditional distributions. This process results in a sequence of probability distributions, each of which approximates the true data distribution more closely than the previous one.\nUnlike traditional diffusion models, which can suffer from instability due to the accumulation of noise over time, the Stable Diffusion model incorporates a stability regularization term to prevent this from happening. This regularization term encourages the diffusion process to converge to a stable distribution, resulting in more accurate and reliable generative models.\nThe Stable Diffusion model is closely related to Generative Diffusion Models (GDMs), which also use a diffusion process to model the data distribution. However, GDMs suffer from instability due to the accumulation of noise over time, and they require careful tuning of the learning rate to avoid this. The Stable Diffusion model addresses this issue by incorporating a stability regularization term, which improves the stability and reliability of the model.\nIn summary, Stable Diffusion is a probabilistic model for generative modeling and data synthesis that uses an iterative diffusion process and a stability regularization term to improve the stability and reliability of the model. It is closely related to Generative Diffusion Models (GDMs) but incorporates additional features to address the instability issues that can arise in traditional diffusion models.\nMe:\nDo you know stable diffusion 2? It\u0026rsquo;s said to be a latent diffusion model, what does it mean?\nChatGPT:\nYes, Stable Diffusion 2 is a follow-up work to the original Stable Diffusion model and is a latent diffusion model.\nA latent diffusion model is a type of generative model that learns a low-dimensional representation (or latent space) of the data that captures the underlying structure of the data. In other words, instead of modeling the data directly, the model learns to generate the data by sampling from a distribution in the latent space and then mapping the samples back to the data space.\nStable Diffusion 2 extends the original Stable Diffusion model by introducing a latent diffusion process. The model learns a diffusion process in the latent space, which allows it to generate data that has a more complex and structured distribution than the original model. The latent diffusion process involves iteratively adding noise to the latent space and then filtering out the noise using a series of conditional distributions. This process results in a sequence of probability distributions in the latent space, each of which approximates the true data distribution more closely than the previous one.\nOnce the model has learned the diffusion process in the latent space, it can generate new data by sampling from the final distribution in the latent space and then mapping the samples back to the data space. By using a latent diffusion process, Stable Diffusion 2 is able to generate data that has a more complex and structured distribution than the original model, making it suitable for more challenging generative modeling tasks.\nIn summary, Stable Diffusion 2 is a latent diffusion model that learns a diffusion process in the latent space to generate complex and structured data distributions. It is a more advanced version of the original Stable Diffusion model and is suitable for more challenging generative modeling tasks.\nMe:\nexplain latent space like I\u0026rsquo;m 5\nChatGPT: 👍\nImagine you have a big box full of toys. Each toy has different colors, shapes, and sizes. Now, imagine you have a secret map that tells you how to mix and match the toys to create new toys that no one has ever seen before. This secret map is like a code, and it\u0026rsquo;s hidden inside the box.\nThe box of toys is like the data, and the secret map is like the latent space. The latent space is a hidden space that tells you how to mix and match the different parts of the data to create something new. It\u0026rsquo;s like a secret code that only the computer knows.\nWhen you train a model to learn the latent space, it\u0026rsquo;s like teaching the computer to find the secret map inside the box of toys. Once the computer knows the secret map, it can create new toys that no one has ever seen before by mixing and matching the parts of the toys in new and interesting ways.\nSummarize the talk Me:\nSummarize our conversation above to give a thorough and concise relationship among all the models, architectures, and other important concepts we talked about.\nChatGPT:\nIn summary, we discussed several important concepts in deep learning, including generative models, discriminative models, language models, and architectures such as Transformers and Generative Adversarial Networks (GANs).\nGenerative models are used for unsupervised learning and aim to model the data distribution to generate new data. Discriminative models, on the other hand, are used for supervised learning and aim to learn the decision boundary between different classes.\nLanguage models are a type of generative model that learns to predict the probability of a sequence of words, given some context. Transformers are a popular architecture used for language modeling, which use a self-attention mechanism to capture long-range dependencies in the input.\nGenerative Adversarial Networks (GANs) are a type of generative model that uses two neural networks, a generator and a discriminator, to generate new data that is similar to the training data. The generator tries to create new data that can fool the discriminator, while the discriminator tries to distinguish between the generated data and the real data.\nFinally, we discussed Stable Diffusion and Stable Diffusion 2, which are probabilistic generative models that use a diffusion process to model the data distribution. Stable Diffusion 2 is a latent diffusion model that learns a diffusion process in the latent space to generate complex and structured data distributions. The latent space is a hidden space that tells the model how to mix and match the different parts of the data to create something new.\n课后笔记 We train a model by choosing an architecture. Some models are named after the architecture they use, e.g., GAN, VAE. We fine-tune existing model as a foundation to generate a new model, with less training effort. LLM and Diffusion are both generative, probabilistic models. ChatGPT is a product powered by GPT model (belongs to LLM family). Stable Diffusion is a Diffusion model, and its sevond version learns pattern in a hidden box called latent space. Above models all use Transformers as the architecture. Transformers are a type of attention-based neural network. mindmap root((Deep Learning)) models discriminative model K nearest Logistic regression SVM Random Forest ... generative model deterministic CNN ... probabilistic VAE GAN LLM Diffusion architectures CNN LSTM GAN VAE ... Transformer BERT GPT ... ","permalink":"https://pps43.github.io/posts/asking_ai_about_ai/","summary":"按照目前AI在学术界、工业界推陈出新的速度，感兴趣的同学们纷纷表示：真的学不动啦！不要慌，AI老师来了~ 在本系列中，我不会提前学习预备知识，","title":"跟AI老师学AI（一）"},{"content":" \u0026ldquo;There are decades where nothing happens; and there are weeks where decades happen.\u0026rdquo; ― Vladimir Lenin\n基于大语言模型的AI在这个月带给人们的感受，用列宁的这句话概括再贴切不过了。作为普通人，去拥抱这些AI工具，就像会使用智能手机和搜索引擎；对人类来说，就像学会用电，学会用火。\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;dark\u0026#39; } }%% timeline title Era of AI comes in 2023 2-7: Microsoft New Bing 3-12: Open AI ChatGPT 90% cheaper 3-15 : Open AI GPT-4 3-16 : Microsoft Copilot : Midjourney V5 : Google PaLM API 3-21 : Adobe FireFly : Nvdia GTC 3-22 : Github Copilot X : Google Bard 3-24 : Open AI ChatGPT Plugins 更新：4月以来，AI应用的新概念、新架构、新产品如寒武纪大爆炸一般涌现（AutoGPT 首当其冲），非人力所能穷举。 这个网站收录了大量AI工具，本文也会持续更新笔者常用、觉得好用的工具。\nFor General Purpose Open AI\u0026rsquo;s ChatGPT, and Plugins. GPT3.5 is free to use. Microsoft\u0026rsquo;s NewBing. It\u0026rsquo;s said to be powered by GPT4 (internal version). Google\u0026rsquo;s Bard. After trying many LLM, ChatGPT is still the best one to be professional and smart. But I still prefer asking different models to get different points of view. Some common tips when asking:\nRole play. act as .... Here is a collection of role-related prompts. Give template input-output. Tell chatgpt to anwser step by step. For Doc Edge + NewBing. Explain any webpage (including PDF) side by side. ChatDoc/ChatPDF, upload PDF and analyze. ⏳Microsoft\u0026rsquo;s Copilot. For Software Development phind, the AI search engine for developers. Cursor editor, or vscode plugin CodeCursor, read/write current document/code, FREE to try. Old Github Copilot (based on OpenAI\u0026rsquo;s CODEX), costs $10/mo after 60d trial. ⏳Github Copilot X ⏳Copilot for Docs, used to learn a SDK/framework/API, can based on private content. The gist to generate code is, to describe a single-responsibility function to let AI generate, rather than a function with long description of chained operations.\nFor 3D/2D Art Stable-Diffusion (SD) web-ui, totally free and opensource, run model locally on PC.\nDownload/Share models on civitai/Hugging face Use ControlNet (Github )to add more controll on specific SD model. Use LoRA (Low-rank adaption) to train faster with less memory. Use Text Inversion to train with amazingly small output. Use [DreamBooth] to train if you need to be really expressive. Midjourney, famous for its artistic style, 25 times FREE try.\nAdobe\u0026rsquo;s Firefly\nOpen AI\u0026rsquo;s DALL-E-2, generates image with natural language and long prompts, but limited-access and less control.\nBing\u0026rsquo;s Image Creator, generate image with natural language, and free to try.\nFor Music Mubert Soundraw.io Want more power? If you want to:\ntrain your own AI based on these models know the strength and weakness of current AI models know why \u0026amp; how Generative AI works, mathematically Here are my personal ideas:\nFor text, play with LLaMA/llama.cpp, or its fined tuned version Alpaca/Alpaca-LoRA. For image, play with Stable-Diffusion and its plugins. They can run on PC/Mac. Weakness of current LLM models: math; chain of decision. But they are improving. \u0026ldquo;Dive into Deep Learning\u0026rdquo; by 李沐。中文版《动手学深度学习》 Hardware considerations Training on cloud is cheaper and least effort to start. (Google\u0026rsquo;s Colab is even FREE) Training on local hardware, if use multiple GPUs (with NVLink), traffic bandwidth between GPUs is the botthleneck. (DGX A100 specs: 8xA100 GPUs, total 640GB VRAM, 600GB/s GPU-to-GPU bandwidth.) ","permalink":"https://pps43.github.io/posts/ai_toolkit_for_dev/","summary":"\u0026ldquo;There are decades where nothing happens; and there are weeks where decades happen.\u0026rdquo; ― Vladimir Lenin 基于大语言模型的AI在这个月带给人们的感受，用列宁的这句话概括再贴切不过了。作为普通人，去拥抱这些AI工具，","title":"Generative AI Toolkit (5.12)"},{"content":"市面上以DevOps为主题的书，以及和项目攻关的影视作品有不少了，但这本小说读起来依然紧张刺激。除了揭示管理现代IT组织与管理传统工厂的共通之处，书里更强调了以一种全局的视角来重新看待自己的工作环境，和自己在工作中扮演的角色，并且，其包含的问题分析方法、实践方法也或多或少可用于个人效能的优化。\n下面是一些书摘和简评。\n面对压力和现实 比尔，我知道你没有申请这个职位，但公司已命悬一线，我需要你来帮助我拯救这家伟大的公司。我能指望你吗？还没来得及再次礼貌谢绝，我突然听到自己说“可以，你可以指望我。”我慌了，强迫自己住嘴，以免做出更多愚蠢的承诺。……“我会尽力的，还有，能不能请你至少解释一下，为什么在这个位子上的人都干不长？你最希望我做什么，最不希望我做什么？”\n临危受命时，一定要弄清楚领导的意图和前任失败的原因。\n我还来不及回应，莎拉就大声说：这恰恰表明，比尔及其团队缺乏对于紧迫性的必要认知。追求完美是成事的大敌，比尔，我们可没有闲工夫为了迎合你的黄金标准而精雕细琢……难道这不是有点太轻率、太不公平了吗？但莎拉不屑一顾的说：我相信决定已经做出了。……接下来的9天里，我们所有人都要熬夜加班了。这种全员出动的工作状态是IT人生活的一部分，但是想到我们又得因为其他人疏于计划 而不得不奋力拼搏，我还是有些恼火。\n在市场负责人莎拉拿出一切为了项目这个尚方宝剑时，由于没有实际数据支撑，只因为道理而不去做是不够的，所以只能先忍下来。反倒是不顾实际情况强行推进项目发布的莎拉，得到了CEO史蒂夫的支持。因为CEO更关心股价、以及董事会对他的看法！\n我设法复述那些缜密理性、条理分明的论点，他们是我花了整个周末的事件排练的……我一边说一边不断观察史蒂夫，到目前为止，他一直面无表情。……史蒂夫愤怒的回答：什么优先级高不高的狗屁问题？……我在心里默数三下才开口：当然，我表达的不够清楚，……我们的基础架构太过脆弱……好吧，我们会尽最大努力，但我要郑重声明，我们的人手严重不足，无法高质量完成其中任何一项工作，更别说全部了。……比尔，凤凰项目已经超支1000万美元，我们必须马上得到正向现金流。增加任何预算都是不可能的，如果会有什么调整，我们可能还得在你的部门减掉几个人。……离开的时候，我把花费整个周末时间准备的演示稿扔进了垃圾桶。\nCEO有他的计算方式和危机视角，所以主角上来摆事实说道理不会得到支持。\n公司高管强迫我的工程师优先执行他们的命令，这完全是胡闹。我（对布伦特）补充道：如果有人为了凤凰项目之外的事和你联系，就把他们推给韦斯（直接上级），让他去对付那帮笨蛋。无论如何都要想办法改正大家直接来找你帮忙的坏习惯。我允许你把电话设成静音，把状态设置成你没空。随便怎么样都行。\n布伦特是一名对系统了解最多的明星员工，所有重要的系统变更都需要他帮忙，但他就像一个黑盒，是系统的约束点。首要任务是发现并保护系统的约束点。\n我惊讶的目瞪口呆：史蒂夫，情况得有多糟糕你才肯推迟这次发布？我告诉你这次是运行将是非常鲁莽的冒险！……虽然很不情愿，但我还是觉得自己欠公司最后一搏，去阻止这次疯狂的行动。……（强行发布后）……史蒂夫停下脚步，用手指着我的前额说：我对责任的理解，比你一辈子学的还要多！我受够了你整天嚷嚷着天要塌了，时候在高高兴兴地说‘我早就告诉过你了’。你得带着实际的解决方案来找我。……我需要业务部门告诉我，他们不再受部门IT部门的钳制。我担任CEO依赖，一直都听到这样的投诉，IT拖累了每一项重要举措。\n原来是这种发酵很久的情绪带来了偏见，导致史蒂夫讨厌比尔的稳扎稳打，偏爱莎拉的冒进。\n（史蒂夫讲述自己的个人经历和弱点，并鼓励会上的每一个人都这么做）展现自己脆弱的一面有助于建立起信任的基础。\n约翰气急败坏的说：你以为你是谁？我在努力保持这家公司的安全，让那些审计师离得远远的！我……得了吧，只会帮倒忙的CISO先生。埃瑞克打断他说，正如你刚才看到的，不用你出手，这家公司就能让审计师离得远远的。你就像个管道工，不知道自己在为一架飞机服务，更别说了解飞行路线，或者航空公司的营业状况了。……你真的不明白，是不是？无极限零部件公司最大的风险是停业破产，而你似乎一心想用你那些不周全的考虑和无关紧要的技术细节，让他加速倒闭。怪不得你会被边缘化！……\n约翰身上有几个问题：\n没有理解总目标。数据安全是重要，但公司和业务都要黄了，保证数据安全只是空谈而已。 没有理解业务全流程。事后他才知道即使前序的IT系统中有各种安全隐患，但后续每一笔交易都有人工审核，所以审计师并不会来找麻烦。 没有真正理解自己的职责和能力如何助力于总体目标。所以他倡导的数据安全在其他人看来只有妨碍作用。他在该阶段真正的作用在于用经验和能力减少其他部门在安全上的无效努力。 克里斯首先回答：我之前就说过，就连次要的漏洞修复都问题重重，我们不能承受每月一次的发布……史蒂夫回答：成败就看这个季度。我们向世人许诺过，会在上个月把凤凰弄出来。……我们没有时间了。我对克里斯说，如果你说凤凰团队应该放缓速度，我不会有异议。但是，我们仍然要想办法满足史蒂夫的要求，如果我们不能再凤凰的框架内做成这件事，也许可以在凤凰之外做到。我提议可以从凤凰主团队分出一小队人，组建特别行动队。\n老项目老流程积重难返，这里提供了一种减少掣肘的思路。后文能看到，特别行动队奏效了，而且其优秀的工作流还被原凤凰团队逐步借鉴，形成反哺。\n建立认知 半成品是个隐形杀手。\n因此，管理任何一家工厂最关键的机制之一，就是工作任务和原材料的发布。没有这个机制就无法控制半成品。为了停止半成品在工作流中堆积，即使其他人闲得无聊，也不应该在瓶颈已经达到饱和的情况下，继续制造半成品。\n在瓶颈之外的任何地方做出的改进都是假象。\n作为IT运维部的副总裁，你的工作时确保形成一条迅速、可预测、持续不断的计划内工作流，从而向业务部门交付工作价值，同时尽可能降低计划外工作的影响和破坏。我不管每个人觉得自己的项目有多重要，我们要知道的是，该项目能否提高我们在约束点上的工作能力。\n实际上有四种类型的工作：来自业务方的、自身基础架构的、操作变更、计划外工作。\n计划外工作是恢复性工作（比如屎山代码出BUG后的救火），占用你的时间并阻碍完成前三种正向产出的工作。如果不加控制，技术债务将导致公司里能够唯一完成的工作就是计划外工作。\n流程是用来保护人的。\n根据韦斯讲的故事，我们甚至都不该让布伦特碰到键盘，他可以告诉大家应该输入什么，但在任何情况下，都不准他做哪些我们无法在时候记录的事……每解决一个问题，我们的知识库里就会多一篇关于如何解决某个疑难杂症的文章，而且能够实施修复的人会越来越多。\n这里提到的解决疑难杂症的文章，类似我在微软工作时组内特别重视的postmortem，也类似达利欧在桥水特别推行的错误日志。目的是同样的错误绝不犯两次，而且下次其他人也可以处理。\n一旦通清楚最常出现的任务是什么，就需要建立起工作中心和工作路径。\n你必须跳出原来的专有领域，才能弄清楚整体的成功需要你的哪些工作来达成。\nIT是一种技能，就像能读会算一样。理解技术能够做什么，不能做什么，已经成为一家公司每个部门必须具备的核心竞争力之一。\n创建一个让人感觉无能为力的系统，使我们能对人类同胞做的最具破坏性的一件事——我们剥夺了他人控制自己成果的能力，甚至营造了一种文化，让人们因为害怕遭受惩罚、失败而不敢做正确的事。这制造了“习得性无助”的环境，人们变得不愿或无法采取行动来避免未来遇到同样的问题。\n三步建立高效的工作流 目标：流量最大化；可灵活应对调整。\n第一工作法 明确价值链。从最终的绩效指标反推，其实现依赖什么，以及不满足时有什么风险。 最大优化正向工作流。 让等待时间可视化。在工厂的流水线上在哪里出现拥堵或空转显而易见，但IT或日常生活中很容易忽略（又叫暗时间）。Kanban是一种比较好的方式，注意只有在生产环境里成功运行起来、或开始产生价值的时候，才能算“完成”。 减少中断。生产中断在制造业里显眼且代价极高，所有的半成品都将报废。但技术工作者很容易被打断，因为后果不可见。通过严格限制多任务的数量、新任务的插入减少打断的次数。 杜绝缺陷向下游传递。发现难题，群策群力，建立机制而不是以后再说。 减小交付的内容大小和等待间隔。 持续识别、改善约束点。约束点可以是一个人，或者大家都倚赖的同一种资源、前置步骤。以DevOps为例，通常要依次优化以下约束点：1、环境搭建；2、代码部署；3、准备和执行测试；4、架构耦合。 消除浪费。例如：1、半成品（文档、变更单等）；2、多余工序（对后续流程无价值）；3、任务切换；4、资源竞争和等待；5、非标准或手动操作。 第二工作法 快速、持续的获得反馈。否则没人敢在一个复杂系统中放心的工作（不用担心自己的某一个操作在将来搞坏系统中另一个本不相关的功能）。 仍以微软为例，Substrate仓库里有多个M365相关的产品，约5000+工程师维护着上百GB的源码文件，即使穷尽一生也没有人能充分了解这样一个已经存活几十年的庞然巨物。然而这样一个系统的迭代速度却非常惊人，保障这个工作流的其中一个方式是：\n每一次代码提交都会触发多种自动化测试和模拟部署，在几分钟到几十分钟内告诉你是否可以将代码合入主干。 同时会自动@所有相关模块的负责人在DevOps系统中进行Review，直到所有利益相关者同意合入。 第三工作法 建立具有创意和高可信度的文化。它强调每个人都是持续学习者，必须勇于承担风险，通过科学的方式改进。彼此分享经验，经验积累沉淀。\n将日常工作的改进制度化 将局部的优秀经验全局化 领导层为团队创造学习条件，领导力并不体现在做出的所有决定都是对的。 ","permalink":"https://pps43.github.io/posts/notes_on_phoenix_project/","summary":"市面上以DevOps为主题的书，以及和项目攻关的影视作品有不少了，但这本小说读起来依然紧张刺激。除了揭示管理现代IT组织与管理传统工厂的共通","title":"凤凰项目"},{"content":" 近日读完《贫穷的质感·王梆的英国观察》（豆瓣读书，Goodreads）一书后，将只言片语组合起来作一点记录。此书说了很多英国普通人的事情、社会传统，但相信我们读起来应该不会太陌生。透过诸多生活中的观察，自然引申出养老、医疗、社会制度、逆全球化等话题。\n只言片语 英国政府曾推出大量廉租房，老旧了便拿来卖钱。由于是市面价格的一半，非常受炒房客的欢迎。有钱人买来继续出租，租金比原来还要高。但由于政府不再负责修缮，这些房子逐渐变得又贵又脏。\n斯蒂芬·阿姆斯特朗指出：穷人不是贫困的罪魁祸首，而是他的产物。贫困也不可能通过自律自愈，紧缩只会导向更极致的贫穷。\n19世纪的改革家为底层人民指出的通往中产之路是五条箴言：勤劳，坚持，节俭，审慎，自力更生。但社会现实是，权贵们掌握着话语权，一旦你陷入贫穷，此前的辛勤付出全不算数，社会对你的困顿只有一条解释：懒惰。\n私人养老院的比例急剧上升，很多投机商大举投资养老院，经营不善则扔给政府逃之夭夭。收费998英镑一周。没钱了怎么办？可以卖房或者先住，死后房子被收走。但一般房产的价格只够住四年。工资低雇员少，虐待事件因此极为普遍。几乎每所大学都在想方设法取消全职合约，因为这样就能省掉不少职业养老金了。\n脱欧是这么多年来他们（农民，流水线工人等等）所获得唯一一次发声，是他们想象中对抗外来移民、以及国际垄断资本主义带来的困苦生活状态的一种方式。\nKeep calm and carry on. 这句话自维多利亚时代就被英国社会视为行为典范。恐慌是可耻的。正如贫穷是可耻的。疫情后，一部分人过上了在家办公的生活，另一部分人则成为“前线工人”（看护，餐饮工作等）。自封城后，人们对前线工人的赞美开始层出不穷，但这改变不了他们的高死亡率。\n1863年，古典自由主义者引领了废奴运动并获得巨大成功。全球化后，新自由主义者（大资本家）却用自由市场重新包装了蓄奴主义，且丝毫不觉有任何道德负担。吊诡的是，许多被损害的人也保持着同样的道德优越感：自由市场是完美的，穷人是不幸的，因为他们天生智商低下。富人是优胜劣汰的产物，通过创造财富缴纳税金，连同穷人一起受益。\n相关阅读 Why Poverty? (PBS documentary films) Poor Economics (中译名《贫穷的本质》)，豆瓣读书, Goodreads ","permalink":"https://pps43.github.io/posts/notes_on_texture_of_hard_times/","summary":"近日读完《贫穷的质感·王梆的英国观察》（豆瓣读书，Goodreads）一书后，将只言片语组合起来作一点记录。此书说了很多英国普通人的事情、社","title":"贫穷的质感（王梆）"},{"content":" 本文总结自Ray Dalio: How the Economic Machine Works，原视频地址\n核心概念简述 什么是交易 买方使用货币、信用（也叫信贷）向卖方交换商品、服务、金融资产。 交易首先是由人的天性驱动的。 买方的支出总额（而不是其他）是经济的驱动力，所有的经济周期和动力都是交易造成的。 交易的主体通常有：个人、企业、银行、政府。 政府如何参与交易 政府其实同时是最大的买方和卖方。 政府主要由两部分：中央政府，负责收税；中央银行，负责花钱。 中央银行通过利率和印钱控制经济中的货币和信用（信贷数量）。 什么是信贷 信贷也是一种交易。比如你找银行借钱，你是借款人，银行是贷款人。你是交易的卖方，银行是买方。 这种交易的特殊之处在于：交易是延迟完成的，即借款人保证将来偿还贷款和利息，直到那个时候，交易才算完成。 交易未完成前的状态，即债务。债务是贷款人的资产，借款人的负债。 只有交易完成时，这些资产和负债才会消失。 什么是价格 $$ 价格 = \\frac{买方支出总额}{产销总量} =\\frac{(货币 + 信贷)} {产销总量} $$\n什么是市场 一种市场，包含同一种商品的所有买方和所有卖方。有千千万万种商品或服务对应千千万万种市场。\n什么是经济 全部市场的全部交易统称为经济。\n经济形势变化的三条脉络 这三个相对独立的因素叠加起来，构成经济形势的变化：\n生产率的提高（近似线性） 债务小周期（5~8年） 债务大周期（75~100年） 深入经济周期 经济为什么依赖信贷 简而言之，因为借款人通过信贷可以增加自己的支出，而支出是经济的驱动力。在没有信贷的社会中，增加支出的唯一方式是增加生产。由于信贷可以凭空产生，支出可以超越当前生产，所以能让经济注入更多活力。\n更具体的说，交易双方中，A的支出是B的收入，A支出增加等于B收入增加，B偿还能力提升，信贷能力也会提升，B的支出可以进一步增加，在另一场交易中，B的支出增加等于C的收入增加……产生循环。不过要注意，借钱实质是向未来某一时刻的自己借钱，即未来那个时刻自己的收入减掉消费必须足够还钱才能保证交易最终的有效性。\n经济上下起伏不取决于人们多么善于创新或勤奋工作，而看信贷的总量。而人的天性和借贷机制决定了信贷总量一定是波动的。\n短期信贷周期为什么出现 考虑单一商品的情况。如果支出（消费）或者收入（消费是另一个人的收入）的增长速度超过所出售商品的生产速度，商品价格就会上涨。央行为了防止通货膨胀，会提高利率。利率直接影响人们贷款和借款意愿——这也是限制支出的唯一因素。还贷增加造成可支配的支出减少，商品价格下跌。\n当信贷易于获得则经济扩张，信贷不易获得则经济收缩，这个周期由央行主导，一般5~8年。\n长期信贷周期为什么出现 经历一个短期周期后，经济增长和债务规模都会超过前一个周期。债务规模的扩大其实正是人的天性：更喜欢消费而不是还钱。因此经历若干短周期后，债务增加的速度其实会超过收入（虽然收入也在增加）。到那时，社会总债务会变得过重而导致支出、收入、信用的全面下降，这个过程叫去杠杆化。\n社会总债务：政府债务、居民债务、非金融企业债务。社会总债务过重一般是指超过GDP的好几倍。\n去杠杆化过程开始后，会出现资产价格下跌、企业破产、信贷消失、无法正常还贷，只能变卖资产，进一步造成资产价格下跌，形成恶性循环，且无法通过降低利率摆脱（降到0以后就到底了）。例如欧美1929年、2008年，日本1989年。\nflowchart LR 资产价格下跌 --\u0026gt; 企业或个人破产 --\u0026gt; 信贷消失 --\u0026gt; 变卖资产 -.-\u0026gt; 资产价格下跌 根据具体情况的不同，该周期可能持续75~100年。\n有哪些方式解决债务过重问题 衡量债务负担是否过重：\n$$ 债务负担 = \\frac{债务}{收入} $$\n解决债务过重的四种主要手段：\n所有交易主体削减支出；\n因为A的支出是B的收入，B收入也下降了，那么B的债务负担更重，所有人更痛苦。会演变成通货紧缩。 例如企业为了削减成本造成大量人口失业。 通过判定违约和重组减少债务；\n大量借款人无法偿还即债务违约后，贷款人会向银行索要存款，造成挤兑，银行自身也会违约。这就叫经济萧条，即所有人都感觉自己的资产蒸发了。 比债务违约稍好一点的方式是债务重组，即还款减少或延期或利率降低。 不论哪种方式，由于A的债务是B的资产，债务消失或减少，意味着B的资产减少。依旧会演变成通货紧缩。 政府财富再分配；\n在去杠杆化的过程中，伴随着失业人口上升，因此政府支出变多甚至财政赤字，必须收更多的税或发行债券。 向谁融资或征税呢？通常是富人。但容易引发社会动荡。如果是国与国之间，会引发战争。 央行增发货币；\n前文提到央行的两个手段：利率和印钞。利率降低到0之后，只能印钞。 印出的钞票只能用来买入金融资产和政府债券。前者可以保住金融资产（例如股市）的价格，因此提高了持有人的信用（但仅仅对持有金融资产的人有用），后者其实是把印出的钱借给政府，由政府通过救济金和刺激计划增加人们收入，实质上是将经济系统分散的总债务，集中到政府的头上。 增发的货币刚好抵消贷款的降幅，就不会引发通货膨胀（参考前文说过的价格公式）。温和的通货膨胀本身对经济并不有害。但增发太猛会导致恶性通货膨胀，例如1920年的德国。 结合具体情势，通过巧妙地组合使用上述四种手段，才可以做到和谐的去杠杆。也不要太悲观，因为和谐的去杠杆化过程比信用扩张时期更好，因为此时经济是在自然缓慢增长的，夯实根基。\n反思 上述模型大致可以判断当前我们处于哪个阶段，预测未来大趋势如何。 上述模型解释了经济周期的必然性：现代社会的经济发展依赖信贷，信贷依赖机制和人性，机制有一定的调整空间（但交易延迟完成和要连本带利是刚性的）、人性则不可避免的包含贪婪、恐惧、喜欢消费而不是还钱、从众…… 上述模型对于个人的启发意义： 不要让债务的增长速度超过收入，因为最终会让债务把你压垮； 不要让收入的增长速度超过生产率，因为最终这会让你失去竞争力； 尽一切努力提高生产率，因为它在长期起最关键的作用。 ","permalink":"https://pps43.github.io/posts/how_the_economic_machine_works/","summary":"本文总结自Ray Dalio: How the Economic Machine Works，原视频地址 核心概念简述 什么是交易 买方使用货币、信用（也叫信贷）向卖方交换商品、服务、金融资产。 交易首先","title":"经济机器如何运行（达利欧）"}]