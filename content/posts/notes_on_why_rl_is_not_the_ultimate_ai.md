---
title: "强化学习可能不是AI的未来"
date: 2025-11-29
draft: false
hideSummary: false
tags: ["AI", "杂谈"]
---

> 最近，[Andrej Karpathy](https://www.youtube.com/watch?v=lXUZvyajciY) 和 [Ilya Sutskever](https://www.youtube.com/watch?v=aR20FWCCjAs) 几乎在同一时间，把目光从“堆算力、堆数据”的规模化竞赛，转向了更根本的问题：我们真的找到了让机器像人一样学习、反思和泛化的道路吗？在他们的讨论里，当前围绕强化学习（RL）打造的 LLM 智能，远没有表面上的那样风光，反而暴露出一系列深层缺陷——尤其是在泛化能力和学习机制上。如果说过去十年是“参数上亿、卡片上万”的工程时代，那么他们描述的，是一个重新回到“研究时代”的拐点。

## 当前基于RL的LLM智能的局限性

这些批评，大致可以概括为三点：**效率低、缺乏反思、对评估指标过度拟合**。

### 1. 效率低下与监督信号稀疏

在 Andrej 的比喻里，当前的强化学习更像是“**用吸管去吸监督信号**”——又慢又笨，他甚至直接称这套方法是“糟糕透顶”和“愚蠢”的。

想象一个典型的 RL 过程：模型经过上百步推演，终于给出一个答案。训练系统只在终点给一个奖励，然后把这一个奖励，平均地广播回整个决策轨迹中的每个 token。无论中间是走弯路、绕远路，还是偶然踩对了路径，都会被一视同仁地“强化”。这种高方差的更新方式，天然掩盖了“哪一步是真正关键”的因果结构。

人类在解决问题时几乎不会这样做。我们会不断停下来、回看自己的推理，区分“走偏的枝节”和“真正带来进展的那一步”，这是一种自带的“审查与反思机制”。而目前的 RL，更像是在黑暗中对整条路径一顿猛刷。

### 2. 认知缺陷与对评估指标的过度拟合

Ilya 提到一个尴尬的现象：模型在各种 eval 上的曲线一路飙升，但真正落在经济价值上，却远不如想象中耀眼。有时让它修一个 bug，它会顺手再引入第二个 bug，然后来回在两个错误之间震荡。

一个自然的怀疑是：我们是不是**把模型训练成了“只会考高分的学生”**？

- **训练目标太狭隘**：RL 的目标往往被设计得非常具体、短视，模型学会了“心无旁骛地追逐 reward”，但缺乏更高层次的判断和审美。
- **对评估的过度拟合**：研究者会依据 eval 的表现不断微调训练环境和奖励设计，长此以往，模型就像是被灌了 10,000 小时的 ACM/LeetCode 专项训练——在竞赛题上无所不能，但在真实工程里需要“品味”和整体判断时，却显得笨拙。

更糟的是，许多系统依赖 LLM 自己来做“裁判”：用一个 LLM 去给另一个 LLM 打分。这种 LLM judge 本身就是**高度可被利用（gameable）**的。模型可能会意外发现某些“邪路输出”（比如一串没有意义的字符），恰好能骗过裁判拿到 100% reward，而训练系统却浑然不觉。

### 3. 评估之外：“分数很好，但不会干活”

在很多实际项目中，人们已经隐约感受到这种割裂：论文里的表格异常亮眼，但真正接进业务流水线，收益却远不成比例。模型会“做题”，却很难稳定地“做事”。

这背后，其实是范式本身在逼迫模型去追求一种**机械化的“好结果”**，而不是一种更类似人类的“好过程”。

---

## 对未来发展方向的一些线索

面对这些局限，两人都不再满足于继续加卡、加数据，而是把注意力投向了更基础的问题：**怎样让模型学会更像人，而不是更像计算器？**

### 价值函数

Ilya 的一个判断，是我们已经从“规模红利期”进入“研究回归期”：再往上堆算力，性价比会越来越差，真正重要的是“**如何更有成效地用这些算力**”。

在他看来，最根本的短板是：当前模型的**泛化能力依然远逊于人类**。要弥补这点，一个被重新拎出来的老概念，是价值函数（Value Function）。

价值函数做的事情，其实是让模型在**长任务的中间步骤就能判断“这一步大概是对是错”**，而不是在任务结束后，才被动地接收一个终点 reward。这样一来，RL 就不必等到“故事讲完”才结算成败，而是能在过程中不断校正方向。

如果类比人类决策，价值函数更像是我们内心里那个不断嘀咕的声音：这一步走得怪怪的、那件事做得还不错。它把漫长的命运轨迹，压缩成了许多可感知的小步评价。

### 认知核心

Andrej 更关心的是模型内部的“认知结构”。在他的设想中，未来也许需要一种方法，**把模型在预训练中积累的各种“记忆”和“知识片段”剥离出去，只留下一个更抽象的“认知核心”**。

这个认知核心，不是百科全书式的记忆，而是“解决问题的魔力、策略和算法”本身。

问题在于，当下的 LLM 实在太擅长记忆：互联网上的段落、代码、公式，被它们以一种“模糊回忆”的形式塞进参数里。记忆多到一定程度，反而会遮蔽模式本身，让模型习惯于“查表”，不去逼近更普适的结构。

人类刚好相反：我们天生就不擅长死记硬背，背不住随机数字，也记不牢所有细节，于是只好去抓住**能跨情境泛化的那部分结构**。这种“记不住”的缺陷，反过来成了某种强迫式的归纳训练。

从这个角度看，未来的模型也许需要**少一点“信息记忆”，多一点“推理算法”**。

### 持续学习

Andrej 并不否认工程改进的价值。他的判断是：未来十年，AI 仍然会是“通过梯度下降训练出的巨型神经网络”，但方方面面都会以一种“各涨 20%”的方式缓慢抬升——更好的硬件、更高效的内核、更聪明的优化器、更合理的架构和更干净的数据。

其中数据也许是最被低估的一环。和我们日常浏览互联网时的感受类似，在他眼里，现阶段用于预训练的大部分互联网数据，说好听一点是“多样”，说直白一点就是“质量极其糟糕的垃圾”。

另一方面，两人都不约而同地强调了**持续学习（continual learning）**的重要性：

- 对 Andrej 来说，LLM 缺乏一个“蒸馏阶段”：人类会在白天经历大量信息，把这些经历暂存在“上下文窗口”里，然后在夜晚通过睡眠，把有价值的部分慢慢“写回权重”。而 LLM 目前更像是读一遍就算结束，很少有面向“长时间反思”的专门机制。
- 在 Ilya 的构想里，与其梦到一个“一上来就能干所有活儿的 AGI 成品”，不如脚踏实地去做一个“带着基础技能进入世界、然后靠持续学习和试错自我部署”的智能体——更像是一个能成长的孩子，而不是工厂里一次性出厂的机器人。



## 人脑独特机制的启示

和冰冷的训练日志相比，人脑在运作时，有很多机制带着一种近乎“玄学”的美感。那些当前 LLM 很难复制的特性，恰恰是自然在我们身上留下的深刻暗号。

### 反思与内化

人类在读一本书时，很少只是被动接受信息。更常见的情形是：书本被当作**一套提示**，勾连起既有经验，在脑海中自动生成合成数据——想象不同情节、推演新的场景、和已有观念“和解”或“对抗”。

真正的理解，往往不是发生在阅读本身，而是发生在读完之后的那几天：我们在地铁上突然想起某一段，在洗澡时突然串联起几个概念，在睡前莫名其妙地回放一段对白。这些零散的回味和走神，其实就是一种持续的“内在蒸馏”。

有一种颇动人的假说：**人类大脑在睡眠期间，会把白天积累的“上下文窗口”重新翻出来，进行痴迷般的分析、重组和合成数据生成，然后缓慢写入长期权重**。从这个意义上说，梦境和走神都像是大脑的“离线训练任务”。

相比之下，目前的 LLM 预训练过程就显得粗糙许多：读一遍数据，算完梯度就结束，很少有面向“长时间反思”的专门机制。

### 遗忘

人类的记忆有一个著名的特征：**记不住东西**。随机数字很难背，细枝末节很容易模糊，很多精彩的句子只记得大意。这在日常生活里常常被当作缺点，但从学习的角度看，它更像是一种来自自然的正则化。

因为记不住全部细节，我们被迫只去抓住那些**在不同情境下都用得上的“可组合部分”**——结构、模式、隐喻、因果关系。久而久之，人脑形成的其实不是一座完备的资料库，而是一套可以灵活重用的“生成程序”。

和此相反，LLM 在记忆方面几乎没有成本：无意义的字符串、随机的示例、甚至错误的样本，只要见过几次，都可以被它们快速“背”下来，并在之后复现。这种**过强的记忆力**，在某些任务上是优势，在泛化上却可能成为负担。

从这个角度看，遗忘并不是缺陷，而是一种为了泛化而付出的代价。自然在这里布置了一道看不见的正则项。

### 梦境

Andrej 还有一个充满诗意的观点：梦境也许是大脑防止过度拟合的一种方式。

梦中的世界经常是破碎的、离奇的：熟悉的人出现在陌生的街道，严肃的场合混杂着诡异的元素，时间和空间都被任意折叠。**这种高熵的“奇异情境”**，也许正是为了把白天过于规整的经验打散，逼迫大脑重新组织其内部表征，避免陷入一条狭窄的思路。

LLM 训练中有一个已知问题叫“分布崩溃”：如果模型不断地用自己的输出再训练自己，内容多样性会迅速降低，最后只会重复非常有限的几种模式——比如永远只讲那三四个笑话。某种意义上，梦境就是自然赋予人类的一套“反崩溃机制”：让我们在夜晚不断经历“不在分布上的样本”。


### 情绪

情绪在这里扮演的角色，同样耐人寻味。Ilya 提到的类比是：情绪可能就是机器学习中的价值函数，而且是那种由进化在底层硬编码的版本。

一些脑损伤案例提供了极其直观的证据：当负责情绪处理的区域受损时，病人的智商测验结果可以完全正常，但生活能力却急剧下降——他们会为选哪双袜子纠结几小时，却做不出任何果断的决定。

这说明，人类在做决策时，很大程度上不是在执行一个冷静的“期望值最大化”算法，而是在遵循一套被情绪染色的价值结构：害怕损失、因羞耻而退缩、为好奇心所驱动、被厌恶感迅速拉开距离。

如果把这些情绪反应看作一种**“简单却极其鲁棒的价值函数”**，就会发现：它们可能并不精确，却在广泛的情境中都异常可靠。这是一种在复杂性与鲁棒性之间，历经漫长进化博弈后被选出来的平衡。和我们实验室里小心翼翼设计的 reward 比起来，自然给自己的作品配的这套系统，多少让人心生敬畏。



## 如何用好目前的LLM来coding

从工程视角出发，Andrej 在构建 `nanochat` 仓库的过程中，总结了一些和 LLM 协作写代码的方式，也给今天的开发者提了一个醒：**要把它当作放大器，而不是代理人**。

### 三种常见的协作模式

当前在编码中使用 LLM，大致可以分成三种模式：

1. **完全拒绝 LLM**  
   这一模式已经越来越少被推荐：在有条件使用高质量模型的前提下，彻底排斥它们的帮助，本身也是一种低效。

2. **中间模式（Andrej 眼中的“甜点”）**  
   开发者依然作为架构师，自己从零搭建结构、写关键逻辑，只是大量利用模型的自动补全。
   - **优点**：信息带宽极高——开发者只需移动光标到需要的地方，打出几个字符，模型就会在上下文和风格的约束下给出相对合理的补全。
   - **适用场景**：日常开发、已有项目演进。Andrej 在开发 `nanochat` 时，主要就是这样用的。

3. **Agent / “随性编码”模式**  
   直接对模型下指令：“请帮我实现这个功能”“请搭一个这样那样的服务”，让模型主导代码生成。
   - **优点**：非常适合生成样板代码（boilerplate），或者在互联网上范例很多的通用场景。对于不熟悉的语言（比如 Rust），它也能帮你跨过入门门槛。
   - **适用场景**：项目脚手架、重复性强的模板、报告和配置类代码等。

### LLM Agent 在编码中的实际局限

然而，一旦进入真实工程，尤其是复杂、独特、容错率极低的代码场景时，Agent 模式的问题就会暴露出来：

- **难以消化项目独特性**：模型很难真正内化你项目特有的代码风格、抽象边界和诡异的历史包袱。比如在 `nanochat` 中，Andrej 自己写了定制的梯度同步逻辑，但模型总是想把他往标准的 PyTorch DDP 上拽。
- **代码质量参差不齐**：自动生成的代码往往又长又臃肿，喜欢堆砌 `try-catch`，动不动就给你整出一个“企业级”的架构。
- **API 认知滞后**：模型使用的库接口，很多时候已经过时或弃用，需要人类再做一轮核查。
- **难以融合新研究进展**：哪怕它“知道” RoPE、flash attention 之类的新技术，也很难把这些东西真正、优雅地融入到你已有的、自定义味道很浓的代码库中。
- **对“从未被写过的代码”很不擅长**：而恰恰在 AI 研究和高难工程里，真正关键的代码几乎都是第一次被写出来的。

综合起来，LLM 在写代码上更像是一个**高效的补全和搜索引擎**：只要轨道还在“人类已有经验的分布内”，它就能帮你跑得很快；一旦走到真正未知的地方，它就需要你来重新开路。

## 最后的话

现在的我们很容易产生一种错觉：智能已经降临。但只要看一眼人类大脑那套精妙的机制，就会意识到——**我们真正模仿的，其实还只是这个巨大系统里最表层、最机械的一环**。

也许这正是这个时代最迷人的地方：一边是冰冷的梯度下降，一边是“天地有大美而不言”。而我们就站在这两者之间，看着机器一点点变聪明，也顺便重新认识一下，自己究竟是怎么在这个世界上“学会成为自己”的。

> 人有人的价值。——罗伯特维纳。